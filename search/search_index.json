{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Patterns and Techniques for Writing High-Performance Applications with Go","text":"<p>The Go App Optimization Guide is a series of in-depth, technical articles for developers who want to get more performance out of their Go code without relying on guesswork or cargo cult patterns. If you\u2019re building services that need to handle real load\u2014APIs, backend pipelines, or distributed systems\u2014this guide focuses on the kind of low-level behavior and tuning opportunities that actually matter in production.</p> <p>Go doesn\u2019t give you the kind of fine-grained control you\u2019d find in C++ or Rust, but it does give you just enough visibility to reason about performance\u2014and just enough tooling to do something about it. From understanding allocation patterns and reducing GC overhead to building efficient network services and managing concurrency at scale, the series focuses on optimizations that are both practical and measurable.</p> <p>The goal isn\u2019t to write clever code\u2014it\u2019s to write fast, predictable code that holds up under pressure. Everything in this guide is backed by real use cases, stripped of theory, and aimed at what you can apply right now.</p>"},{"location":"#common-go-patterns-for-performance","title":"Common Go Patterns for Performance","text":"<p>This first article series covers a set of performance patterns that come up again and again when writing real-world Go code. It\u2019s not an exhaustive list, but it hits the areas where small changes tend to make a noticeable difference:</p> <ul> <li>Making proper use of <code>sync.Pool</code></li> <li>Cutting down on unnecessary allocations</li> <li>Struct layout and memory alignment details that affect cache performance</li> <li>Error handling that doesn\u2019t drag down the fast path</li> <li>Using interfaces without paying for them</li> <li>Reusing slices and sorting in-place</li> </ul> <p>Each pattern includes real code and numbers you can apply directly\u2014no theory, no fluff.</p>"},{"location":"#high-performance-networking-in-go","title":"High-Performance Networking in Go","text":"<p>This section takes a focused look at what it takes to build fast, reliable network services in Go. It covers not just how to use the standard library, but how to push it further when you\u2019re dealing with real load.</p> <p>Topics include:</p> <ul> <li>Efficient use of <code>net/http</code>, <code>net.Conn</code>, and connection pooling</li> <li>Serving thousands of concurrent connections without collapsing</li> <li>Tuning Go\u2019s scheduler and system-level settings (GOMAXPROCS, epoll, kqueue)</li> <li>Building resilient services with load shedding and backpressure</li> <li>Preventing memory leaks in long-lived connections</li> <li>Choosing and tuning transport protocols: TCP, HTTP/2, gRPC, QUIC</li> </ul> <p>While the goal is to keep everything grounded in practical examples, this part leans more theoretical for now due to the exploratory nature of the networking topics being developed.</p>"},{"location":"#who-this-is-for","title":"Who This Is For","text":"<p>This series is ideal for:</p> <ul> <li>Engineers working on backend systems where Go\u2019s performance actually matters</li> <li>Developers building latency-sensitive services or handling high-throughput traffic</li> <li>Teams moving critical paths to Go and needing to understand the trade-offs</li> <li>Anyone who wants a clearer picture of how Go behaves under load</li> </ul> <p>More content is coming soon\u2014additional articles, practical code examples, and tooling insights. Bookmark this page to keep up as the series grows.</p>"},{"location":"01-common-patterns/","title":"Common Go Patterns for Performance","text":"<p>Optimizing Go applications requires understanding common patterns that help reduce latency, improve memory efficiency, and enhance concurrency. This guide organizes 15 key techniques into four practical categories.</p>"},{"location":"01-common-patterns/#memory-management-efficiency","title":"Memory Management &amp; Efficiency","text":"<p>These strategies help reduce memory churn, avoid excessive allocations, and improve cache behavior.</p> <ul> <li> <p>Object Pooling   Reuse objects to reduce GC pressure and allocation overhead.</p> </li> <li> <p>Memory Preallocation   Allocate slices and maps with capacity upfront to avoid costly resizes.</p> </li> <li> <p>Struct Field Alignment   Optimize memory layout to minimize padding and improve locality.</p> </li> <li> <p>Avoiding Interface Boxing   Prevent hidden allocations by avoiding unnecessary interface conversions.</p> </li> <li> <p>Zero-Copy Techniques   Minimize data copying with slicing and buffer tricks.</p> </li> <li> <p>Memory Efficiency and Go\u2019s Garbage Collector   Reduce GC overhead by minimizing heap usage and reusing memory.</p> </li> <li> <p>Stack Allocations and Escape Analysis   Use escape analysis to help values stay on the stack where possible.</p> </li> </ul>"},{"location":"01-common-patterns/#concurrency-and-synchronization","title":"Concurrency and Synchronization","text":"<p>Manage goroutines, shared resources, and coordination efficiently.</p> <ul> <li> <p>Goroutine Worker Pools   Control concurrency with a fixed-size pool to limit resource usage.</p> </li> <li> <p>Atomic Operations and Synchronization Primitives   Use atomic operations or lightweight locks to manage shared state.</p> </li> <li> <p>Lazy Initialization (<code>sync.Once</code>)   Delay expensive setup logic until it's actually needed.</p> </li> <li> <p>Immutable Data Sharing   Share data safely between goroutines without locks by making it immutable.</p> </li> <li> <p>Efficient Context Management   Use <code>context</code> to propagate timeouts and cancel signals across goroutines.</p> </li> </ul>"},{"location":"01-common-patterns/#io-optimization-and-throughput","title":"I/O Optimization and Throughput","text":"<p>Reduce system call overhead and increase data throughput for I/O-heavy workloads.</p> <ul> <li> <p>Efficient Buffering   Use buffered readers/writers to minimize I/O calls.</p> </li> <li> <p>Batching Operations   Combine multiple small operations to reduce round trips and improve throughput.</p> </li> </ul>"},{"location":"01-common-patterns/#compiler-level-optimization-and-tuning","title":"Compiler-Level Optimization and Tuning","text":"<p>Tap into Go\u2019s compiler and linker to further optimize your application.</p> <ul> <li> <p>Leveraging Compiler Optimization Flags   Use build flags like <code>-gcflags</code> and <code>-ldflags</code> for performance tuning.</p> </li> <li> <p>Stack Allocations and Escape Analysis   Analyze which values escape to the heap to help the compiler optimize memory placement.</p> </li> </ul>"},{"location":"01-common-patterns/atomic-ops/","title":"Atomic Operations and Synchronization Primitives","text":"<p>In high-concurrency systems, performance isn't just about what you do\u2014it's about what you avoid. Lock contention, cache line bouncing and memory fences quietly shape throughput long before you hit your scaling ceiling. Atomic operations are among the leanest tools Go offers to sidestep these pitfalls.</p> <p>While Go provides the full suite of synchronization primitives, there's a class of problems where locks feel like overkill. Atomics offers clarity and speed for low-level coordination\u2014counters, flags, and simple state machines, especially under pressure.</p>"},{"location":"01-common-patterns/atomic-ops/#understanding-atomic-operations","title":"Understanding Atomic Operations","text":"<p>Atomic operations allow safe concurrent access to shared data without explicit locking mechanisms like mutexes. The <code>sync/atomic</code> package provides low-level atomic memory primitives ideal for counters, flags, or simple state transitions.</p> <p>The key benefit of atomic operations is performance under contention. Locking introduces coordination overhead\u2014when many goroutines contend for a mutex, performance can degrade due to context switching and lock queue management. Atomics avoids this by operating directly at the hardware level using CPU instructions like <code>CAS</code> (compare-and-swap). This makes them particularly useful for:</p> <ul> <li>High-throughput counters and flags</li> <li>Lock-free queues and freelists</li> <li>Low-latency paths where locks are too expensive</li> </ul>"},{"location":"01-common-patterns/atomic-ops/#memory-model-and-comparison-to-c","title":"Memory Model and Comparison to C++","text":"<p>Understanding memory models is crucial when reasoning about concurrency. In C++, developers have fine-grained control over atomic operations via memory orderings, which allows them to trade-off between performance and consistency. By default, Go's atomic operations enforce sequential consistency, which means they behave like <code>std::memory_order_seq_cst</code> in C++. This is the strongest and safest memory ordering:</p> <ul> <li>All threads observe atomic operations in the same order.</li> <li>Full memory barrier are applied before and after each operation.</li> <li>Reads and writes are not reordered across atomic operations.</li> </ul> C++ Memory Order Go Equivalent Notes <code>memory_order_seq_cst</code> All <code>atomic.*</code> ops Full sequential consistency <code>memory_order_acquire</code> Not exposed Not available in Go <code>memory_order_release</code> Not exposed Not available in Go <code>memory_order_relaxed</code> Not exposed Not available in Go <p>Go does not expose weaker memory models like <code>relaxed</code>, <code>acquire</code>, or <code>release</code>. This is an intentional simplification to promote safety and reduce the risk of subtle data races. All atomic operations in Go imply synchronization across goroutines, ensuring correct behavior without manual memory fencing.</p> <p>This means you don\u2019t have to reason about instruction reordering or memory visibility at a low level\u2014but it also means you can\u2019t fine-tune for performance in the way C++ or Rust developers might use relaxed atomics.</p> <p>Low-level access to relaxed memory ordering in Go exists internally (e.g., in the runtime or through <code>go:linkname</code>), but it\u2019s not safe or supported for use in application-level code.</p>"},{"location":"01-common-patterns/atomic-ops/#common-atomic-operations","title":"Common Atomic Operations","text":"<ul> <li><code>atomic.AddInt64</code>, <code>atomic.AddUint32</code>, etc.: Adds values atomically.</li> <li><code>atomic.LoadInt64</code>, <code>atomic.LoadPointer</code>: Reads values atomically.</li> <li><code>atomic.StoreInt64</code>, <code>atomic.StorePointer</code>: Writes values atomically.</li> <li><code>atomic.CompareAndSwapInt64</code>: Conditionally updates a value atomically.</li> </ul>"},{"location":"01-common-patterns/atomic-ops/#when-to-use-atomic-operations-in-real-life","title":"When to Use Atomic Operations in Real Life","text":""},{"location":"01-common-patterns/atomic-ops/#high-throughput-metrics-and-counters","title":"High-throughput metrics and Counters","text":"<p>Tracking request counts, dropped packets, or other lightweight stats:</p> <pre><code>var requests atomic.Int64\n\nfunc handleRequest() {\n    requests.Add(1)\n}\n</code></pre> <p>This code allows multiple goroutines to safely increment a shared counter without using locks. <code>atomic.AddInt64</code> ensures each addition is performed atomically, preventing race conditions and keeping performance high under heavy load.</p>"},{"location":"01-common-patterns/atomic-ops/#fast-lock-free-flags","title":"Fast, Lock-Free Flags","text":"<p>Simple boolean state shared across threads:</p> <pre><code>var shutdown atomic.Int32\n\nfunc mainLoop() {\n    for {\n        if shutdown.Load() == 1 {\n            break\n        }\n        // do work\n    }\n}\n\nfunc stop() {\n    shutdown.Store(1)\n}\n</code></pre> <p>This pattern allows one goroutine to signal another to stop. <code>atomic.LoadInt32</code> reads the flag with synchronization guarantees, and <code>atomic.StoreInt32</code> sets the flag in a way visible to all goroutines. It helps implement safe shutdown signals.</p>"},{"location":"01-common-patterns/atomic-ops/#once-only-initialization","title":"Once-Only Initialization","text":"<p>For scenarios where <code>sync.Once</code> isn\u2019t flexible enough\u2014such as needing retryable or restartable initialization \u2013 a more precise control can be achieved using atomic operations:</p> <pre><code>import (\n    \"runtime\"\n    \"sync/atomic\"\n    \"unsafe\"\n)\n\nvar resource unsafe.Pointer\nvar initStatus int32 // 0: not started, 1: in progress, 2: completed\n\nfunc getResource() *MyResource {\n    if atomic.LoadInt32(&amp;initStatus) == 2 {\n        return (*MyResource)(atomic.LoadPointer(&amp;resource))\n    }\n\n    if atomic.CompareAndSwapInt32(&amp;initStatus, 0, 1) {\n        newRes := expensiveInit() // initialization logic\n        atomic.StorePointer(&amp;resource, unsafe.Pointer(newRes))\n        atomic.StoreInt32(&amp;initStatus, 2)\n        return newRes\n    }\n\n    for atomic.LoadInt32(&amp;initStatus) != 2 {\n        runtime.Gosched() // yield until the initializer finishes\n    }\n    return (*MyResource)(atomic.LoadPointer(&amp;resource))\n}\n</code></pre> <p>This pattern uses an explicit three-state protocol:</p> <ul> <li><code>0</code> = uninitialized</li> <li><code>1</code> = initialization in progress</li> <li><code>2</code> = initialized</li> </ul> <p>The first goroutine that successfully flips the state from <code>0</code> to <code>1</code> takes charge of the initialization. The rest wait in a lightweight spin loop, briefly yielding with <code>runtime.Gosched()</code> until initialization completes. Once the state flips to <code>2</code>, they read the resource and continue.</p> <p>Unlike <code>sync.Once</code>, this approach avoids mutex overhead and gives you full control over how and when initialization happens. It\u2019s well-suited for high-performance paths or systems where partial or retryable initialization is necessary.</p>"},{"location":"01-common-patterns/atomic-ops/#lock-free-queues-or-freelist-structures","title":"Lock-Free Queues or Freelist Structures","text":"<p>Building high-performance data structures:</p> <pre><code>type node struct {\n    next *node\n    val  any\n}\n\nvar head atomic.Pointer[node]\n\nfunc push(n *node) {\n    for {\n        old := head.Load()\n        n.next = old\n        if head.CompareAndSwap(old, n) {\n            return\n        }\n    }\n}\n</code></pre> <p>This implements a lock-free stack (LIFO queue). It repeatedly tries to insert a node at the head of the list by atomically replacing the head pointer only if it hasn't changed\u2014a classic <code>CAS</code> loop. It's commonly used in object pools and work-stealing queues.</p>"},{"location":"01-common-patterns/atomic-ops/#reducing-lock-contention","title":"Reducing Lock Contention","text":"<p>This approach is common in real-world systems to reduce unnecessary lock contention, such as feature toggles, one-time initialization paths, or conditional caching mechanisms. Atomics serves as a fast-path filter before acquiring a more expensive lock.</p> <p>Combining atomics with mutexes to gate expensive work:</p> <pre><code>if atomic.LoadInt32(&amp;someFlag) == 0 {\n    return\n}\nmu.Lock()\ndefer mu.Unlock()\n// do something heavy\n</code></pre> <p>This pattern is effective when <code>someFlag</code> is set by another goroutine, and the current goroutine only uses it as a read-only signal to determine if it should proceed. It avoids unnecessary lock acquisition in high-throughput paths, such as short-circuiting when a feature is disabled or a task has already been completed.</p> <p>However, if the same goroutine is also responsible forsetting the flag, a simple load followed by a lock is not safe. Another goroutine could interleave between the check and the lock, leading to inconsistent behavior.</p> <p>To make the operation safe and atomic, use <code>CompareAndSwap</code>:</p> <pre><code>if !atomic.CompareAndSwapInt32(&amp;someFlag, 0, 1) {\n    return // work already in progress or completed\n}\nmu.Lock()\ndefer mu.Unlock()\n// perform one-time expensive initialization\n</code></pre> <p>This version guarantees that only one goroutine proceeds and others exit early. It ensures both the check and the update to <code>someFlag</code> happen atomically.</p> <p>Here, the atomic read acts as a fast gatekeeper. If the flag is unset, acquiring the mutex is unnecessary. This avoids unnecessary locking in high-frequency code paths, improving responsiveness under load.</p>"},{"location":"01-common-patterns/atomic-ops/#synchronization-primitives","title":"Synchronization Primitives","text":"<p>This section is intentionally kept minimal. Go's synchronization primitives\u2014such as <code>sync.Mutex</code>, <code>sync.RWMutex</code>, and <code>sync.Cond</code>\u2014are already thoroughly documented and widely understood. They are essential tools for managing shared memory and coordinating goroutines, but they are not the focus here.</p> <p>In the context of this article, we reference them only as a performance comparison baseline against atomic operations. When appropriate, these primitives offer clarity and correctness, but they often come at a higher cost in high-contention scenarios, where atomics can provide leaner alternatives.</p> <p>We\u2019ll use them as contrast points to highlight when and why atomic operations might offer performance advantages.</p>"},{"location":"01-common-patterns/atomic-ops/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>To understand the impact of atomic operations versus mutex locks, we can compare the time taken to increment a shared counter across goroutines using a simple benchmark.</p> <pre><code>func BenchmarkAtomicIncrement(b *testing.B) {\n    var counter int64\n    b.RunParallel(func(pb *testing.PB) {\n        for pb.Next() {\n            atomic.AddInt64(&amp;counter, 1)\n        }\n    })\n}\n\nfunc BenchmarkMutexIncrement(b *testing.B) {\n    var (\n        counter int64\n        mu      sync.Mutex\n    )\n    b.RunParallel(func(pb *testing.PB) {\n        for pb.Next() {\n            mu.Lock()\n            counter++\n            mu.Unlock()\n        }\n    })\n}\n</code></pre> <p>Benchmark results:</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkAtomicIncrement-14 39,910,514 80.40 0 0 BenchmarkMutexIncrement-14 32,629,298 110.7 0 0 <p>Atomic operations outperform mutex-based increments in both throughput and latency. The difference becomes more significant under higher contention, where avoiding lock acquisition helps reduce context switching and scheduler overhead.</p> Show the complete benchmark file <pre><code>package perf\n\nimport (\n    \"testing\"\n    \"sync/atomic\"\n    \"sync\"\n)\n\n// bench-start\nfunc BenchmarkAtomicIncrement(b *testing.B) {\n    var counter int64\n    b.RunParallel(func(pb *testing.PB) {\n        for pb.Next() {\n            atomic.AddInt64(&amp;counter, 1)\n        }\n    })\n}\n\nfunc BenchmarkMutexIncrement(b *testing.B) {\n    var (\n        counter int64\n        mu      sync.Mutex\n    )\n    b.RunParallel(func(pb *testing.PB) {\n        for pb.Next() {\n            mu.Lock()\n            counter++\n            mu.Unlock()\n        }\n    })\n}\n// bench-end\n</code></pre>"},{"location":"01-common-patterns/atomic-ops/#when-to-use-atomic-operations-vs-mutexes","title":"When to Use Atomic Operations vs. Mutexes","text":"<p> Atomic operations shine in simple, high-frequency scenarios\u2014counters, flags, coordination signals\u2014where the cost of a lock would be disproportionate. They avoid lock queues and reduce context switching. But they come with limitations: no grouping of multiple operations, no rollback, and increased complexity when applied beyond their niche.</p> <p> Mutexes remain the right tool for managing complex shared state, protecting multi-step critical sections, and maintaining invariants. They're easier to reason and generally safer when logic grows beyond a few lines.</p> <p>Choosing between atomics and locks isn't about ideology but scope. When the job is simple, atomics get out of the way. When the job gets complex, locks keep you safe.</p>"},{"location":"01-common-patterns/batching-ops/","title":"Batching Operations in Go","text":"<p>Batching is one of those techniques that\u2019s easy to overlook but incredibly useful when performance starts to matter. Instead of handling one operation at a time, you group them together\u2014cutting down on the overhead of repeated calls, whether that\u2019s hitting the network, writing to disk, or making a database commit. It\u2019s a practical, low-complexity approach that can reduce latency and stretch your system\u2019s throughput further than you\u2019d expect.</p>"},{"location":"01-common-patterns/batching-ops/#why-batching-matters","title":"Why Batching Matters","text":"<p>Most systems don\u2019t struggle because individual operations are too slow\u2014they struggle because they do too many of them. Every call out to a database, API, or filesystem adds some fixed cost: a system call, a network round trip, maybe a lock or a context switch. When those costs add up across high-volume workloads, the impact is hard to ignore. Batching helps by collapsing those calls into fewer, more efficient units of work, which often leads to measurable gains in both performance and resource usage.</p> <p>Consider a logging service writing to disk:</p> <pre><code>func logLine(line string) {\n    f.WriteString(line + \"\\n\")\n}\n</code></pre> <p>When invoked thousands of times per second, the file system is inundated with individual write system calls, significantly degrading performance. A better approach could be aggregates log entries and flushes them in bulk:</p> <pre><code>var batch []string\n\nfunc logBatch(line string) {\n    batch = append(batch, line)\n    if len(batch) &gt;= 100 {\n        f.WriteString(strings.Join(batch, \"\\n\") + \"\\n\")\n        batch = batch[:0]\n    }\n}\n</code></pre> <p>With batching, each write operation handles multiple entries simultaneously, reducing syscall overhead and improving disk I/O efficiency.</p> <p>Warning</p> <p>While batching offers substantial performance advantages, it also introduces the risk of data loss. If an application crashes before a batch is flushed, the in-memory data can be lost. Systems dealing with critical or transactional data must incorporate safeguards such as periodic flushes, persistent storage buffers, or recovery mechanisms to mitigate this risk.</p>"},{"location":"01-common-patterns/batching-ops/#how-generic-batcher-may-looks-like","title":"How generic Batcher may looks like","text":"<p>We can implement a generic batcher in very straight forward manner:</p> <pre><code>type Batcher[T any] struct {\n    mu     sync.Mutex\n    buffer []T\n    size   int\n    flush  func([]T)\n}\n\nfunc NewBatcher[T any](size int, flush func([]T)) *Batcher[T] {\n    return &amp;Batcher[T]{\n        buffer: make([]T, 0, size),\n        size:   size,\n        flush:  flush,\n    }\n}\n\nfunc (b *Batcher[T]) Add(item T) {\n    b.mu.Lock()\n    defer b.mu.Unlock()\n    b.buffer = append(b.buffer, item)\n    if len(b.buffer) &gt;= b.size {\n        b.flushNow()\n    }\n}\n\nfunc (b *Batcher[T]) flushNow() {\n    if len(b.buffer) == 0 {\n        return\n    }\n    b.flush(b.buffer)\n    b.buffer = b.buffer[:0]\n}\n</code></pre> <p>Warning</p> <p>This batcher implementation expects that you will never call <code>Batcher.Add(...)</code> from your <code>flush()</code> function. We have this limitation because Go mutexes are not recursive.</p> <p>This batcher works with any data type, making it a flexible solution for aggregating logs, metrics, database writes, or other grouped operations. Internally, the buffer acts as a queue that accumulates items until a flush threshold is reached. The use of <code>sync.Mutex</code> ensures that <code>Add()</code> and <code>flushNow()</code> are safe for concurrent access, which is necessary in most real-world systems where multiple goroutines may write to the batcher.</p> <p>From a performance standpoint, it's true that a lock-free implementation\u2014using atomic operations or concurrent ring buffers\u2014could reduce contention and improve throughput under heavy load. However, such designs are more complex, harder to maintain, and generally not justified unless you're pushing extremely high concurrency or low-latency boundaries. For most practical workloads, the simplicity and safety of a <code>sync.Mutex</code>-based design offers a great balance between performance and maintainability.</p>"},{"location":"01-common-patterns/batching-ops/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>To validate batching performance, we tested six scenarios across three categories: in-memory processing, file I/O, and CPU-intensive hashing. Each category included both unbatched and batched variants, with all benchmarks running over 10,000 items per operation.</p> Show the benchmark file <pre><code>package perf\n\nimport (\n    \"crypto/sha256\"\n    \"encoding/hex\"\n    \"fmt\"\n    \"os\"\n    \"strings\"\n    \"testing\"\n)\n\nvar lines = make([]string, 10000)\n\nfunc init() {\n    for i := range lines {\n        lines[i] = fmt.Sprintf(\"log entry %d %s\", i, strings.Repeat(\"x\", 100))\n    }\n}\n\n// --- 1. No I/O ---\n\nfunc BenchmarkUnbatchedProcessing(b *testing.B) {\n    for b.Loop() {\n        for _, line := range lines {\n            strings.ToUpper(line)\n        }\n    }\n}\n\nfunc BenchmarkBatchedProcessing(b *testing.B) {\n    batchSize := 100\n    for b.Loop() {\n        for i := 0; i &lt; len(lines); i += batchSize {\n            end := i + batchSize\n            if end &gt; len(lines) {\n                end = len(lines)\n            }\n            batch := strings.Join(lines[i:end], \"|\")\n            strings.ToUpper(batch)\n        }\n    }\n}\n\n// --- 2. With I/O ---\n\nfunc BenchmarkUnbatchedIO(b *testing.B) {\n    for b.Loop() {\n        f, err := os.CreateTemp(\"\", \"unbatched\")\n        if err != nil {\n            b.Fatal(err)\n        }\n        for _, line := range lines {\n            _, _ = f.WriteString(line + \"\\n\")\n        }\n        f.Close()\n        os.Remove(f.Name())\n    }\n}\n\nfunc BenchmarkBatchedIO(b *testing.B) {\n    batchSize := 100\n    for b.Loop() {\n        f, err := os.CreateTemp(\"\", \"batched\")\n        if err != nil {\n            b.Fatal(err)\n        }\n        for i := 0; i &lt; len(lines); i += batchSize {\n            end := i + batchSize\n            if end &gt; len(lines) {\n                end = len(lines)\n            }\n            batch := strings.Join(lines[i:end], \"\\n\") + \"\\n\"\n            _, _ = f.WriteString(batch)\n        }\n        f.Close()\n        os.Remove(f.Name())\n    }\n}\n\n// --- 3. With Crypto ---\n\nfunc hash(s string) string {\n    h := sha256.Sum256([]byte(s))\n    return hex.EncodeToString(h[:])\n}\n\nfunc BenchmarkUnbatchedCrypto(b *testing.B) {\n    for b.Loop() {\n        for _, line := range lines {\n            hash(line)\n        }\n    }\n}\n\nfunc BenchmarkBatchedCrypto(b *testing.B) {\n    batchSize := 100\n    for b.Loop() {\n        for i := 0; i &lt; len(lines); i += batchSize {\n            end := i + batchSize\n            if end &gt; len(lines) {\n                end = len(lines)\n            }\n            joined := strings.Join(lines[i:end], \"\")\n            hash(joined)\n        }\n    }\n}\n</code></pre> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkUnbatchedProcessing-14 530 2,028,492 1,279,850 10,000 BenchmarkBatchedProcessing-14 573 2,094,168 2,457,603 200 <p>In-memory string manipulation showed a modest performance delta. While the batched variant reduced memory allocations by 50x, the execution time was only marginally slower due to the cost of joining large strings. This highlights that batching isn\u2019t always faster in raw throughput, but it consistently reduces pressure on the garbage collector.</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkUnbatchedIO-14 87 12,766,433 1,280,424 10,007 BenchmarkBatchedIO-14 1324 993,912 2,458,026 207 <p>File I/O benchmarks showed the most dramatic gains. The batched version was over 12 times faster than the unbatched one, with far fewer syscalls and significantly lower execution time. Grouping disk writes amortized the I/O cost, leading to a huge efficiency boost despite temporarily using more memory.</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkUnbatchedCrypto-14 978 1,232,242 2,559,840 30,000 BenchmarkBatchedCrypto-14 1760 675,303 2,470,406 400 <p>The cryptographic benchmarks demonstrated batching\u2019s value in CPU-bound scenarios. Batched hashing nearly halved the total processing time while reducing allocation count by more than 70x. This reinforces batching as an effective strategy even in CPU-intensive workloads where fewer operations yield better locality and cache behavior.</p>"},{"location":"01-common-patterns/batching-ops/#when-to-use-batching","title":"When To Use Batching","text":"<p> Use batching when:</p> <ul> <li>Individual operations are expensive (e.g., I/O, RPC, DB writes). Grouping multiple operations into a single batch reduces the overhead of repeated calls and improves efficiency.</li> <li>The system benefits from reducing the frequency of external interactions. Fewer external calls can ease load on downstream systems and reduce contention or rate-limiting issues.</li> <li>You have some tolerance for per-item latency in favor of higher throughput. Batching introduces slight delays but can significantly increase overall system throughput.</li> </ul> <p> Avoid batching when:</p> <ul> <li>Immediate action is required for each individual input. Delaying processing to build a batch may violate time-sensitive requirements.</li> <li>Holding data introduces risk (e.g., crash before flush). If data must be processed or persisted immediately to avoid loss, batching can be unsafe.</li> <li>Predictable latency is more important than throughput. Batching adds variability in timing, which may not be acceptable in systems with strict latency expectations.</li> </ul>"},{"location":"01-common-patterns/buffered-io/","title":"Efficient Buffering in Go","text":"<p>Buffering is a core performance technique in systems programming. In Go, it's especially relevant when working with I/O\u2014file access, network communication, and stream processing. Without buffering, many operations incur excessive system calls or synchronization overhead. Proper buffering reduces the frequency of such interactions, improves throughput, and smooths latency spikes.</p>"},{"location":"01-common-patterns/buffered-io/#why-buffering-matters","title":"Why Buffering Matters","text":"<p>Every time you read from or write to a file or socket, there\u2019s a good chance you\u2019re triggering a system call\u2014and that\u2019s not cheap. System calls move control from user space into kernel space, which means crossing a boundary that comes with overhead: entering kernel mode, possible context switches, interacting with I/O buffers, and sometimes queuing operations behind the scenes. Doing that once in a while is fine. Doing it thousands of times per second? That\u2019s a problem. Buffering helps by batching small reads or writes into larger chunks, reducing how often you cross that boundary and making far better use of each syscall.</p> <p>For example, writing to a file in a loop without buffering, like this:</p> <pre><code>f, _ := os.Create(\"output.txt\")\nfor i := 0; i &lt; 10000; i++ {\n    f.Write([]byte(\"line\\n\"))\n}\n</code></pre> <p>This can easily result in 10,000 separate system calls, each carrying its own overhead and dragging down performance. On top of that, a flood of small writes tends to fragment disk operations, which puts extra pressure on I/O subsystems and wastes CPU cycles handling what could have been a single, efficient batch.</p>"},{"location":"01-common-patterns/buffered-io/#with-buffering","title":"With Buffering","text":"<pre><code>f, _ := os.Create(\"output.txt\")\nbuf := bufio.NewWriter(f)\nfor i := 0; i &lt; 10000; i++ {\n    buf.WriteString(\"line\\n\")\n}\nbuf.Flush() // ensure all buffered data is written\n</code></pre> <p>This version significantly reduces the number of system calls. The <code>bufio.Writer</code> accumulates writes in an internal memory buffer (typically 4KB or more). It only triggers a syscall when the buffer is full or explicitly flushed. As a result, you achieve faster I/O, reduced CPU usage, and improved performance.</p> <p>Note</p> <p><code>bufio.Writer</code> does not automatically flush when closed. If you forget to call <code>Flush()</code>, any unwritten data remaining in the buffer will be lost. Always call <code>Flush()</code> before closing or returning from a function, especially if the total written size is smaller than the buffer capacity.</p>"},{"location":"01-common-patterns/buffered-io/#controlling-buffer-capacity","title":"Controlling Buffer Capacity","text":"<p>By default, <code>bufio.NewWriter()</code> allocates a 4096-byte (4 KB) buffer. This size aligns with the common block size of file systems and the standard memory page size on most operating systems (such as Linux, BSD, and macOS). Reading or writing in 4 KB increments minimizes page faults, aligns with kernel read-ahead strategies, and maps efficiently onto underlying disk I/O operations.</p> <p>While 4 KB is a practical general-purpose default, it might not be optimal for all workloads. For high-throughput scenarios\u2014such as streaming large files or generating extensive logs\u2014a larger buffer can help reduce syscall frequency further:</p> <pre><code>f, _ := os.Create(\"output.txt\")\nbuf := bufio.NewWriterSize(f, 16*1024) // 16 KB buffer\n</code></pre> <p>Conversely, if latency is more critical than throughput (e.g., interactive systems or command-line utilities), a smaller buffer may be more appropriate, as it flushes data more frequently.</p> <p>Similar logic applies when reading data:</p> <pre><code>reader := bufio.NewReaderSize(f, 32*1024) // 32 KB buffer for input\n</code></pre> <p>Buffer size isn\u2019t something to guess at\u2014it\u2019s something to measure. The ideal size depends on too many variables to hard-code: whether you\u2019re writing to SSDs or spinning disks, how your filesystem buffers writes, how much CPU cache is available, and what else is competing for resources on the system. Profiling and benchmarking are the only reliable ways to dial it in. What works well on one setup might be suboptimal\u2014or even harmful\u2014on another.</p>"},{"location":"01-common-patterns/buffered-io/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Buffered writes and reads consistently demonstrate significant performance gains under load. Benchmarks measuring system calls, memory allocations, and CPU usage typically show that buffered I/O operations are faster and more efficient than unbuffered counterparts. For example, writing one million lines to disk might exhibit up to an order-of-magnitude improvement using <code>bufio.Writer</code> compared to direct <code>os.File.Write()</code> calls. The more structured and bursty your I/O operations, the more substantial the benefits from buffering.</p> Show the benchmark file <pre><code>package perf\n\nimport (\n    \"bufio\"\n    \"io\"\n    \"os\"\n    \"strconv\"\n    \"sync\"\n    \"testing\"\n)\n\ntype Data struct {\n    Value []byte\n}\n\nvar dataPool = sync.Pool{\n    New: func() any {\n        return &amp;Data{Value: make([]byte, 0, 32)}\n    },\n}\n\nconst N = 10000\n\nfunc writeNotBuffered(w io.Writer, count int) {\n    for i := 0; i &lt; count; i++ {\n        d := dataPool.Get().(*Data)\n        d.Value = strconv.AppendInt(d.Value[:0], int64(i), 10)\n        w.Write(d.Value)\n        w.Write([]byte(\":val\\n\"))\n        dataPool.Put(d)\n    }\n}\n\nfunc writeBuffered(w io.Writer, count int) {\n    buf := bufio.NewWriterSize(w, 16*1024)\n    for i := 0; i &lt; count; i++ {\n        d := dataPool.Get().(*Data)\n        d.Value = strconv.AppendInt(d.Value[:0], int64(i), 10)\n        buf.Write(d.Value)\n        buf.Write([]byte(\":val\\n\"))\n        dataPool.Put(d)\n    }\n    buf.Flush()\n}\n\nfunc BenchmarkWriteNotBuffered(b *testing.B) {\n    for b.Loop() {\n        f, _ := os.CreateTemp(\"\", \"nobuf\")\n        writeNotBuffered(f, N)\n        f.Close()\n        os.Remove(f.Name())\n    }\n}\n\nfunc BenchmarkWriteBuffered(b *testing.B) {\n    for b.Loop() {\n        f, _ := os.CreateTemp(\"\", \"buf\")\n        writeBuffered(f, N)\n        f.Close()\n        os.Remove(f.Name())\n    }\n}\n</code></pre> <p>Results:</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkWriteNotBuffered-14 49 23,672,792 53,773 10,007 BenchmarkWriteBuffered-14 3241 379,703 70,127 10,008"},{"location":"01-common-patterns/buffered-io/#when-to-buffer","title":"When To Buffer","text":"<p> Use buffering when:</p> <ul> <li>Performing frequent, small-sized I/O operations. Buffering groups small writes or reads into larger batches, which reduces the overhead of each individual operation.</li> <li>Reducing syscall overhead is crucial. Fewer syscalls mean lower context-switching costs and improved performance, especially in I/O-heavy applications.</li> <li>High throughput is more important than minimal latency. Buffered I/O can increase total data processed per second, even if it introduces slight delays in delivery.</li> </ul> <p> Avoid buffering when:</p> <ul> <li>Immediate data availability and low latency are critical. Buffers introduce delays by design, which can be unacceptable in real-time or interactive systems.</li> <li>Buffering excessively might lead to uncontrolled memory usage. Without limits or proper flushing, buffers can grow large and put pressure on system memory.</li> </ul>"},{"location":"01-common-patterns/comp-flags/","title":"Leveraging Compiler Optimization Flags in Go","text":"<p>When tuning Go applications for performance, most of the attention goes to runtime behavior\u2014profiling hot paths, trimming allocations, improving concurrency. But there\u2019s another layer that\u2019s easy to miss: what the Go compiler does with your code before it ever runs. The build process includes several optimization passes, and understanding how to surface or influence them can give you clearer insights into what\u2019s actually happening under the hood. It\u2019s not about tweaking obscure flags to squeeze out extra instructions\u2014it\u2019s about knowing how the compiler treats your code so you\u2019re not working against it.</p> <p>While Go doesn\u2019t expose the same granular set of compiler flags as C or Rust, it still provides useful ways to influence how your code is built\u2014especially when targeting performance, binary size, or specific environments.</p>"},{"location":"01-common-patterns/comp-flags/#why-compiler-flags-matter","title":"Why Compiler Flags Matter","text":"<p>Go's compiler (specifically <code>cmd/compile</code> and <code>cmd/link</code>) performs several default optimizations: inlining, escape analysis, dead code elimination, and more. However, there are scenarios where you can squeeze more performance or control from your build using the right flags.</p> <p>Use cases include:</p> <ul> <li>Reducing binary size for minimal containers or embedded systems  </li> <li>Building for specific architectures or OSes  </li> <li>Removing debug information for release builds  </li> <li>Disabling optimizations temporarily for easier debugging  </li> <li>Enabling experimental or unsafe performance tricks (carefully)</li> </ul>"},{"location":"01-common-patterns/comp-flags/#key-compiler-and-linker-flags","title":"Key Compiler and Linker Flags","text":""},{"location":"01-common-patterns/comp-flags/#-ldflags-s-w-strip-debug-info","title":"<code>-ldflags=\"-s -w\"</code> \u2014 Strip Debug Info","text":"<p>When you want to shrink binary size, especially in production or containers:</p> <pre><code>go build -ldflags=\"-s -w\" -o app main.go\n</code></pre> <ul> <li><code>-s</code>: Omit the symbol table</li> <li><code>-w</code>: Omit DWARF debugging information</li> </ul> <p>Why it matters: This can reduce binary size by up to 30-40%, depending on your codebase. It is useful in Docker images or when distributing binaries.</p>"},{"location":"01-common-patterns/comp-flags/#-gcflags-control-compiler-optimizations","title":"<code>-gcflags</code> \u2014 Control Compiler Optimizations","text":"<p>The <code>-gcflags</code> flag allows you to control how the compiler treats specific packages. For example, you cab disable optimizations for debugging:</p> <pre><code>go build -gcflags=\"all=-N -l\" -o app main.go\n</code></pre> <ul> <li><code>-N</code>: Disable optimizations</li> <li><code>-l</code>: Disable inlining</li> </ul> <p>When to use: During debugging sessions with Delve or similar tools. Turning off inlining and optimizations make stack traces and breakpoints more reliable.</p>"},{"location":"01-common-patterns/comp-flags/#cross-compilation-flags","title":"Cross-Compilation Flags","text":"<p>Need to build for another OS or architecture?</p> <pre><code>GOOS=linux GOARCH=arm64 go build -o app main.go\n</code></pre> <ul> <li><code>GOOS</code>, <code>GOARCH</code>: Set target OS and architecture</li> <li>Common values: <code>windows</code>, <code>darwin</code>, <code>linux</code>, <code>amd64</code>, <code>arm64</code>, <code>386</code>, <code>wasm</code></li> </ul>"},{"location":"01-common-patterns/comp-flags/#build-tags","title":"Build Tags","text":"<p>Build tags allow conditional compilation. Use <code>//go:build</code> or <code>// +build</code> in your source code to control what gets compiled in.</p> <p>Example:</p> <pre><code>//go:build debug\n\npackage main\n\nimport \"log\"\n\nfunc debugLog(msg string) {\n    log.Println(\"[DEBUG]\", msg)\n}\n</code></pre> <p>Then build with:</p> <pre><code>go build -tags=debug -o app main.go\n</code></pre>"},{"location":"01-common-patterns/comp-flags/#-ldflags-x-inject-build-time-variables","title":"<code>-ldflags=\"-X ...\"</code> \u2014 Inject Build-Time Variables","text":"<p>You can inject version numbers or metadata into your binary at build time:</p> <pre><code>// main.go\npackage main\n\nimport \"fmt\"\n\nvar version = \"dev\"\n\nfunc main() {\n    fmt.Printf(\"App version: %s\\n\", version)\n}\n</code></pre> <p>Then build with:</p> <pre><code>go build -ldflags=\"-s -w -X main.version=1.0.0\" -o app main.go\n</code></pre> <p>This sets the <code>version</code> variable at link time without modifying your source code. It's useful for embedding release versions, commit hashes, or build dates.</p>"},{"location":"01-common-patterns/comp-flags/#-extldflags-static-build-fully-static-binaries","title":"<code>-extldflags='-static'</code> \u2014 Build Fully Static Binaries","text":"<p>The <code>-extldflags '-static'</code> option passes the <code>-static</code> flag to the external system linker, instructing it to produce a fully statically linked binary.</p> <p>This is especially useful when you're using CGO and want to avoid runtime dynamic library dependencies:</p> <pre><code>CGO_ENABLED=1 GOOS=linux GOARCH=amd64 \\\nCC=gcc \\\ngo build -ldflags=\"-linkmode=external -extldflags '-static'\" -o app main.go\n</code></pre> <p>What it does:</p> <ul> <li>Statically links all C libraries into the binary</li> <li>Produces a portable, self-contained executable</li> <li>Ideal for minimal containers (like <code>scratch</code> or <code>distroless</code>)</li> </ul> <p>To go further and ensure your binary avoids relying on C library DNS resolution (such as <code>glibc</code>'s <code>getaddrinfo</code>), you can use the <code>netgo</code> build tag. This forces Go to use its pure Go implementation of the DNS resolver:</p> <pre><code>CGO_ENABLED=1 GOOS=linux GOARCH=amd64 \\\nCC=gcc \\\ngo build -tags netgo -ldflags=\"-linkmode=external -extldflags '-static'\" -o app main.go\n</code></pre> <p>This step is especially important when building for minimal container environments, where dynamic libc dependencies may not be available.</p> <p>Note</p> <p>Static linking requires static versions (<code>.a</code>) of the libraries you're using, and may not work with all C libraries by default.</p>"},{"location":"01-common-patterns/comp-flags/#example-static-build-with-libcurl-via-cgo","title":"Example: Static Build with libcurl via CGO","text":"<p>If you\u2019re using libcurl via CGO, here\u2019s how you can create a statically linked Go binary:</p> <pre><code>package main\n\n/*\n#cgo LDFLAGS: -lcurl\n#include &lt;curl/curl.h&gt;\n*/\nimport \"C\"\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"libcurl version:\", C.GoString(C.curl_version()))\n}\n</code></pre> <p>Static Build Command:</p> <pre><code>CGO_ENABLED=1 GOOS=linux GOARCH=amd64 \\\nCC=gcc \\\ngo build -tags netgo -ldflags=\"-linkmode=external -extldflags '-static'\" -o app main.go\n</code></pre> <p>Ensure the static version of libcurl (<code>libcurl.a</code>) is available on your system. You may need to install development packages or build libcurl from source with <code>--enable-static</code>.</p>"},{"location":"01-common-patterns/context/","title":"Efficient Context Management","text":"<p>Whether you're handling HTTP requests, coordinating worker goroutines, or querying external services, there's often a need to cancel in-flight operations or enforce execution deadlines. Go\u2019s <code>context</code> package is designed for precisely that\u2014it provides a consistent and thread-safe way to manage operation lifecycles, propagate metadata, and ensure resources are cleaned up promptly.</p>"},{"location":"01-common-patterns/context/#why-context-matters","title":"Why Context Matters","text":"<p>Go provides two base context constructors: <code>context.Background()</code> and <code>context.TODO()</code>.</p> <ul> <li><code>context.Background()</code> is the root context typically used at the top level of your application\u2014such as in <code>main</code>, <code>init</code>, or server setup\u2014where no existing context is available.</li> <li><code>context.TODO()</code> is a placeholder used when it\u2019s unclear which context to use, or when the surrounding code hasn\u2019t yet been fully wired for context propagation. It serves as a reminder that the context logic needs to be filled in later.</li> </ul> <p>The <code>context</code> package in Go is designed to carry deadlines, cancellation signals, and other request-scoped values across API boundaries. It's especially useful in concurrent programs where operations need to be coordinated and canceled cleanly.</p> <p>A typical context workflow begins at the entry point of a program or request\u2014like an HTTP handler, main function, or RPC server. From there, a base context is created using <code>context.Background()</code> or <code>context.TODO()</code>. This context can then be extended using constructors like:</p> <ul> <li><code>context.WithCancel(parent)</code> to create a cancelable context.</li> <li><code>context.WithTimeout(parent, duration)</code> to cancel automatically after a specific time.</li> <li><code>context.WithDeadline(parent, time)</code> for cancelling at a fixed moment.</li> <li><code>context.WithValue(parent, key, value)</code> to attach request-scoped data.</li> </ul> <p>Each of these functions returns a new context that wraps its parent. Cancellation signals, deadlines, and values are automatically propagated down the call stack. When a context is canceled\u2014either manually or by timeout\u2014any goroutines or functions listening on <code>&lt;-ctx.Done()</code> are immediately notified.</p> <p>By passing context explicitly through function parameters, you avoid hidden dependencies and gain fine-grained control over the execution lifecycle of concurrent operations.</p>"},{"location":"01-common-patterns/context/#practical-examples-of-context-usage","title":"Practical Examples of Context Usage","text":"<p>The following examples show how <code>context.Context</code> enables better control, observability, and resource management across a variety of real-world scenarios.</p>"},{"location":"01-common-patterns/context/#http-server-request-cancellation","title":"HTTP Server Request Cancellation","text":"<p>Contexts help gracefully handle cancellations when clients disconnect early. Every incoming HTTP request in Go carries a context that gets canceled if the client closes the connection. By checking <code>&lt;-ctx.Done()</code>, you can exit early instead of doing unnecessary work:</p> <pre><code>func handler(w http.ResponseWriter, req *http.Request) {\n    ctx := req.Context()\n    select {\n    case &lt;-time.After(5 * time.Second):\n        fmt.Fprintln(w, \"Response after delay\")\n    case &lt;-ctx.Done():\n        log.Println(\"Client disconnected\")\n    }\n}\n</code></pre> <p>In this example, the handler waits for either a simulated delay or cancellation. If the client closes the connection before the timeout, <code>ctx.Done()</code> is triggered, allowing the handler to clean up without writing a response.</p>"},{"location":"01-common-patterns/context/#database-operations-with-timeouts","title":"Database Operations with Timeouts","text":"<p>Contexts provide a straightforward way to enforce timeouts on database queries. Many drivers support <code>QueryContext</code> or similar methods that respect cancellation:</p> <pre><code>ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)\ndefer cancel()\n\nrows, err := db.QueryContext(ctx, \"SELECT * FROM users\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer rows.Close()\n</code></pre> <p>In this case, the context is automatically canceled if the database does not respond within two seconds. The query is aborted, and the application doesn\u2019t hang indefinitely. This helps manage resources and avoids cascading failures in high-load environments.</p>"},{"location":"01-common-patterns/context/#propagating-request-ids-for-distributed-tracing","title":"Propagating Request IDs for Distributed Tracing","text":"<p>Contexts allow passing tracing information across different layers of a distributed system. For example, a request ID generated at the edge can be attached to the context and logged or used throughout the application:</p> <pre><code>func main() {\n    ctx := context.WithValue(context.Background(), \"requestID\", \"12345\")\n    handleRequest(ctx)\n}\n\nfunc handleRequest(ctx context.Context) {\n    log.Printf(\"Handling request with ID: %v\", ctx.Value(\"requestID\"))\n}\n</code></pre> <p>In this example, <code>WithValue</code> attaches a request ID to the context. The function <code>handleRequest</code> retrieves it using <code>ctx.Value</code>, enabling consistent logging and observability without modifying function signatures. This approach is common in middleware, logging, and tracing pipelines.</p>"},{"location":"01-common-patterns/context/#concurrent-worker-management","title":"Concurrent Worker Management","text":"<p>Context provides control over multiple worker goroutines. By using <code>WithCancel</code>, you can propagate a stop signal to all workers from a central point:</p> <pre><code>ctx, cancel := context.WithCancel(context.Background())\n\nfor i := 0; i &lt; 10; i++ {\n    go worker(ctx, i)\n}\n\n// Cancel workers after some condition or signal\ncancel()\n</code></pre> <p>Each worker function should check for <code>&lt;-ctx.Done()</code> and return immediately when the context is canceled. This keeps the system responsive, avoids dangling goroutines, and allows graceful termination of parallel work.</p>"},{"location":"01-common-patterns/context/#graceful-shutdown-in-cli-tools","title":"Graceful Shutdown in CLI Tools","text":"<p>In command-line applications or long-running background processes, context simplifies OS signal handling and graceful shutdown:</p> <pre><code>ctx, stop := signal.NotifyContext(context.Background(), os.Interrupt)\ndefer stop()\n\n&lt;-ctx.Done()\nfmt.Println(\"Shutting down...\")\n</code></pre> <p>In this pattern, <code>signal.NotifyContext</code> returns a context that is canceled automatically when an interrupt signal (e.g., Ctrl+C) is received. Listening on <code>&lt;-ctx.Done()</code> allows the application to perform cleanup and exit gracefully instead of terminating abruptly.</p>"},{"location":"01-common-patterns/context/#streaming-and-real-time-data-pipelines","title":"Streaming and Real-Time Data Pipelines","text":"<p>Context is ideal for coordinating readers in streaming systems like Kafka consumers, WebSocket readers, or custom pub/sub pipelines:</p> <pre><code>func streamData(ctx context.Context, ch &lt;-chan Data) {\n    for {\n        select {\n        case &lt;-ctx.Done():\n            return\n        case data := &lt;-ch:\n            process(data)\n        }\n    }\n}\n</code></pre> <p>Here, the function processes incoming data from a channel. If the context is canceled (e.g., during shutdown or timeout), the loop breaks and the goroutine exits cleanly. This makes the system more responsive to control signals and easier to manage under load.</p>"},{"location":"01-common-patterns/context/#middleware-and-rate-limiting","title":"Middleware and Rate Limiting","text":"<p>Contexts are often used in middleware chains to enforce quotas, trace requests, or carry rate-limit decisions between layers. In a typical HTTP stack, middleware can determine whether a request is allowed based on custom logic (e.g., IP-based rate limiting or user quota checks), and attach that decision to the context so that downstream handlers can inspect it.</p> <p>Here's a simplified example of how that might work:</p> <pre><code>func rateLimitMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        // Suppose this is the result of some rate-limiting logic\n        rateLimited := true // or false depending on logic\n\n        // Embed the result into the context\n        ctx := context.WithValue(r.Context(), \"rateLimited\", rateLimited)\n\n        // Pass the updated context to the next handler\n        next.ServeHTTP(w, r.WithContext(ctx))\n    })\n}\n</code></pre> <p>In a downstream handler, you might inspect that value like so:</p> <pre><code>func handler(w http.ResponseWriter, r *http.Request) {\n    ctx := r.Context()\n    if limited, ok := ctx.Value(\"rateLimited\").(bool); ok &amp;&amp; limited {\n        http.Error(w, \"Too many requests\", http.StatusTooManyRequests)\n        return\n    }\n    fmt.Fprintln(w, \"Request accepted\")\n}\n</code></pre> <p>This pattern avoids the need for shared state between middleware and handlers. Instead, the context acts as a lightweight channel for passing metadata between layers of the request pipeline in a safe and composable way.</p>"},{"location":"01-common-patterns/context/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>There's usually nothing to benchmark directly in terms of raw performance when using <code>context.Context</code>. Its real benefit lies in improving responsiveness, avoiding wasted computation, and enabling clean cancellations. The impact shows up in reduced memory leaks, fewer stuck goroutines, and more predictable resource lifetimes\u2014metrics best observed through real-world profiling and observability tools.</p>"},{"location":"01-common-patterns/context/#best-practices-for-context-usage","title":"Best Practices for Context Usage","text":"<ul> <li>Always pass <code>context.Context</code> explicitly, typically as the first argument to a function. This makes context propagation transparent and traceable, especially across API boundaries or service layers. Don\u2019t store contexts in struct fields or global variables. Doing so can lead to stale contexts being reused unintentionally and make cancellation logic harder to reason about.</li> <li>Use 1 only for request-scoped metadata, not to pass business logic or application state. Overusing context for general-purpose data storage leads to tight coupling and makes testing and tracing harder.</li> <li>Check <code>ctx.Err()</code> to differentiate between <code>context.Canceled</code> and <code>context.DeadlineExceeded</code> where needed. This allows your application to respond appropriately\u2014for example, distinguishing between user-initiated cancellation and timeouts.</li> </ul> <p>Following these practices helps keep context usage predictable and idiomatic.</p>"},{"location":"01-common-patterns/fields-alignment/","title":"Struct Field Alignment","text":"<p>When optimizing Go programs for performance, struct layout and memory alignment often go unnoticed\u2014yet they have a measurable impact on memory usage and cache efficiency. Go automatically aligns struct fields based on platform-specific rules, inserting padding to satisfy alignment constraints. Understanding and controlling memory alignment isn\u2019t just a low-level detail\u2014it can have a real impact on how your Go programs perform, especially in tight loops or high-throughput systems. Proper alignment can reduce the overall memory footprint, make better use of CPU caches, and eliminate subtle performance penalties that add up under load.</p>"},{"location":"01-common-patterns/fields-alignment/#why-alignment-matters","title":"Why Alignment Matters","text":"<p>Modern CPUs are tuned for predictable memory access. When struct fields are misaligned or split across cache lines, the processor often has to do extra work to fetch the data. That can mean additional memory cycles, more cache misses, and slower performance overall. These costs are easy to overlook in everyday code but show up quickly in code that\u2019s sensitive to throughput or latency. In Go, struct fields are aligned according to their type requirements, and the compiler inserts padding bytes to meet these constraints. If fields are arranged without care, unnecessary padding may inflate struct size significantly, affecting memory use and bandwidth.</p> <p>Consider the following two structs:</p> <pre><code>type PoorlyAligned struct {\n    flag bool\n    count int64\n    id byte\n}\n\ntype WellAligned struct {\n    count int64\n    flag bool\n    id byte\n}\n</code></pre> <p>On a 64-bit system, <code>PoorlyAligned</code> requires 24 bytes due to the padding between fields, whereas <code>WellAligned</code> fits into 16 bytes by ordering fields from largest to smallest alignment requirement.</p>"},{"location":"01-common-patterns/fields-alignment/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>We benchmarked both struct layouts by allocating 10 million instances of each and measuring allocation time and memory usage:</p> <pre><code>func BenchmarkPoorlyAligned(b *testing.B) {\n    for b.Loop() {\n        var items = make([]PoorlyAligned, 10_000_000)\n        for j := range items {\n            items[j].count = int64(j)\n        }\n    }\n}\n\nfunc BenchmarkWellAligned(b *testing.B) {\n    for b.Loop() {\n        var items = make([]WellAligned, 10_000_000)\n        for j := range items {\n            items[j].count = int64(j)\n        }\n    }\n}\n</code></pre> <p>Benchmark Results</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op PoorlyAligned-14 177 20,095,621 240,001,029 1 WellAligned-14 186 19,265,714 160,006,148 1 <p>In a test with 10 million structs, the <code>WellAligned</code> version used 80MB less memory than its poorly aligned counterpart\u2014and it also ran a bit faster. This isn\u2019t just about saving RAM; it shows how struct layout directly affects allocation behavior and memory bandwidth. When you\u2019re working with large volumes of data or performance-critical paths, reordering fields for better alignment can lead to measurable gains with minimal effort.</p>"},{"location":"01-common-patterns/fields-alignment/#avoiding-false-sharing-in-concurrent-workloads","title":"Avoiding False Sharing in Concurrent Workloads","text":"<p>In addition to memory layout efficiency, struct alignment also plays a crucial role in concurrent systems. When multiple goroutines access different fields of the same struct that reside on the same CPU cache line, they may suffer from false sharing\u2014where changes to one field cause invalidations in the other, even if logically unrelated.</p> <p>On modern CPUs, a typical cache line is 64 bytes wide. When a struct is accessed in memory, the CPU loads the entire cache line that contains it, not just the specific field. This means that two unrelated fields within the same 64-byte block will both reside in the same line\u2014even if they are used independently by separate goroutines. If one goroutine writes to its field, the cache line becomes invalidated and must be reloaded on the other core, leading to degraded performance due to false sharing.</p> <p>To illustrate, we compared two structs\u2014one vulnerable to false sharing, and another with padding to separate fields across cache lines:</p> <pre><code>type SharedCounterBad struct {\n    a int64\n    b int64\n}\n\ntype SharedCounterGood struct {\n    a int64\n    _ [56]byte // Padding to prevent a and b from sharing a cache line\n    b int64\n}\n</code></pre> <p>Each field is incremented by a separate goroutine 1 million times:</p> <pre><code>func BenchmarkFalseSharing(b *testing.B) {\n    var c SharedCounterBad  // (1)\n    var wg sync.WaitGroup\n\n    for b.Loop() {\n        wg.Add(2)\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.a++\n            }\n            wg.Done()\n        }()\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.b++\n            }\n            wg.Done()\n        }()\n        wg.Wait()\n    }\n}\n</code></pre> <ol> <li><code>FalseSharing</code> and <code>NoFalseSharing</code> benchmarks are identical, except we will use <code>SharedCounterGood</code> for the <code>NoFalseSharing</code> benchmark.</li> </ol> <p>Benchmark Results:</p> Benchmark Time per op (ns) Bytes per op Allocs per op FalseSharing 996,234 55 2 NoFalseSharing 958,180 58 2 <p>Placing padding between the two fields prevented false sharing, resulting in a measurable performance improvement. The version with padding completed ~3.8% faster (the value could vary between re-runs from 3% to 6%), which can make a difference in tight concurrent loops or high-frequency counters. It also shows how false sharing may unpredictably affect memory use due to invalidation overhead.</p> Show the complete benchmark file <pre><code>package perf\n\nimport (\n    \"sync\"\n    \"testing\"\n)\n\n// types-simple-start\ntype PoorlyAligned struct {\n    flag bool\n    count int64\n    id byte\n}\n\ntype WellAligned struct {\n    count int64\n    flag bool\n    id byte\n}\n// types-simple-end\n\n// simple-start\nfunc BenchmarkPoorlyAligned(b *testing.B) {\n    for b.Loop() {\n        var items = make([]PoorlyAligned, 10_000_000)\n        for j := range items {\n            items[j].count = int64(j)\n        }\n    }\n}\n\nfunc BenchmarkWellAligned(b *testing.B) {\n    for b.Loop() {\n        var items = make([]WellAligned, 10_000_000)\n        for j := range items {\n            items[j].count = int64(j)\n        }\n    }\n}\n// simple-end\n\n\n// types-shared-start\ntype SharedCounterBad struct {\n    a int64\n    b int64\n}\n\ntype SharedCounterGood struct {\n    a int64\n    _ [56]byte // Padding to prevent a and b from sharing a cache line\n    b int64\n}\n// types-shared-end\n\n// shared-start\n\nfunc BenchmarkFalseSharing(b *testing.B) {\n    var c SharedCounterBad  // (1)\n    var wg sync.WaitGroup\n\n    for b.Loop() {\n        wg.Add(2)\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.a++\n            }\n            wg.Done()\n        }()\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.b++\n            }\n            wg.Done()\n        }()\n        wg.Wait()\n    }\n}\n// shared-end\n\nfunc BenchmarkNoFalseSharing(b *testing.B) {\n    var c SharedCounterGood\n    var wg sync.WaitGroup\n\n    for b.Loop() {\n        wg.Add(2)\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.a++\n            }\n            wg.Done()\n        }()\n        go func() {\n            for i := 0; i &lt; 1_000_000; i++ {\n                c.b++\n            }\n            wg.Done()\n        }()\n        wg.Wait()\n    }\n}\n</code></pre>"},{"location":"01-common-patterns/fields-alignment/#when-to-align-structs","title":"When To Align Structs","text":"<p> Always align structs. It's free to implement and often leads to better memory efficiency without changing any logic\u2014only field order needs to be adjusted.</p> <p>Guidelines for struct alignment:</p> <ul> <li>Order fields from largest to smallest. Starting with larger fields helps the compiler avoid inserting padding to meet alignment requirements. Smaller fields can fill in the gaps naturally.</li> <li>Group fields of the same size together. This lets the compiler pack them more efficiently and minimizes wasted space.</li> <li>Insert padding intentionally when needed. In concurrent code, separating fields that are accessed by different goroutines can prevent false sharing\u2014a subtle but costly issue where multiple goroutines compete over the same cache line.</li> <li>Avoid interleaving small and large fields. Mixing sizes leads to inefficient memory usage due to extra alignment padding between fields.</li> <li>Use the fieldalignment linter to verify. This tool helps catch suboptimal layouts automatically during development.</li> </ul>"},{"location":"01-common-patterns/gc/","title":"Memory Efficiency: Mastering Go\u2019s Garbage Collector","text":"<p>Memory management in Go is automated\u2014but it\u2019s not invisible. Every allocation you make contributes to GC workload. The more frequently objects are created and discarded, the more work the runtime has to do reclaiming memory.</p> <p>This becomes especially relevant in systems prioritizing low latency, predictable resource usage, or high throughput. Tuning your allocation patterns and leveraging newer features like weak references can help reduce pressure on the GC without adding complexity to your code.</p>"},{"location":"01-common-patterns/gc/#how-gos-garbage-collector-works","title":"How Go's Garbage Collector Works","text":"<p>Info</p> <p>Highly encourage you to read the official A Guide to the Go Garbage Collector! The document provides a detailed description of multiple Go's GC internals.</p> <p>Go uses a non-generational, concurrent, tri-color mark-and-sweep garbage collector. Here's what that means in practice and how it's implemented.</p>"},{"location":"01-common-patterns/gc/#non-generational","title":"Non-generational","text":"<p>Many modern GCs, like those in the JVM or .NET CLR, divide memory into generations (young and old) under the assumption that most objects die young. These collectors focus on the young generation, which leads to shorter collection cycles.</p> <p>Go\u2019s GC takes a different approach. It treats all objects equally\u2014no generational segmentation\u2014not because generational GC conflicts with short pause times or concurrent scanning, but because it hasn\u2019t shown clear, consistent benefits in real-world Go programs with the designs tried so far. This choice avoids the complexity of promotion logic and specialized memory regions. While it can mean scanning more objects overall, this cost is mitigated by concurrent execution and efficient write barriers.</p>"},{"location":"01-common-patterns/gc/#concurrent","title":"Concurrent","text":"<p>Go\u2019s GC runs concurrently with your application, which means it does most of its work without stopping the world. Concurrency is implemented using multiple phases that interleave with normal program execution:</p> <p>Even though Go\u2019s garbage collector is mostly concurrent, it still requires brief Stop-The-World (STW) pauses at several points to maintain correctness. These pauses are kept extremely short\u2014typically under 100 microseconds\u2014even with large heaps and hundreds of goroutines.</p> <p>STW is essential for ensuring that memory structures are not mutated while the GC analyzes them. In most applications, these pauses are imperceptible. However, even sub-millisecond pauses in latency-sensitive systems can be significant\u2014so understanding and monitoring STW behavior becomes important when optimizing for tail latencies or jitter.</p> <ul> <li>STW Start Phase: The application is briefly paused to initiate GC. The runtime scans stacks, globals, and root objects.</li> <li>Concurrent Mark Phase: The garbage collector traverses the heap, marking all reachable objects while the program continues running. This is the heaviest phase in terms of work but runs concurrently to avoid long stop-the-world pauses.</li> <li>STW Mark Termination: Once marking is mostly complete, the GC briefly pauses the program to finish any remaining work and ensure the heap is in a consistent state before sweeping begins. This pause is typically very short\u2014measured in microseconds.</li> <li>Concurrent Sweep Phase: The GC reclaims memory from unreachable (white) objects and returns it to the heap for reuse, all while your program continues running.</li> </ul> <p>Write barriers ensure correctness while the application mutates objects during concurrent marking. These barriers help track references created or modified mid-scan so the GC doesn\u2019t miss them.</p>"},{"location":"01-common-patterns/gc/#tri-color-mark-and-sweep","title":"Tri-color Mark and Sweep","text":"<p>The tri-color algorithm breaks the heap into three working sets during garbage collection:</p> <ul> <li>White: Objects that haven\u2019t been reached\u2014if they stay white, they\u2019ll be collected.</li> <li>Grey: Objects that have been discovered (i.e., marked as reachable) but haven\u2019t had their references scanned yet.</li> <li>Black: Objects that are both reachable and fully scanned\u2014they\u2019re retained and don\u2019t need further processing.</li> </ul> <p>Garbage collection starts by marking all root objects (stack, globals, etc.) grey. It then walks the grey set: for each object, it scans its fields. Any referenced objects that are still white are added to the grey set. Once an object\u2019s references are fully processed, it\u2019s marked black.</p> <p>When no grey objects remain, anything still white is unreachable and gets cleaned up during the sweep phase. This model ensures that no live object is accidentally collected\u2014even if references change mid-scan\u2014thanks to Go\u2019s write barriers that maintain the algorithm\u2019s core invariants.</p> <p>A key optimization is incremental marking: Go spreads out GC work to avoid long pauses, supported by precise stack scanning and conservative write barriers. The use of concurrent sweeping further reduces latency, allowing memory to be reclaimed without halting execution.</p> <p>This design gives Go a GC that\u2019s safe, fast, and friendly to server workloads with large heaps and many cores.</p>"},{"location":"01-common-patterns/gc/#gc-tuning-gogc","title":"GC Tuning: GOGC","text":"<p>Go\u2019s garbage collector is tuned to deliver good performance without manual configuration. The default <code>GOGC</code> setting typically strikes the right balance between memory consumption and CPU effort, adapting well across a wide range of workloads. In most cases, manually tweaking it offers little benefit\u2014and in many, it actually makes things worse by increasing either pause times or memory pressure. Unless you\u2019ve profiled a specific bottleneck and understand the trade-offs, it\u2019s usually best to leave <code>GOGC</code> alone.</p> <p>That said, there are specific cases where tuning <code>GOGC</code> can yield significant gains. For example, Uber implemented dynamic GC tuning across their Go services to reduce CPU usage and saved tens of thousands of cores in the process. Their approach relied on profiling, metric collection, and automation to safely adjust GC behavior based on actual memory pressure and workload characteristics.</p> <p>Another unusual case is from Cloudflare. They profiled a high-concurrency cryptographic workload and found that Go\u2019s GC became a bottleneck as goroutines increased. Their application produced minimal garbage, yet GC overhead grew with concurrency. By tuning GOGC to a much higher value\u2014specifically 11300\u2014they significantly reduced GC frequency and improved throughput, achieving over 22\u00d7 performance gains compared to the single-core baseline. This case highlights how allowing more heap growth in CPU-bound and low-allocation scenarios can yield major improvements.</p> <p>So, if you decide to tune the garbage collector, be methodical:</p> <ul> <li>Always profile first. Use tools like <code>pprof</code> to confirm that GC activity is a bottleneck.</li> <li>Change settings incrementally. For example, increasing <code>GOGC</code> from 100 to 150 means the GC will run less frequently, using less CPU but more memory.</li> <li>Verify impact. After tuning, validate with profiling data that the change had a positive effect. Without that confirmation, it's easy to make things worse.</li> </ul> <pre><code>GOGC=100  # Default: GC runs when heap grows 100% since last collection\nGOGC=off  # Disables GC (use only in special cases like short-lived CLI tools)\n</code></pre>"},{"location":"01-common-patterns/gc/#memory-limiting-with-gomemlimit","title":"Memory Limiting with <code>GOMEMLIMIT</code>","text":"<p>In addition to <code>GOGC</code>, Go provides <code>GOMEMLIMIT</code>\u2014a soft memory limit that caps the total heap size the runtime will try to stay under. This allows you to explicitly control memory growth, especially useful in environments like containers or systems with strict memory budgets.</p> <p>Why is this helpful? In containerized environments (like Kubernetes), memory limits are typically enforced at the OS or orchestrator level. If your application exceeds its memory quota, the OOM killer may abruptly terminate the container. Go's GC isn't aware of those limits by default.</p> <p>Setting a <code>GOMEMLIMIT</code> helps prevent this. For example, if your container has a 512MiB memory limit, you might set:</p> <pre><code>GOMEMLIMIT=400MiB\n</code></pre> <p>This buffer gives the Go runtime room to act before reaching the hard system-imposed memory cap. It allows the garbage collector to become more aggressive as total memory usage grows, reducing the chances of the process being killed due to an out-of-memory condition. It also leaves space for non-heap allocations\u2014like goroutine stacks, OS threads, and other internal runtime structures\u2014which don\u2019t count toward heap size but still consume real memory.</p> <p>You can also set the limit programmatically:</p> <pre><code>import \"runtime/debug\"\n\ndebug.SetMemoryLimit(2 &lt;&lt; 30) // 2 GiB\n</code></pre> <p>The GC will become more aggressive as heap usage nears the limit, which can increase CPU load. Be careful not to set the limit too low\u2014especially if your application maintains a large live set of objects\u2014or you may trigger excessive GC cycles.</p> <p>While <code>GOGC</code> controls how frequently the GC runs based on heap growth, <code>GOMEMLIMIT</code> constrains the heap size itself. The two can be combined for more precise control:</p> <pre><code>GOGC=100 GOMEMLIMIT=4GiB ./your-service\n</code></pre> <p>This tells the GC to operate with the default growth ratio and to start collecting sooner if heap usage nears 4 GiB.</p>"},{"location":"01-common-patterns/gc/#gomemlimitx-and-gogcoff-configuration","title":"GOMEMLIMIT=X and GOGC=off configuration","text":"<p>In scenarios where memory availability is fixed and predictable\u2014such as within containers or VMs, you can use these two variables together:</p> <ul> <li><code>GOMEMLIMIT=X</code> tells the runtime to aim for a specific memory ceiling. For example, <code>GOMEMLIMIT=2GiB</code> will trigger garbage collection when total memory usage nears 2 GiB.</li> <li><code>GOGC=off</code> disables the default GC pacing algorithm, so garbage collection only runs when the memory limit is hit.</li> </ul> <p>This configuration maximizes memory usage efficiency and avoids the overhead of frequent GC cycles. It's especially effective in high-throughput or latency-sensitive systems where predictable memory usage matters.</p> <p>Example:</p> <pre><code>GOMEMLIMIT=2GiB GOGC=off ./my-app\n</code></pre> <p>With this setup, memory usage grows freely until the 2 GiB threshold is reached. At that point, Go performs a full garbage collection pass.</p> <p>Warning</p> <ul> <li>Always benchmark with your real workload. Disabling automatic GC can backfire if your application produces a lot of short-lived allocations.</li> <li>Monitor memory pressure and GC pause times using <code>runtime.ReadMemStats</code> or <code>pprof</code>.</li> <li>This approach works best when your memory usage patterns are well understood and stable.</li> </ul>"},{"location":"01-common-patterns/gc/#practical-strategies-for-reducing-gc-pressure","title":"Practical Strategies for Reducing GC Pressure","text":""},{"location":"01-common-patterns/gc/#prefer-stack-allocation","title":"Prefer Stack Allocation","text":"<p>Go allocates variables on the stack whenever possible. Avoid escaping variables to the heap:</p> <pre><code>// BAD: returns pointer to heap-allocated struct\nfunc newUser(name string) *User {\n    return &amp;User{Name: name}  // escapes to heap\n}\n\n// BETTER: use value types if pointer is unnecessary\nfunc printUser(u User) {\n    fmt.Println(u.Name)\n}\n</code></pre> <p>Use <code>go build -gcflags=\"-m\"</code> to view escape analysis diagnostics. See Stack Allocations and Escape Analysis for more details.</p>"},{"location":"01-common-patterns/gc/#use-syncpool-for-short-lived-objects","title":"Use sync.Pool for Short-Lived Objects","text":"<p><code>sync.Pool</code> is ideal for temporary, reusable allocations that are expensive to GC.</p> <pre><code>var bufPool = sync.Pool{\n    New: func() any { return new(bytes.Buffer) },\n}\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    buf := bufPool.Get().(*bytes.Buffer)\n    buf.Reset()\n    defer bufPool.Put(buf)\n\n    // Use buf...\n}\n</code></pre> <p>See Object Pooling for more details.</p>"},{"location":"01-common-patterns/gc/#batch-allocations","title":"Batch Allocations","text":"<p>Group allocations into fewer objects to reduce GC pressure.</p> <pre><code>// Instead of allocating many small structs, allocate a slice of structs\nusers := make([]User, 0, 1000)  // single large allocation\n</code></pre> <p>See Memory Preallocation for more details.</p>"},{"location":"01-common-patterns/gc/#weak-references-in-go","title":"Weak References in Go","text":"<p>Go 1.24 added the <code>weak</code> package, providing a standardized way to create weak references\u2014pointers that don\u2019t keep their target objects alive. In garbage-collected systems like Go, strong references extend an object\u2019s lifetime: as long as something points to it, it won\u2019t be collected. That\u2019s usually what you want, but in structures like caches, deduplication maps, or object graphs, this can lead to memory staying alive much longer than intended. Weak references solve that by allowing you to refer to an object without blocking the GC from reclaiming it when nothing else is using it.</p> <p>A weak reference, by contrast, tells the garbage collector: \u201cyou can collect this object if nothing else is strongly referencing it.\u201d This pattern is important for building memory-sensitive data structures that should not interfere with garbage collection.</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"runtime\"\n    \"weak\"\n)\n\ntype Data struct {\n    Value string\n}\n\nfunc main() {\n    data := &amp;Data{Value: \"Important\"}\n    wp := weak.Make(data) // create weak pointer\n\n    fmt.Println(\"Original:\", wp.Value().Value)\n\n    data = nil // remove strong reference\n    runtime.GC()\n\n    if v := wp.Value(); v != nil {\n        fmt.Println(\"Still alive:\", v.Value)\n    } else {\n        fmt.Println(\"Data has been collected\")\n    }\n}\n</code></pre> <pre><code>Original: Important\nData has been collected\n</code></pre> <p>In this example, <code>wp</code> holds a weak reference to a <code>Data</code> object. After the strong reference (<code>data</code>) goes out of scope and the garbage collector runs, the <code>Data</code> may be collected\u2014at which point <code>wp.Value()</code> will return nil. This pattern is especially useful in memory-sensitive contexts like caches or canonicalization maps, where you want to avoid artificially extending object lifetimes. Always check the result of <code>Value()</code> before using it, since the target may have been reclaimed.</p>"},{"location":"01-common-patterns/gc/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>It's tempting to rely on synthetic benchmarks to evaluate the performance of Go's garbage collector, but generic benchmarks rarely capture the nuances of real-world workloads. Memory behavior is highly dependent on allocation patterns, object lifetimes, concurrency, and how frequently short-lived versus long-lived data structures are used.</p> <p>For example, the impact of GC in a CPU-bound microservice that maintains large in-memory indexes will differ dramatically from an I/O-heavy API server with minimal heap usage. As such, tuning decisions should always be informed by your application's profiling data.</p> <p>We cover targeted use cases and their GC performance trade-offs in more focused articles:</p> <ul> <li>Object Pooling: Reducing allocation churn using <code>sync.Pool</code></li> <li>Stack Allocations and Escape Analysis: Minimizing heap usage by keeping values on the stack</li> <li>Memory Preallocation: Avoiding unnecessary growth of slices and maps</li> </ul> <p>When applied to the right context, these techniques can make a measurable difference, but they don\u2019t lend themselves to one-size-fits-all benchmarks.</p>"},{"location":"01-common-patterns/immutable-data/","title":"Immutable Data Sharing","text":"<p>One common source of slowdown in high-performance Go programs is the way shared data is accessed under concurrency. The usual tools\u2014mutexes and channels\u2014work well, but they\u2019re not free. Mutexes can become choke points if many goroutines try to grab the same lock. Channels, while elegant for coordination, can introduce blocking and make control flow harder to reason about. Both require careful use: it\u2019s easy to introduce subtle bugs or unexpected performance issues if synchronization isn\u2019t tight.</p> <p>A powerful alternative is immutable data sharing. Instead of protecting data with locks, you design your system so that shared data is never mutated after it's created. This minimizes contention and simplifies reasoning about your program.</p>"},{"location":"01-common-patterns/immutable-data/#why-immutable-data","title":"Why Immutable Data?","text":"<p>Immutability brings several advantages to concurrent programs:</p> <ul> <li>No locks needed: Multiple goroutines can safely read immutable data without synchronization.</li> <li>Easier reasoning: If data can't change, you avoid entire classes of race conditions.</li> <li>Copy-on-write optimizations: You can create new versions of a structure without altering the original, which is useful for config reloading or versioning a state.</li> </ul>"},{"location":"01-common-patterns/immutable-data/#practical-example-shared-config","title":"Practical Example: Shared Config","text":"<p>Imagine you have a long-running service that periodically reloads its configuration from a disk or a remote source. Multiple goroutines read this configuration to make decisions.</p> <p>Here's how immutable data helps:</p>"},{"location":"01-common-patterns/immutable-data/#step-1-define-the-config-struct","title":"Step 1: Define the Config Struct","text":"<pre><code>// config.go\ntype Config struct {\n    LogLevel string\n    Timeout  time.Duration\n    Features map[string]bool // This needs attention!\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#step-2-ensure-deep-immutability","title":"Step 2: Ensure Deep Immutability","text":"<p>Maps and slices in Go are reference types. Even if the Config struct isn't changed, someone could accidentally mutate a shared map. To prevent this, we make defensive copies:</p> <pre><code>func NewConfig(logLevel string, timeout time.Duration, features map[string]bool) *Config {\n    copiedFeatures := make(map[string]bool, len(features))\n    for k, v := range features {\n        copiedFeatures[k] = v\n    }\n\n    return &amp;Config{\n        LogLevel: logLevel,\n        Timeout:  timeout,\n        Features: copiedFeatures,\n    }\n}\n</code></pre> <p>Now, every config instance is self-contained and safe to share.</p>"},{"location":"01-common-patterns/immutable-data/#step-3-atomic-swapping","title":"Step 3: Atomic Swapping","text":"<p>Use <code>atomic.Value</code> to store and safely update the current config.</p> <pre><code>var currentConfig atomic.Pointer[Config]\n\nfunc LoadInitialConfig() {\n    cfg := NewConfig(\"info\", 5*time.Second, map[string]bool{\"beta\": true})\n    currentConfig.Store(cfg)\n}\n\nfunc GetConfig() *Config {\n    return currentConfig.Load()\n}\n</code></pre> <p>Now all goroutines can safely call <code>GetConfig()</code> with no locks. When the config is reloaded, you just <code>Store</code> a new immutable copy.</p>"},{"location":"01-common-patterns/immutable-data/#step-4-using-it-in-handlers","title":"Step 4: Using It in Handlers","text":"<pre><code>func handler(w http.ResponseWriter, r *http.Request) {\n    cfg := GetConfig()\n    if cfg.Features[\"beta\"] {\n        // Enable beta path\n    }\n    // Use cfg.Timeout, cfg.LogLevel, etc.\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#practical-example-immutable-routing-table","title":"Practical Example: Immutable Routing Table","text":"<p>Suppose you're building a lightweight reverse proxy or API gateway and must route incoming requests based on path or host. The routing table is read thousands of times per second and updated only occasionally (e.g., from a config file or service discovery).</p>"},{"location":"01-common-patterns/immutable-data/#step-1-define-route-structs","title":"Step 1: Define Route Structs","text":"<pre><code>type Route struct {\n    Path    string\n    Backend string\n}\n\ntype RoutingTable struct {\n    Routes []Route\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#step-2-build-immutable-version","title":"Step 2: Build Immutable Version","text":"<p>To ensure immutability, we deep-copy the slice of routes when constructing a new routing table.</p> <pre><code>func NewRoutingTable(routes []Route) *RoutingTable {\n    copied := make([]Route, len(routes))\n    copy(copied, routes)\n    return &amp;RoutingTable{Routes: copied}\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#step-3-store-it-atomically","title":"Step 3: Store It Atomically","text":"<pre><code>var currentRoutes atomic.Pointer[RoutingTable]\n\nfunc LoadInitialRoutes() {\n    table := NewRoutingTable([]Route{\n        {Path: \"/api\", Backend: \"http://api.internal\"},\n        {Path: \"/admin\", Backend: \"http://admin.internal\"},\n    })\n    currentRoutes.Store(table)\n}\n\nfunc GetRoutingTable() *RoutingTable {\n    return currentRoutes.Load()\n}\n</code></pre>"},{"location":"01-common-patterns/immutable-data/#step-4-route-requests-concurrently","title":"Step 4: Route Requests Concurrently","text":"<pre><code>func routeRequest(path string) string {\n    table := GetRoutingTable()\n    for _, route := range table.Routes {\n        if strings.HasPrefix(path, route.Path) {\n            return route.Backend\n        }\n    }\n    return \"\"\n}\n</code></pre> <p>Now, your routing logic can scale safely under load with zero locking overhead.</p>"},{"location":"01-common-patterns/immutable-data/#scaling-immutable-routing-tables","title":"Scaling Immutable Routing Tables","text":"<p>As systems grow, routing tables can expand to hundreds or even thousands of entries. While immutability brings clear benefits\u2014safe concurrent access, predictable behavior\u2014it becomes costly if every update means copying the entire structure. At some point, rebuilding the whole table for each minor change doesn\u2019t scale.</p> <p>To keep immutability without paying for full reconstruction on every update, the design needs to evolve. There are several ways to do this\u2014each preserving the core benefits while reducing overhead.</p>"},{"location":"01-common-patterns/immutable-data/#scenario-1-segmented-routing","title":"Scenario 1: Segmented Routing","text":"<p>Imagine a multi-tenant system where each customer has their own set of routing rules. Instead of one giant slice of routes, you can split them into a map:</p> <pre><code>type MultiTable struct {\n    Tables map[string]RoutingTable // key = tenant ID\n}\n</code></pre> <p>If only customer \"acme\" updates their rules, you clone just that slice and update the map. Then you atomically swap in a new version of the full map. All other tenants continue using their existing, untouched routing tables.</p> <p>This approach reduces memory pressure and speeds up updates without losing immutability. It also isolates blast radius: a broken rule set in one segment doesn\u2019t affect others.</p>"},{"location":"01-common-patterns/immutable-data/#scenario-2-indexed-routing-table","title":"Scenario 2: Indexed Routing Table","text":"<p>Let\u2019s say your router matches by exact path, and lookup speed is critical. You can use a <code>map[string]RouteHandler</code> as an index:</p> <pre><code>type RouteIndex map[string]RouteHandler\n</code></pre> <p>When a new path is added, clone the current map, add the new route, and publish the new version. Because maps are shallow, this is fast for moderate numbers of routes. Reads are constant time, and updates are efficient because only a small part of the structure changes.</p>"},{"location":"01-common-patterns/immutable-data/#scenario-3-hybrid-staging-and-publishing","title":"Scenario 3: Hybrid Staging and Publishing","text":"<p>Suppose you\u2019re doing a batch update \u2014 maybe reading hundreds of routes from a database. Instead of rebuilding live, you keep a mutable staging area:</p> <pre><code>var mu sync.Mutex\nvar stagingRoutes []Route\n</code></pre> <p>You load and manipulate data in staging under a mutex, then convert to an immutable <code>RoutingTable</code> and store it atomically. This lets you safely prepare complex changes without locking readers or affecting live traffic.</p>"},{"location":"01-common-patterns/immutable-data/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Benchmarking immutable data sharing in real-world systems is difficult to do in a generic, meaningful way. Factors like structure size, read/write ratio, and memory layout all heavily influence results.</p> <p>Rather than presenting artificial benchmarks here, we recommend reviewing the results in the Atomic Operations and Synchronization Primitives article. Those benchmarks clearly illustrate the potential performance benefits of using atomic.Value over traditional synchronization primitives like sync.RWMutex, especially in highly concurrent read scenarios.</p>"},{"location":"01-common-patterns/immutable-data/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p> Immutable data sharing is ideal when:</p> <ul> <li> <p>The data is read-heavy and write-light (e.g., configuration, feature flags, global mappings). This works well because the cost of creating new immutable versions is amortized over many reads, and avoiding locks provides a performance boost.</p> </li> <li> <p>You want to minimize locking without sacrificing safety. By sharing read-only data, you remove the need for mutexes or coordination, reducing the chances of deadlocks or race conditions.</p> </li> <li> <p>You can tolerate minor delays between update and read (eventual consistency). Since data updates are not coordinated with readers, there might be a small delay before all goroutines see the new version. If exact timing isn't critical, this tradeoff simplifies your concurrency model.</p> </li> </ul> <p> It\u2019s less suitable when updates must be transactional across multiple pieces of data or happen frequently. In those cases, the cost of repeated copying or lack of coordination can outweigh the benefits.</p>"},{"location":"01-common-patterns/interface-boxing/","title":"Avoiding Interface Boxing","text":"<p>Go\u2019s interfaces make it easy to write flexible, decoupled code. But behind that convenience is a detail that can trip up performance: when a concrete value is assigned to an interface, Go wraps it in a hidden structure\u2014a process called interface boxing.</p> <p>In many cases, boxing is harmless. But in performance-sensitive code\u2014like tight loops, hot paths, or high-throughput services\u2014it can introduce hidden heap allocations, extra memory copying, and added pressure on the garbage collector. These effects often go unnoticed during development, only showing up later as latency spikes or memory bloat.</p>"},{"location":"01-common-patterns/interface-boxing/#what-is-interface-boxing","title":"What is Interface Boxing?","text":"<p>Interface boxing refers to the process of converting a concrete value to an interface type. In Go, an interface value is internally represented as two words:</p> <ul> <li>A type descriptor, which holds information about the concrete type (its identity and method set).</li> <li>A data pointer, which points to the actual value being stored.</li> </ul> <p>When you assign a value to an interface variable, Go creates this two-part structure. If the value is a non-pointer type\u2014like a struct or primitive\u2014and is not already on the heap, Go may allocate a copy of it on the heap to satisfy the interface assignment. This behavior is especially relevant when working with large values or when storing items in a slice of interfaces, where each element gets individually boxed. These implicit allocations can add up and are a common source of hidden memory pressure in Go programs.</p> <p>Here\u2019s a simple example:</p> <pre><code>var i interface{}\ni = 42\n</code></pre> <p>In this case, the integer <code>42</code> is boxed into an interface: Go stores the type information (<code>int</code>) and a copy of the value <code>42</code>. This is inexpensive for small values like <code>int</code>, but for large structs, the cost becomes non-trivial.</p> <p>Another example:</p> <pre><code>type Shape interface {\n    Area() float64\n}\n\ntype Square struct {\n    Size float64\n}\n\nfunc (s Square) Area() float64 { return s.Size * s.Size }\n\nfunc main() {\n    var shapes []Shape\n    for i := 0; i &lt; 1000; i++ {\n        s := Square{Size: float64(i)}\n        shapes = append(shapes, s) // boxing occurs here\n    }\n}\n</code></pre> <p>Warning</p> <p>Pay attention to this code! In this example, even though <code>shapes</code> is a slice of interfaces, each <code>Square</code> value is copied into an interface when appended to <code>shapes</code>. If <code>Square</code> were a large struct, this would introduce 1000 allocations and large memory copying.</p> <p>To avoid that, you could pass pointers:</p> <pre><code>        shapes = append(shapes, &amp;s) // avoids large struct copy\n</code></pre> <p>This way, only an 8-byte pointer is stored in the interface, reducing both allocation size and copying overhead.</p>"},{"location":"01-common-patterns/interface-boxing/#why-it-matters","title":"Why It Matters","text":"<p>In tight loops or high-throughput paths, such as unmarshalling JSON, rendering templates, or processing large collections, interface boxing can degrade performance by triggering unnecessary heap allocations and increasing GC pressure. This overhead is especially costly in systems with high concurrency or real-time responsiveness constraints.</p> <p>Boxing can also make profiling and benchmarking misleading, since allocations attributed to innocuous-looking lines may actually stem from implicit conversions to interfaces.</p>"},{"location":"01-common-patterns/interface-boxing/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>For the benchmarking we will define an interface and a struct with a significant payload that implements the interface.</p> <pre><code>type Worker interface {\n    Work()\n}\n\ntype LargeJob struct {\n    payload [4096]byte\n}\n\nfunc (LargeJob) Work() {}\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#boxing-large-structs","title":"Boxing Large Structs","text":"<p>To demonstrate the real impact of boxing large values vs. pointers, we benchmarked the cost of assigning 1,000 large structs to an interface slice:</p> <pre><code>func BenchmarkBoxedLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs = jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            var job LargeJob\n            jobs = append(jobs, job)\n        }\n    }\n}\n\nfunc BenchmarkPointerLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs := jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            job := &amp;LargeJob{}\n            jobs = append(jobs, job)\n        }\n    }\n}\n</code></pre> <p>Benchmark Results</p> Benchmark Time per op (ns) Bytes per op Allocs per op BoxedLargeSliceGrowth 404,649 ~4.13 MB 1011 PointerLargeSliceGrowth 340,549 ~4.13 MB 1011 <p>Boxing large values is significantly slower\u2014about 19% in this case\u2014due to the cost of copying the entire 4KB struct for each interface assignment. Boxing a pointer, however, avoids that cost and keeps the copy small (just 8 bytes). While both approaches allocate the same overall memory (since all values escape to the heap), pointer boxing has clear performance advantages under pressure.</p>"},{"location":"01-common-patterns/interface-boxing/#passing-to-a-function-that-accepts-an-interface","title":"Passing to a Function That Accepts an Interface","text":"<p>Another common source of boxing is when a large value is passed directly to a function that accepts an interface. Even without storing to a slice, boxing will occur at the call site.</p> <pre><code>var sink Worker\n\nfunc call(w Worker) {\n    sink = w\n}\n\nfunc BenchmarkCallWithValue(b *testing.B) {\n    for b.Loop() {\n        var j LargeJob\n        call(j)\n    }\n}\n\nfunc BenchmarkCallWithPointer(b *testing.B) {\n    for b.Loop() {\n        j := &amp;LargeJob{}\n        call(j)\n    }\n}\n</code></pre> <p>Benchmark Results</p> Benchmark ns/op B/op allocs/op CallWithValue 422.5 4096 1 CallWithPointer 379.9 4096 1 <p>Passing a value to a function expecting an interface causes boxing, copying the full struct and allocating it on the heap. In our benchmark, this results in approximately 11% higher CPU cost compared to using a pointer. Passing a pointer avoids copying the struct, reduces memory movement, and results in smaller, more cache-friendly interface values, making it the more efficient choice in performance-sensitive scenarios.</p> Show the complete benchmark file <pre><code>package perf\n\nimport \"testing\"\n\n\n// interface-start\n\ntype Worker interface {\n    Work()\n}\n\ntype LargeJob struct {\n    payload [4096]byte\n}\n\nfunc (LargeJob) Work() {}\n// interface-end\n\n// bench-slice-start\nfunc BenchmarkBoxedLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs = jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            var job LargeJob\n            jobs = append(jobs, job)\n        }\n    }\n}\n\nfunc BenchmarkPointerLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs := jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            job := &amp;LargeJob{}\n            jobs = append(jobs, job)\n        }\n    }\n}\n// bench-slice-end\n\n// bench-call-start\nvar sink Worker\n\nfunc call(w Worker) {\n    sink = w\n}\n\nfunc BenchmarkCallWithValue(b *testing.B) {\n    for b.Loop() {\n        var j LargeJob\n        call(j)\n    }\n}\n\nfunc BenchmarkCallWithPointer(b *testing.B) {\n    for b.Loop() {\n        j := &amp;LargeJob{}\n        call(j)\n    }\n}\n// bench-call-end\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#when-interface-boxing-is-acceptable","title":"When Interface Boxing Is Acceptable","text":"<p>Despite its performance implications in some contexts, interface boxing is often perfectly reasonable\u2014and sometimes preferred.</p>"},{"location":"01-common-patterns/interface-boxing/#when-abstraction-is-more-important-than-performance","title":"When abstraction is more important than performance","text":"<p>Interfaces enable decoupling and modularity. If you're designing a clean, testable API, the cost of boxing is negligible compared to the benefit of abstraction.</p> <pre><code>type Storage interface {\n    Save([]byte) error\n}\nfunc Process(s Storage) { /* ... */ }\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#when-values-are-small-and-boxing-is-allocation-free","title":"When values are small and boxing is allocation-free","text":"<p>Boxing small, copyable values like <code>int</code>, <code>float64</code>, or small structs typically causes no allocations.</p> <pre><code>var i interface{}\ni = 123 // safe and cheap\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#when-values-are-short-lived","title":"When values are short-lived","text":"<p>If the boxed value is used briefly (e.g. for logging or interface-based sorting), the overhead is minimal.</p> <pre><code>fmt.Println(\"value:\", someStruct) // implicit boxing is fine\n</code></pre>"},{"location":"01-common-patterns/interface-boxing/#when-dynamic-behavior-is-required","title":"When dynamic behavior is required","text":"<p>Interfaces allow runtime polymorphism. If you need different types to implement the same behavior, boxing is necessary and idiomatic.</p> <pre><code>for _, s := range []Shape{Circle{}, Square{}} {\n    fmt.Println(s.Area())\n}\n</code></pre> <p>Use boxing when it supports clarity, reusability, or design goals\u2014and avoid it only in performance-critical code paths.</p>"},{"location":"01-common-patterns/interface-boxing/#how-to-avoid-interface-boxing","title":"How to Avoid Interface Boxing","text":"<ul> <li>Use pointers when assigning to interfaces. If the method set requires a pointer receiver or the value is large, explicitly pass a pointer to avoid repeated copying and heap allocation.     <pre><code>for i := range tasks {\n   result = append(result, &amp;tasks[i]) // Avoids boxing copies\n}\n</code></pre></li> <li>Avoid interfaces in hot paths. If the concrete type is known and stable, avoid interface indirection entirely\u2014especially in compute-intensive or allocation-sensitive functions.</li> <li>Use type-specific containers. Instead of <code>[]interface{}</code>, prefer generic slices or typed collections where feasible. This preserves static typing and reduces unnecessary allocations.</li> <li>Benchmark and inspect with pprof. Use <code>go test -bench</code> and <code>pprof</code> to observe where allocations occur. If the allocation site is in <code>runtime.convT2E</code> (convert T to interface), you're likely boxing.</li> </ul>"},{"location":"01-common-patterns/lazy-init/","title":"Lazy Initialization","text":""},{"location":"01-common-patterns/lazy-init/#lazy-initialization-for-performance-in-go","title":"Lazy Initialization for Performance in Go","text":"<p>In Go, some resources are expensive to initialize, or simply unnecessary unless certain code paths are triggered. That\u2019s where lazy initialization becomes useful: it defers the construction of a value until the moment it\u2019s actually needed. This pattern can improve performance, reduce startup overhead, and avoid unnecessary work\u2014especially in high-concurrency applications.</p>"},{"location":"01-common-patterns/lazy-init/#why-lazy-initialization-matters","title":"Why Lazy Initialization Matters","text":"<p>Initializing heavy resources like database connections, caches, or large in-memory structures at startup can slow down application launch and consume memory before it\u2019s actually needed. Lazy initialization defers this work until the first time the resource is used, keeping startup fast and memory usage lean.</p> <p>It\u2019s also a practical pattern when you have logic that might be triggered multiple times but should only run once\u2014ensuring that expensive operations aren\u2019t repeated and that initialization remains safe and idempotent across concurrent calls.</p>"},{"location":"01-common-patterns/lazy-init/#using-synconce-for-thread-safe-initialization","title":"Using <code>sync.Once</code> for Thread-Safe Initialization","text":"<p>Go provides the <code>sync.Once</code> type to implement lazy initialization safely in concurrent environments:</p> <pre><code>var (\n    resource *MyResource\n    once     sync.Once\n)\n\nfunc getResource() *MyResource {\n    once.Do(func() {\n        resource = expensiveInit()\n    })\n    return resource\n}\n</code></pre> <p>In this example, the function <code>expensiveInit()</code> executes exactly once, no matter how many goroutines invoke <code>getResource()</code> concurrently. This ensures thread-safe initialization without additional synchronization overhead.</p>"},{"location":"01-common-patterns/lazy-init/#using-synconcevalue-and-synconcevalues-for-initialization-with-output-values","title":"Using <code>sync.OnceValue</code> and <code>sync.OnceValues</code> for Initialization with Output Values","text":"<p>Since Go 1.21, if your initialization logic returns a value, you might prefer using <code>sync.OnceValue</code> (single value) or <code>sync.OnceValues</code> (multiple values) for simpler, more expressive code:</p> <pre><code>var getResource = sync.OnceValue(func() *MyResource {\n    return expensiveInit()\n})\n\nfunc processData() {\n    res := getResource()\n    // use res\n}\n</code></pre> <p>Here, <code>sync.OnceValue</code> provides a concise way to wrap one-time initialization logic and access the result without managing flags or mutexes manually. It simplifies lazy loading by directly returning the computed value on demand.</p> <p>For cases where the initializer returns more than one value\u2014such as a resource and an error\u2014<code>sync.OnceValues</code> extends the same idea. It ensures the function runs exactly once and cleanly unpacks the results, keeping the code readable and thread-safe without boilerplate.</p> <pre><code>var getConfig = sync.OnceValues(func() (*Config, error) {\n    return loadConfig(\"config.yml\")\n})\n\nfunc processData() {\n    config, err := getConfig()\n    if err != nil {\n        log.Fatal(err)\n    }\n    // use config\n}\n</code></pre> <p>Choosing <code>sync.OnceValue</code> or <code>sync.OnceValues</code> helps you clearly express initialization logic with direct value returns, whereas <code>sync.Once</code> remains best suited for general scenarios requiring flexible initialization logic without immediate value returns.</p>"},{"location":"01-common-patterns/lazy-init/#custom-lazy-initialization-with-atomic-operations","title":"Custom Lazy Initialization with Atomic Operations","text":"<p>Yes, it\u2019s technically possible to replace <code>sync.Once</code>, <code>sync.OnceValue</code>, or <code>sync.OnceFunc</code> with custom logic using low-level atomic operations like <code>atomic.CompareAndSwap</code> or <code>atomic.Load/Store</code>. In rare, performance-critical paths, this can avoid the small overhead or allocations that come with the standard types.</p> <p>However, the trade-off is complexity. You lose the safety guarantees and clarity of the standard primitives, and it becomes easier to introduce subtle bugs\u2014especially under concurrency. Unless profiling shows that sync.Once is a bottleneck, the standard versions are almost always the better choice.</p> <p>That said, it\u2019s rarely worth the tradeoff.</p> <p>Manual atomic-based initialization is more error-prone, harder to read, and easier to get wrong\u2014especially when concurrency and memory visibility guarantees are involved. For the vast majority of cases, <code>sync.Once*</code> is safer, clearer, and performant enough.</p> <p>Info</p> <p>If you\u2019re convinced that atomic-based lazy initialization is justified in your case, this blog post walks through the details and caveats: Lazy initialization in Go using atomics</p>"},{"location":"01-common-patterns/lazy-init/#performance-considerations","title":"Performance Considerations","text":"<p>While lazy initialization can offer clear benefits, it also brings added complexity. It\u2019s important to handle initialization carefully to avoid subtle issues like race conditions or concurrency bugs. Using built-in tools like <code>sync.Once</code> or <code>atomic</code> operations typically ensures thread-safety without much hassle. Still, it\u2019s always a good idea to measure actual improvements through profiling, confirming lazy initialization truly enhances startup speed, reduces memory usage, or boosts your application's responsiveness.</p>"},{"location":"01-common-patterns/lazy-init/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>There is typically nothing specific to benchmark with lazy initialization itself, as the main benefit is deferring expensive resource creation. The performance gains are inherently tied to the avoided cost of unnecessary initialization, startup speed improvements, and reduced memory consumption, rather than direct runtime throughput differences.</p>"},{"location":"01-common-patterns/lazy-init/#when-to-choose-lazy-initialization","title":"When to Choose Lazy Initialization","text":"<ul> <li>When resource initialization is costly or involves I/O. Delaying construction avoids paying the cost of setup\u2014like opening files, querying databases, or loading large structures\u2014unless it\u2019s actually needed.</li> <li>To improve startup performance and memory efficiency. Deferring work until first use allows your application to start faster and avoid allocating memory for resources that may never be used.</li> <li>When not all resources are needed immediately or at all during runtime. Lazy initialization helps you avoid initializing fields or services that only apply in specific code paths.</li> <li>To guarantee a block of code executes exactly once despite repeated calls. Using tools like <code>sync.Once</code> ensures thread-safe, one-time setup in concurrent environments.</li> </ul>"},{"location":"01-common-patterns/mem-prealloc/","title":"Memory Preallocation","text":"<p>Memory preallocation is a simple but effective way to improve performance in Go programs that work with slices or maps that grow over time. Instead of letting the runtime resize these structures as they fill up\u2014often at unpredictable points\u2014you allocate the space you need upfront. This avoids the cost of repeated allocations, internal copying, and extra GC pressure as intermediate objects are created and discarded.</p> <p>In high-throughput or latency-sensitive systems, preallocating memory makes execution more predictable and helps avoid performance cliffs that show up under load. If the workload size is known or can be reasonably estimated, there\u2019s no reason to let the allocator do the guessing.</p>"},{"location":"01-common-patterns/mem-prealloc/#why-preallocation-matters","title":"Why Preallocation Matters","text":"<p>Go\u2019s slices and maps grow automatically as new elements are added, but that convenience comes with a cost. When capacity is exceeded, the runtime allocates a larger backing array or hash table and copies the existing data over. This reallocation adds memory pressure, burns CPU cycles, and can stall tight loops in high-throughput paths. In performance-critical code\u2014especially where the size is known or can be estimated\u2014frequent resizing is unnecessary overhead. Preallocating avoids these penalties by giving the runtime enough room to work without interruption.</p> <p>Go uses a hybrid growth strategy for slices to balance speed and memory efficiency. Early on, capacities double with each expansion\u20142, 4, 8, 16\u2014minimizing the number of allocations. But once a slice exceeds around 1024 elements, the growth rate slows to roughly 25%. So instead of jumping from 1024 to 2048, the next allocation might grow to about 1280.</p> <p>This shift reduces memory waste on large slices but increases the frequency of allocations if the final size is known but not preallocated. In those cases, using make([]T, 0, expectedSize) is the more efficient choice\u2014it avoids repeated resizing and cuts down on unnecessary copying.</p> <pre><code>s := make([]int, 0)\nfor i := 0; i &lt; 10_000; i++ {\n    s = append(s, i)\n    fmt.Printf(\"Len: %d, Cap: %d\\n\", len(s), cap(s))\n}\n</code></pre> <p>Output illustrating typical growth:</p> <pre><code>Len: 1, Cap: 1\nLen: 2, Cap: 2\nLen: 3, Cap: 4\nLen: 5, Cap: 8\n...\nLen: 1024, Cap: 1024\nLen: 1025, Cap: 1280\n</code></pre>"},{"location":"01-common-patterns/mem-prealloc/#practical-preallocation-examples","title":"Practical Preallocation Examples","text":""},{"location":"01-common-patterns/mem-prealloc/#slice-preallocation","title":"Slice Preallocation","text":"<p>Without preallocation, each append operation might trigger new allocations:</p> <pre><code>// Inefficient\nvar result []int\nfor i := 0; i &lt; 10000; i++ {\n    result = append(result, i)\n}\n</code></pre> <p>This pattern causes Go to allocate larger underlying arrays repeatedly as the slice grows, resulting in memory copying and GC pressure. We can avoid that by using <code>make</code> with a specified capacity:</p> <pre><code>// Efficient\nresult := make([]int, 0, 10000)\nfor i := 0; i &lt; 10000; i++ {\n    result = append(result, i)\n}\n</code></pre> <p>If it is known that the slice will be fully populated, we can be even more efficient by avoiding bounds checks:</p> <pre><code>// Efficient\nresult := make([]int, 10000)\nfor i := range result {\n    result[i] = i\n}\n</code></pre>"},{"location":"01-common-patterns/mem-prealloc/#map-preallocation","title":"Map Preallocation","text":"<p>Maps grow similarly. By default, Go doesn\u2019t know how many elements you\u2019ll add, so it resizes the underlying structure as needed.</p> <pre><code>// Inefficient\nm := make(map[int]string)\nfor i := 0; i &lt; 10000; i++ {\n    m[i] = fmt.Sprintf(\"val-%d\", i)\n}\n</code></pre> <p>Starting with Go 1.11, you can preallocate <code>map</code> capacity too:</p> <pre><code>// Efficient\nm := make(map[int]string, 10000)\nfor i := 0; i &lt; 10000; i++ {\n    m[i] = fmt.Sprintf(\"val-%d\", i)\n}\n</code></pre> <p>This helps the runtime allocate enough internal storage upfront, avoiding rehashing and resizing costs.</p>"},{"location":"01-common-patterns/mem-prealloc/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Here\u2019s a simple benchmark comparing appending to a preallocated slice vs. a zero-capacity slice:</p> Show the benchmark file <pre><code>package perf\n\nimport (\n    \"testing\"\n)\n\nfunc BenchmarkAppendNoPrealloc(b *testing.B) {\n    for b.Loop() {\n        var s []int\n        for j := 0; j &lt; 10000; j++ {\n            s = append(s, j)\n        }\n    }\n}\n\nfunc BenchmarkAppendWithPrealloc(b *testing.B) {\n    for b.Loop() {\n        s := make([]int, 0, 10000)\n        for j := 0; j &lt; 10000; j++ {\n            s = append(s, j)\n        }\n    }\n}\n</code></pre> <p>You\u2019ll typically observe that preallocation reduces allocations to a single one per operation and significantly improves throughput.</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkAppendNoPrealloc-14 41,727 28,539 357,626 19 BenchmarkAppendWithPrealloc-14 170,154 7,093 81,920 1"},{"location":"01-common-patterns/mem-prealloc/#when-to-preallocate","title":"When To Preallocate","text":"<p> Preallocate when:</p> <ul> <li>The number of elements in slices or maps is known or reasonably predictable. Allocating memory up front avoids the cost of repeated resizing as the data structure grows.</li> <li>Your application involves tight loops or high-throughput data processing. Preallocation reduces per-iteration overhead and helps maintain steady performance under load.</li> <li>Minimizing garbage collection overhead is crucial for your application's performance. Fewer allocations mean less work for the garbage collector, resulting in lower latency and more consistent behavior.</li> </ul> <p> Avoid preallocation when:</p> <ul> <li>The data size is highly variable and unpredictable. If input sizes fluctuate widely, any fixed-size preallocation risks being either too small (leading to reallocations) or too large (wasting memory).</li> <li>Over-allocation risks significant memory waste. Reserving more memory than needed increases your application\u2019s footprint and can negatively impact cache locality or trigger unnecessary GC activity.</li> <li>You\u2019re prematurely optimizing. Always verify with profiling. Preallocation is effective, but only when it addresses a real bottleneck or allocation hotspot in your workload.</li> </ul>"},{"location":"01-common-patterns/object-pooling/","title":"Object Pooling","text":"<p>Object pooling helps reduce allocation churn in high-throughput Go programs by reusing objects instead of allocating fresh ones each time. This avoids repeated work for the allocator and eases pressure on the garbage collector, especially when dealing with short-lived or frequently reused structures.</p> <p>Go\u2019s <code>sync.Pool</code> provides a built-in way to implement pooling with minimal code. It\u2019s particularly effective for objects that are expensive to allocate or that would otherwise contribute to frequent garbage collection cycles. While not a silver bullet, it\u2019s a low-friction tool that can lead to noticeable gains in latency and CPU efficiency under sustained load.</p>"},{"location":"01-common-patterns/object-pooling/#how-object-pooling-works","title":"How Object Pooling Works","text":"<p>Object pooling allows programs to reuse memory by recycling previously allocated objects instead of creating new ones on every use. Rather than hitting the heap each time, objects are retrieved from a shared pool and returned once they\u2019re no longer needed. This reduces the number of allocations, cuts down on garbage collection workload, and leads to more predictable performance\u2014especially in workloads with high object churn or tight latency requirements.</p>"},{"location":"01-common-patterns/object-pooling/#using-syncpool-for-object-reuse","title":"Using <code>sync.Pool</code> for Object Reuse","text":""},{"location":"01-common-patterns/object-pooling/#without-object-pooling-inefficient-memory-usage","title":"Without Object Pooling (Inefficient Memory Usage)","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\ntype Data struct {\n    Value int\n}\n\nfunc createData() *Data {\n    return &amp;Data{Value: 42}\n}\n\nfunc main() {\n    for i := 0; i &lt; 1000000; i++ {\n        obj := createData() // Allocating a new object every time\n        _ = obj // Simulate usage\n    }\n    fmt.Println(\"Done\")\n}\n</code></pre> <p>In the above example, every iteration creates a new <code>Data</code> instance, leading to unnecessary allocations and increased GC pressure.</p>"},{"location":"01-common-patterns/object-pooling/#with-object-pooling-optimized-memory-usage","title":"With Object Pooling (Optimized Memory Usage)","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\ntype Data struct {\n    Value int\n}\n\nvar dataPool = sync.Pool{\n    New: func() any {\n        return &amp;Data{}\n    },\n}\n\nfunc main() {\n    for i := 0; i &lt; 1000000; i++ {\n        obj := dataPool.Get().(*Data) // Retrieve from pool\n        obj.Value = 42 // Use the object\n        dataPool.Put(obj) // Return object to pool for reuse\n    }\n    fmt.Println(\"Done\")\n}\n</code></pre>"},{"location":"01-common-patterns/object-pooling/#pooling-byte-buffers-for-efficient-io","title":"Pooling Byte Buffers for Efficient I/O","text":"<p>Object pooling is especially effective when working with large byte slices that would otherwise lead to high allocation and garbage collection overhead.</p> <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"fmt\"\n    \"sync\"\n)\n\nvar bufferPool = sync.Pool{\n    New: func() any {\n        return new(bytes.Buffer)\n    },\n}\n\nfunc main() {\n    buf := bufferPool.Get().(*bytes.Buffer)\n    buf.Reset()\n    buf.WriteString(\"Hello, pooled world!\")\n    fmt.Println(buf.String())\n    bufferPool.Put(buf) // Return buffer to pool for reuse\n}\n</code></pre> <p>Using <code>sync.Pool</code> for byte buffers significantly reduces memory pressure when dealing with high-frequency I/O operations.</p>"},{"location":"01-common-patterns/object-pooling/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>To prove that object pooling actually reduces allocations and improves speed, we can use Go's built-in memory profiling tools (<code>pprof</code>) and compare memory allocations between the non-pooled and pooled versions. Simulating a full-scale application that actively uses memory for benchmarking is challenging, so we need a controlled test to evaluate direct heap allocations versus pooled allocations.</p> Show the benchmark file <pre><code>package perf\n\nimport (\n    \"sync\"\n    \"testing\"\n)\n\n// Data is a struct with a large fixed-size array to simulate a memory-intensive object.\ntype Data struct {\n    Values [1024]int\n}\n\n// BenchmarkWithoutPooling measures the performance of direct heap allocations.\nfunc BenchmarkWithoutPooling(b *testing.B) {\n    for b.Loop() {\n        data := &amp;Data{}      // Allocating a new object each time\n        data.Values[0] = 42  // Simulating some memory activity\n    }\n}\n\n// dataPool is a sync.Pool that reuses instances of Data to reduce memory allocations.\nvar dataPool = sync.Pool{\n    New: func() any {\n        return &amp;Data{}\n    },\n}\n\n// BenchmarkWithPooling measures the performance of using sync.Pool to reuse objects.\nfunc BenchmarkWithPooling(b *testing.B) {\n    for b.Loop() {\n        obj := dataPool.Get().(*Data) // Retrieve from pool\n        obj.Values[0] = 42            // Simulate memory usage\n        dataPool.Put(obj)             // Return object to pool for reuse\n    }\n}\n</code></pre> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkWithoutPooling-14 1,692,014 705.4 8,192 1 BenchmarkWithPooling-14 160,440,506 7.455 0 0 <p>The benchmark results highlight the contrast in performance and memory usage between direct allocations and object pooling. In <code>BenchmarkWithoutPooling</code>, each iteration creates a new object on the heap, leading to higher execution time and increased memory consumption. This constant allocation pressure triggers more frequent garbage collection, which adds latency and reduces throughput. The presence of nonzero allocation counts per operation confirms that each iteration contributes to GC load, making this approach less efficient in high-throughput scenarios.</p>"},{"location":"01-common-patterns/object-pooling/#when-should-you-use-syncpool","title":"When Should You Use <code>sync.Pool</code>?","text":"<p> Use sync.Pool when:</p> <ul> <li>You have short-lived, reusable objects (e.g., buffers, scratch memory, request state). Pooling avoids repeated allocations and lets you recycle memory efficiently.</li> <li>Allocation overhead or GC churn is measurable and significant. Reusing objects reduces the number of heap allocations, which in turn lowers garbage collection frequency and pause times.</li> <li>The object\u2019s lifecycle is local and can be reset between uses. When objects don\u2019t need complex teardown and are safe to reuse after a simple reset, pooling is straightforward and effective.</li> <li>You want to reduce pressure on the garbage collector in high-throughput systems. In systems handling thousands of requests per second, pooling helps maintain consistent performance and minimizes GC-related latency spikes.</li> </ul> <p> Avoid sync.Pool when:</p> <ul> <li>Objects are long-lived or shared across multiple goroutines. <code>sync.Pool</code> is optimized for short-lived, single-use objects and doesn\u2019t manage shared ownership or coordination.</li> <li>The reuse rate is low and pooled objects are not frequently accessed. If objects sit idle in the pool, you gain little benefit and may even waste memory.</li> <li>Predictability or lifecycle control is more important than allocation speed. Pooling makes lifecycle tracking harder and may not be worth the tradeoff.</li> <li>Memory savings are negligible or code complexity increases significantly. If pooling doesn\u2019t provide clear benefits, it can add unnecessary complexity to otherwise simple code.</li> </ul>"},{"location":"01-common-patterns/stack-alloc/","title":"Stack Allocations and Escape Analysis","text":"<p>When writing performance-critical Go applications, one of the subtle but significant optimizations you can make is encouraging values to be allocated on the stack rather than the heap. Stack allocations are cheaper, faster, and garbage-free\u2014but Go doesn't always put your variables there automatically. That decision is made by the Go compiler during escape analysis.</p> <p>In this article, we\u2019ll explore what escape analysis is, how to read the compiler\u2019s escape diagnostics, what causes values to escape, and how to structure your code to minimize unnecessary heap allocations. We'll also benchmark different scenarios to show the real-world impact.</p>"},{"location":"01-common-patterns/stack-alloc/#what-is-escape-analysis","title":"What Is Escape Analysis?","text":"<p>Escape analysis is a static analysis performed by the Go compiler to determine whether a variable can be safely allocated on the stack or if it must be moved (\"escape\") to the heap.</p>"},{"location":"01-common-patterns/stack-alloc/#why-does-it-matter","title":"Why does it matter?","text":"<ul> <li>Stack allocations are cheap: the memory is automatically freed when the function returns.</li> <li>Heap allocations are more expensive: they involve garbage collection overhead.</li> </ul> <p>The compiler decides where to place each variable based on how it's used. If a variable can be guaranteed to not outlive its declaring function, it can stay on the stack. If not, it escapes to the heap.</p>"},{"location":"01-common-patterns/stack-alloc/#example-stack-vs-heap","title":"Example: Stack vs Heap","text":"<pre><code>func allocate() *int {\n    x := 42\n    return &amp;x // x escapes to the heap\n}\n\nfunc noEscape() int {\n    x := 42\n    return x // x stays on the stack\n}\n</code></pre> <p>In <code>allocate</code>, <code>x</code> is returned as a pointer. Since the pointer escapes the function, the Go compiler places <code>x</code> on the heap. In <code>noEscape</code>, <code>x</code> is a plain value and doesn\u2019t escape.</p>"},{"location":"01-common-patterns/stack-alloc/#how-to-view-escape-analysis-output","title":"How to View Escape Analysis Output","text":"<p>You can inspect escape analysis with the <code>-gcflags</code> compiler option:</p> <pre><code>go build -gcflags=\"-m\" ./path/to/pkg\n</code></pre> <p>Or for a specific file:</p> <pre><code>go run -gcflags=\"-m\" main.go\n</code></pre> <p>This will print lines like:</p> <pre><code>main.go:10:6: moved to heap: x\nmain.go:14:6: can inline noEscape\n</code></pre> <p>Look for messages like <code>moved to heap</code> to identify escape points.</p>"},{"location":"01-common-patterns/stack-alloc/#what-causes-variables-to-escape","title":"What Causes Variables to Escape?","text":"<p>Here are common scenarios that force heap allocation:</p>"},{"location":"01-common-patterns/stack-alloc/#returning-pointers-to-local-variables","title":"Returning Pointers to Local Variables","text":"<pre><code>func escape() *int {\n    x := 10\n    return &amp;x // escapes\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#capturing-variables-in-closures","title":"Capturing Variables in Closures","text":"<pre><code>func closureEscape() func() int {\n    x := 5\n    return func() int { return x } // x escapes\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#interface-conversions","title":"Interface Conversions","text":"<p>When a value is stored in an interface, it may escape:</p> <pre><code>func toInterface(i int) interface{} {\n    return i // escapes if type info needed at runtime\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#assignments-to-global-variables-or-struct-fields","title":"Assignments to Global Variables or Struct Fields","text":"<pre><code>var global *int\n\nfunc assignGlobal() {\n    x := 7\n    global = &amp;x // escapes\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#large-composite-literals","title":"Large Composite Literals","text":"<p>Go may allocate large structs or slices on the heap even if they don\u2019t strictly escape.</p> <pre><code>func makeLargeSlice() []int {\n    s := make([]int, 10000) // may escape due to size\n    return s\n}\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#benchmarking-stack-vs-heap-allocations","title":"Benchmarking Stack vs Heap Allocations","text":"<p>Let\u2019s run a benchmark to explore when heap allocations actually occur\u2014and when they don\u2019t, even if we return a pointer.</p> <pre><code>func StackAlloc() Data {\n    return Data{1, 2, 3} // stays on stack\n}\n\nfunc HeapAlloc() *Data {\n    return &amp;Data{1, 2, 3} // escapes to heap\n}\n\nfunc BenchmarkStackAlloc(b *testing.B) {\n    for b.Loop() {\n        _ = StackAlloc()\n    }\n}\n\nfunc BenchmarkHeapAlloc(b *testing.B) {\n    for b.Loop() {\n        _ = HeapAlloc()\n    }\n}\n</code></pre> <p>Benchmark Results</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkStackAlloc-14 1,000,000,000 0.2604 ns 0 B 0 BenchmarkHeapAlloc-14 1,000,000,000 0.2692 ns 0 B 0 <p>You might expect <code>HeapAlloc</code> to always allocate memory on the heap\u2014but it doesn\u2019t here. That\u2019s because the compiler is smart: in this isolated benchmark, the pointer returned by <code>HeapAlloc</code> doesn\u2019t escape the function in any meaningful way. The compiler can see it\u2019s only used within the benchmark and short-lived, so it safely places it on the stack too.</p>"},{"location":"01-common-patterns/stack-alloc/#forcing-a-heap-allocation","title":"Forcing a Heap Allocation","text":"<pre><code>var sink *Data\n\nfunc HeapAllocEscape() {\n    d := &amp;Data{1, 2, 3}\n    sink = d // d escapes to heap\n}\n\nfunc BenchmarkHeapAllocEscape(b *testing.B) {\n    for b.Loop() {\n        HeapAllocEscape()\n    }\n}\n</code></pre> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkHeapAllocEscape-14 331,469,049 10.55 ns 24 B 1 <p>As shown in <code>BenchmarkHeapAllocEscape</code>, assigning the pointer to a global variable causes a real heap escape. This introduces real overhead: a 40x slower call, a 24-byte allocation, and one garbage-collected object per call.</p> Show the benchmark file <pre><code>package main\n\nimport \"testing\"\n\ntype Data struct {\n    A, B, C int\n}\n\n// heap-alloc-start\nfunc StackAlloc() Data {\n    return Data{1, 2, 3} // stays on stack\n}\n\nfunc HeapAlloc() *Data {\n    return &amp;Data{1, 2, 3} // escapes to heap\n}\n\nfunc BenchmarkStackAlloc(b *testing.B) {\n    for b.Loop() {\n        _ = StackAlloc()\n    }\n}\n\nfunc BenchmarkHeapAlloc(b *testing.B) {\n    for b.Loop() {\n        _ = HeapAlloc()\n    }\n}\n// heap-alloc-end\n\n// escape-start\nvar sink *Data\n\nfunc HeapAllocEscape() {\n    d := &amp;Data{1, 2, 3}\n    sink = d // d escapes to heap\n}\n\nfunc BenchmarkHeapAllocEscape(b *testing.B) {\n    for b.Loop() {\n        HeapAllocEscape()\n    }\n}\n// escape-end\n</code></pre>"},{"location":"01-common-patterns/stack-alloc/#when-to-optimize-for-stack-allocation","title":"When to Optimize for Stack Allocation","text":"<p>Not all escapes are worth preventing. Here\u2019s when it makes sense to focus on stack allocation\u2014and when it\u2019s better to let values escape.</p> <p> When to Avoid Escape</p> <ul> <li>In performance-critical paths. Reducing heap usage in tight loops or latency-sensitive code lowers GC pressure and speeds up execution.</li> <li>For short-lived, small objects. These can be efficiently stack-allocated without involving the garbage collector, reducing memory churn.</li> <li>When you control the full call chain. If the object stays within your code and you can restructure it to avoid escape, it\u2019s often worth the small refactor.</li> <li>If profiling reveals GC bottlenecks. Escape analysis helps you target and shrink memory-heavy allocations identified in real-world traces.</li> </ul> <p> When It\u2019s Fine to Let Values Escape</p> <ul> <li>When returning values from constructors or factories. Returning a pointer from <code>NewThing()</code> is idiomatic Go\u2014even if it causes an escape, it improves clarity and usability.</li> <li>When objects must outlive the function. If you're storing data in a global, sending to a goroutine, or saving it in a struct, escaping is necessary and correct.</li> <li>When allocation size is small and infrequent. If the heap allocation isn\u2019t in a hot path, the benefit of avoiding it is often negligible.</li> <li>When preventing escape hurts readability. Writing awkward code to keep everything on the stack can reduce maintainability for a micro-optimization that won\u2019t matter.</li> </ul>"},{"location":"01-common-patterns/worker-pool/","title":"Goroutine Worker Pools in Go","text":"<p>Go\u2019s concurrency model makes it deceptively easy to spin up thousands of goroutines\u2014but that ease can come at a cost. Each goroutine starts small, but under load, unbounded concurrency can cause memory usage to spike, context switches to pile up, and overall performance to become unpredictable.</p> <p>A worker pool helps apply backpressure by limiting the number of active goroutines. Instead of spawning one per task, a fixed pool handles work in controlled parallelism\u2014keeping memory usage predictable and avoiding overload. This makes it easier to maintain steady performance even as demand scales.</p>"},{"location":"01-common-patterns/worker-pool/#why-worker-pools-matter","title":"Why Worker Pools Matter","text":"<p>While launching a goroutine for every task is idiomatic and often effective, doing so at scale comes with trade-offs. Each goroutine requires stack space and introduces scheduling overhead. Performance can degrade sharply when the number of active goroutines grows, especially in systems handling unbounded input like HTTP requests, jobs from a queue, or tasks from a channel.</p> <p>A worker pool maintains a fixed number of goroutines that pull tasks from a shared job queue. This creates a backpressure mechanism, ensuring the system never processes more work concurrently than it can handle. Worker pools are particularly valuable when the cost of each task is predictable, and the overall system throughput needs to be stable.</p>"},{"location":"01-common-patterns/worker-pool/#basic-worker-pool-implementation","title":"Basic Worker Pool Implementation","text":"<p>Here\u2019s a minimal implementation of a worker pool:</p> <pre><code>func worker(id int, jobs &lt;-chan int, results chan&lt;- [32]byte) {\n    for j := range jobs {\n        results &lt;- doWork(j)\n    }\n}\n\nfunc doWork(n int) [32]byte {\n    data := []byte(fmt.Sprintf(\"payload-%d\", n))\n    return sha256.Sum256(data)                  // (1)\n}\n\nfunc main() {\n    jobs := make(chan int, 100)\n    results := make(chan [32]byte, 100)\n\n    for w := 1; w &lt;= 5; w++ {\n        go worker(w, jobs, results)\n    }\n\n    for j := 1; j &lt;= 10; j++ {\n        jobs &lt;- j\n    }\n    close(jobs)\n\n    for a := 1; a &lt;= 10; a++ {\n        &lt;-results\n    }\n}\n</code></pre> <ol> <li>Cryptography is for illustration purposes of CPU-bound code</li> </ol> <p>In this example, five workers pull from the <code>jobs</code> channel and push results to the <code>results</code> channel. The worker pool limits concurrency to five tasks at a time, regardless of how many tasks are sent.</p>"},{"location":"01-common-patterns/worker-pool/#worker-count-and-cpu-cores","title":"Worker Count and CPU Cores","text":"<p>The optimal number of workers in a pool is closely tied to the number of CPU cores, which you can obtain in Go using <code>runtime.NumCPU()</code> or <code>runtime.GOMAXPROCS(0)</code>. For CPU-bound tasks\u2014where each worker consumes substantial CPU time\u2014you generally want the number of workers to be equal to or slightly less than the number of logical CPU cores. This ensures maximum core utilization without excessive overhead.</p> <p>If your tasks are I/O-bound (e.g., network calls, disk I/O, database queries), the pool size can be larger than the number of cores. This is because workers will spend much of their time blocked, allowing others to run. In contrast, CPU-heavy workloads benefit from a smaller, tightly bounded pool that avoids contention and context switching.</p>"},{"location":"01-common-patterns/worker-pool/#why-too-many-workers-hurts-performance","title":"Why Too Many Workers Hurts Performance","text":"<p>Adding more workers can seem like a straightforward way to boost throughput, but the benefits taper off quickly past a certain point. Once you exceed the system\u2019s optimal level of concurrency, performance often degrades instead of improving.</p> <ul> <li>Scheduler contention increases as the Go runtime juggles more runnable goroutines than it has logical CPUs to run them.</li> <li>Context switching grows more frequent, burning CPU cycles without doing real work.</li> <li>Memory pressure rises because each goroutine holds its own stack, even when idle.</li> <li>Cache thrashing becomes more likely as goroutines bounce across cores, disrupting locality and degrading CPU cache performance.</li> </ul> <p>The result: higher latency, increased GC activity, and reduced throughput\u2014the exact opposite of what a properly tuned worker pool is supposed to deliver.</p>"},{"location":"01-common-patterns/worker-pool/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Worker pools shine in scenarios where the workload is CPU-bound or where concurrency must be capped to avoid saturating a shared resource (e.g., database connections or file descriptors). Benchmarks comparing unbounded goroutine launches vs. worker pools typically show:</p> <ul> <li>Lower peak memory usage</li> <li>More stable response times under load</li> <li>Improved CPU cache locality</li> </ul> Show the benchmark file <pre><code>package perf\n\nimport (\n    // \"log\"\n    \"fmt\"\n    // \"os\"\n    \"runtime\"\n    \"sync\"\n    \"testing\"\n    \"crypto/sha256\"\n)\n\nconst (\n    numJobs     = 10000\n    workerCount = 10\n)\n\nfunc doWork(n int) [32]byte {\n    data := []byte(fmt.Sprintf(\"payload-%d\", n))\n    return sha256.Sum256(data)\n}\n\nfunc BenchmarkUnboundedGoroutines(b *testing.B) {\n    for b.Loop() {\n        var wg sync.WaitGroup\n        wg.Add(numJobs)\n\n        for j := 0; j &lt; numJobs; j++ {\n            go func(job int) {\n                _ = doWork(job)\n                wg.Done()\n            }(j)\n        }\n        wg.Wait()\n    }\n}\n\nfunc worker(jobs &lt;-chan int, wg *sync.WaitGroup) {\n    for job := range jobs {\n        _ = doWork(job)\n        wg.Done()\n    }\n}\n\nfunc BenchmarkWorkerPool(b *testing.B) {\n    for b.Loop() {\n        var wg sync.WaitGroup\n        wg.Add(numJobs)\n\n        jobs := make(chan int, numJobs)\n        for w := 0; w &lt; workerCount; w++ {\n            go worker(jobs, &amp;wg)\n        }\n\n        for j := 0; j &lt; numJobs; j++ {\n            jobs &lt;- j\n        }\n\n        close(jobs)\n        wg.Wait()\n    }\n}\n</code></pre> <p>Results:</p> Benchmark Iterations Time per op (ns) Bytes per op Allocs per op BenchmarkUnboundedGoroutines-14 2,274 2,499,213 ns 639,350 39,754 BenchmarkWorkerPool-14 3,325 1,791,772 ns 320,707 19,762 <p>In our benchmark, each task performed a CPU-intensive operation (e.g., cryptographic hashing, math, or serialization). With <code>workerCount = 10</code> on an Apple M3 Max machine, the worker pool outperformed the unbounded goroutine model by a significant margin, using fewer resources and completing work faster. Increasing the worker count beyond the number of available cores led to worse performance due to contention.</p>"},{"location":"01-common-patterns/worker-pool/#when-to-use-worker-pools","title":"When To Use Worker Pools","text":"<p> Use a goroutine worker pool when:</p> <ul> <li>The workload is unbounded or high volume. A pool prevents uncontrolled goroutine growth, which can lead to memory exhaustion, GC pressure, and unpredictable performance.</li> <li>Unbounded concurrency risks resource saturation. Capping the number of concurrent workers helps avoid overwhelming the CPU, network, database, or disk I/O\u2014especially under load.</li> <li>You need predictable parallelism for stability. Limiting concurrency smooths out performance spikes and keeps system behavior consistent, even during traffic surges.</li> <li>Tasks are relatively uniform and queue-friendly. When task cost is consistent, a fixed pool size provides efficient scheduling with minimal overhead, ensuring good throughput without complex coordination.</li> </ul> <p> Avoid a worker pool when:</p> <ul> <li>Each task must be processed immediately with minimal latency. Queuing in a worker pool introduces delay. For latency-critical tasks, direct goroutine spawning avoids the scheduling overhead.</li> <li>You can rely on Go's scheduler for natural load balancing in low-load scenarios. In light workloads, the overhead of managing a pool may outweigh its benefits. Go\u2019s scheduler can often handle lightweight parallelism efficiently on its own.</li> <li>Workload volume is small and bounded. Spinning up goroutines directly keeps code simpler for limited, predictable workloads without risking uncontrolled growth.</li> </ul>"},{"location":"01-common-patterns/zero-copy/","title":"Zero-Copy Techniques","text":"<p>When writing performance-critical Go code, how memory is managed often has a bigger impact than it first appears. Zero-copy techniques are one of the more effective ways to tighten that control. Instead of moving bytes from buffer to buffer, these techniques work directly on existing memory\u2014avoiding copies altogether. That means less pressure on the CPU, better cache behavior, and fewer GC-triggered pauses. For I/O-heavy systems\u2014whether you\u2019re streaming files, handling network traffic, or parsing large datasets\u2014this can translate into much higher throughput and lower latency without adding complexity.</p>"},{"location":"01-common-patterns/zero-copy/#understanding-zero-copy","title":"Understanding Zero-Copy","text":"<p>In the usual I/O path, data moves back and forth between user space and kernel space\u2014first copied into a kernel buffer, then into your application\u2019s buffer, or the other way around. It works, but it\u2019s wasteful. Every copy burns CPU cycles and clogs up memory bandwidth. Zero-copy changes that. Instead of bouncing data between buffers, it lets applications work directly with what\u2019s already in place\u2014no detours, no extra copies. The result? Lower CPU load, better use of memory, and faster I/O, especially when throughput or latency actually matter.</p>"},{"location":"01-common-patterns/zero-copy/#common-zero-copy-techniques-in-go","title":"Common Zero-Copy Techniques in Go","text":""},{"location":"01-common-patterns/zero-copy/#using-ioreader-and-iowriter-interfaces","title":"Using <code>io.Reader</code> and <code>io.Writer</code> Interfaces","text":"<p>Using interfaces like <code>io.Reader</code> and <code>io.Writer</code> gives you fine-grained control over how data flows. Instead of spinning up new buffers every time, you can reuse existing ones and keep memory usage steady. In practice, this avoids unnecessary garbage collection pressure and keeps your I/O paths clean and efficient\u2014especially when you\u2019re dealing with high-throughput or streaming workloads.</p> <pre><code>func StreamData(src io.Reader, dst io.Writer) error {\n    buf := make([]byte, 4096) // Reusable buffer\n    _, err := io.CopyBuffer(dst, src, buf)\n    return err\n}\n</code></pre> <p><code>io.CopyBuffer</code> reuses a provided buffer, avoiding repeated allocations and intermediate copies. An in-depth <code>io.CopyBuffer</code> explanation is available on SO.</p>"},{"location":"01-common-patterns/zero-copy/#slicing-for-efficient-data-access","title":"Slicing for Efficient Data Access","text":"<p>Slicing large byte arrays or buffers instead of copying data into new slices is a powerful zero-copy strategy:</p> <pre><code>func process(buffer []byte) []byte {\n    return buffer[128:256] // returns a slice reference without copying\n}\n</code></pre> <p>Slices in Go are inherently zero-copy since they reference the underlying array.</p>"},{"location":"01-common-patterns/zero-copy/#memory-mapping-mmap","title":"Memory Mapping (<code>mmap</code>)","text":"<p>Using memory mapping enables direct access to file contents without explicit read operations:</p> <pre><code>import \"golang.org/x/exp/mmap\"\n\nfunc ReadFileZeroCopy(path string) ([]byte, error) {\n    r, err := mmap.Open(path)\n    if err != nil {\n        return nil, err\n    }\n    defer r.Close()\n\n    data := make([]byte, r.Len())\n    _, err = r.ReadAt(data, 0)\n    return data, err\n}\n</code></pre> <p>This approach maps file contents directly into memory, entirely eliminating copying between kernel and user-space.</p>"},{"location":"01-common-patterns/zero-copy/#benchmarking-impact","title":"Benchmarking Impact","text":"<p>Here's a basic benchmark illustrating performance differences between explicit copying and zero-copy slicing:</p> <pre><code>func BenchmarkCopy(b *testing.B) {\n    data := make([]byte, 64*1024)\n    for b.Loop() {\n        buf := make([]byte, len(data))\n        copy(buf, data)\n    }\n}\n\nfunc BenchmarkSlice(b *testing.B) {\n    data := make([]byte, 64*1024)\n    for b.Loop() {\n        _ = data[:]\n    }\n}\n</code></pre> <p>In <code>BenchmarkCopy</code>, each iteration copies a 64KB buffer into a fresh slice\u2014allocating memory and duplicating data every time. That cost adds up fast. <code>BenchmarkSlice</code>, on the other hand, just re-slices the same buffer\u2014no allocation, no copying, just new view on the same data. The difference is night and day. When performance matters, avoiding copies isn\u2019t just a micro-optimization\u2014it\u2019s fundamental.</p> <p>Info</p> <p>These two functions are not equivalent in behavior\u2014<code>BenchmarkCopy</code> makes an actual deep copy of the buffer, while <code>BenchmarkSlice</code> only creates a new slice header pointing to the same underlying data. This benchmark is not comparing functional correctness but is intentionally contrasting performance characteristics to highlight the cost of unnecessary copying.</p> Benchmark Time per op (ns) Bytes per op Allocs per op BenchmarkCopy 4,246 65536 1 BenchmarkSlice 0.592 0 0"},{"location":"01-common-patterns/zero-copy/#file-io-memory-mapping-vs-standard-read","title":"File I/O: Memory Mapping vs. Standard Read","text":"<p>We also benchmarked file reading performance using <code>os.ReadAt</code> versus <code>mmap.Open</code> for a 4MB binary file.</p> <pre><code>func BenchmarkReadWithCopy(b *testing.B) {\n    f, err := os.Open(\"testdata/largefile.bin\")\n    if err != nil {\n        b.Fatalf(\"failed to open file: %v\", err)\n    }\n    defer f.Close()\n\n    buf := make([]byte, 4*1024*1024) // 4MB buffer\n    for b.Loop() {\n        _, err := f.ReadAt(buf, 0)\n        if err != nil &amp;&amp; err != io.EOF {\n            b.Fatal(err)\n        }\n    }\n}\n\nfunc BenchmarkReadWithMmap(b *testing.B) {\n    r, err := mmap.Open(\"testdata/largefile.bin\")\n    if err != nil {\n        b.Fatalf(\"failed to mmap file: %v\", err)\n    }\n    defer r.Close()\n\n    buf := make([]byte, r.Len())\n    for b.Loop() {\n        _, err := r.ReadAt(buf, 0)\n        if err != nil &amp;&amp; err != io.EOF {\n            b.Fatal(err)\n        }\n    }\n}\n</code></pre> How to run the benchmark <p>To run the benchmark involving <code>mmap</code>, you\u2019ll need to install the required package and create a test file:</p> <pre><code>go get golang.org/x/exp/mmap\nmkdir -p testdata\ndd if=/dev/urandom of=./testdata/largefile.bin bs=1M count=4\n</code></pre> <p>Benchmark Results</p> Benchmark Time per op (ns) Bytes per op Allocs per op ReadWithCopy 94,650 0 0 ReadWithMmap 50,082 0 0 <p>The memory-mapped version (<code>mmap</code>) is nearly 2\u00d7 faster than the standard read call. This illustrates how zero-copy access through memory mapping can substantially reduce read latency and CPU usage for large files.</p> Show the complete benchmark file <pre><code>package perf\n\nimport \"testing\"\n\n\n// interface-start\n\ntype Worker interface {\n    Work()\n}\n\ntype LargeJob struct {\n    payload [4096]byte\n}\n\nfunc (LargeJob) Work() {}\n// interface-end\n\n// bench-slice-start\nfunc BenchmarkBoxedLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs = jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            var job LargeJob\n            jobs = append(jobs, job)\n        }\n    }\n}\n\nfunc BenchmarkPointerLargeSlice(b *testing.B) {\n    jobs := make([]Worker, 0, 1000)\n    for b.Loop() {\n        jobs := jobs[:0]\n        for j := 0; j &lt; 1000; j++ {\n            job := &amp;LargeJob{}\n            jobs = append(jobs, job)\n        }\n    }\n}\n// bench-slice-end\n\n// bench-call-start\nvar sink Worker\n\nfunc call(w Worker) {\n    sink = w\n}\n\nfunc BenchmarkCallWithValue(b *testing.B) {\n    for b.Loop() {\n        var j LargeJob\n        call(j)\n    }\n}\n\nfunc BenchmarkCallWithPointer(b *testing.B) {\n    for b.Loop() {\n        j := &amp;LargeJob{}\n        call(j)\n    }\n}\n// bench-call-end\n</code></pre>"},{"location":"01-common-patterns/zero-copy/#when-to-use-zero-copy","title":"When to Use Zero-Copy","text":"<p> Zero-copy techniques are highly beneficial for:</p> <ul> <li>Network servers handling large amounts of concurrent data streams. Avoiding unnecessary memory copies helps reduce CPU usage and latency, especially under high load.</li> <li>Applications with heavy I/O operations like file streaming or real-time data processing. Zero-copy allows data to move through the system efficiently without redundant allocations or copies.</li> </ul> <p>Warning</p> <p> Zero-copy isn\u2019t a free win. Slices share underlying memory, so reusing them means you\u2019re also sharing state. If one part of your code changes the data while another is still reading it, you\u2019re setting yourself up for subtle, hard-to-track bugs. This kind of shared memory requires discipline\u2014clear ownership and tight control. It also adds complexity, which might not be worth it unless the performance gains are real and measurable. Always benchmark before committing to it.</p>"},{"location":"01-common-patterns/zero-copy/#real-world-use-cases-and-libraries","title":"Real-World Use Cases and Libraries","text":"<p>Zero-copy strategies aren't just theoretical\u2014they're used in production by performance-critical Go systems:</p> <ul> <li>fasthttp: A high-performance HTTP server designed to avoid allocations. It returns slices directly and avoids <code>string</code> conversions to minimize copying.</li> <li>gRPC-Go: Uses internal buffer pools and avoids deep copying of large request/response messages to reduce GC pressure.</li> <li>MinIO: An object storage system that streams data directly between disk and network using <code>io.Reader</code> without unnecessary buffer replication.</li> <li>Protobuf and MsgPack libraries: Efficient serialization frameworks like <code>google.golang.org/protobuf</code> and <code>vmihailenco/msgpack</code> support decoding directly into user-managed buffers.</li> <li>InfluxDB and Badger: These storage engines use <code>mmap</code> extensively for fast, zero-copy access to database files.</li> </ul> <p>These libraries show how zero-copy techniques help reduce allocations, GC overhead, and system call frequency\u2014all while increasing throughput.</p>"},{"location":"02-networking/","title":"Practical Networking Patterns in Go","text":"<p>A 13-part guide to building scalable, efficient, and resilient networked applications in Go\u2014grounded in real-world benchmarks, low-level optimizations, and practical design patterns.</p>"},{"location":"02-networking/#benchmarking-first","title":"Benchmarking First","text":"<ul> <li> <p>Benchmarking and Load Testing for Networked Go Apps</p> <p>Establish performance baselines before optimizing anything. Learn how to simulate realistic traffic using tools like <code>vegeta</code>, <code>wrk</code>, and <code>k6</code>. Covers throughput, latency percentiles, connection concurrency, and profiling under load. Sets the foundation for diagnosing bottlenecks and measuring the impact of every optimization in the series.</p> </li> </ul>"},{"location":"02-networking/#foundations-and-core-concepts","title":"Foundations and Core Concepts","text":"<ul> <li> <p>How Go Handles Networking: Concurrency, Goroutines, and the net Package</p> <p>Understand Go\u2019s approach to networking from the ground up. Covers how goroutines, the <code>net</code> package, and the runtime scheduler interact, including blocking I/O behavior, connection handling, and the use of pollers like <code>epoll</code> or <code>kqueue</code> under the hood.</p> </li> <li> <p>Efficient Use of <code>net/http</code>, <code>net.Conn</code>, and UDP</p> <p>Compare idiomatic and advanced usage of <code>net/http</code> vs raw <code>net.Conn</code>. Dive into connection pooling, custom dialers, stream reuse, and buffer tuning. Demonstrates how to avoid common pitfalls like leaking connections, blocking handlers, or over-allocating buffers.</p> </li> </ul>"},{"location":"02-networking/#scaling-and-performance-engineering","title":"Scaling and Performance Engineering","text":"<ul> <li> <p>Managing 10K++ Concurrent Connections in Go</p> <p>Handling massive concurrency requires intentional architecture. Explore how to efficiently serve 10,000+ concurrent sockets using Go\u2019s goroutines, proper resource capping, socket tuning, and runtime configuration. Focuses on connection lifecycles, scaling pitfalls, and real-world tuning.</p> </li> <li> <p>GOMAXPROCS, epoll/kqueue, and Scheduler-Level Tuning</p> <p>Dive into low-level performance knobs like <code>GOMAXPROCS</code>, <code>GODEBUG</code>, thread pinning, and how Go\u2019s scheduler interacts with epoll/kqueue. Learn when increasing parallelism helps\u2014and when it doesn\u2019t. Includes tools for CPU affinity and benchmarking the effect of these changes.</p> </li> </ul>"},{"location":"02-networking/#diagnostics-and-resilience","title":"Diagnostics and Resilience","text":"<ul> <li> <p>Building Resilient Connection Handling with Load Shedding and Backpressure</p> <p>Learn how to prevent overloads from crashing your system. Covers circuit breakers, passive vs active load shedding, backpressure strategies using channel buffering and timeouts, and how to reject or degrade requests gracefully under pressure.</p> </li> <li> <p>Memory Management and Leak Prevention in Long-Lived Connections</p> <p>Long-lived connections like WebSockets or TCP streams can slowly leak memory or accumulate goroutines. This post shows how to identify common leaks, enforce read/write deadlines, manage backpressure, and trace heap growth with memory profiles.</p> </li> </ul>"},{"location":"02-networking/#transport-level-optimization","title":"Transport-Level Optimization","text":"<ul> <li> <p>Comparing TCP, HTTP/2, and gRPC Performance in Go</p> <p>Benchmark and analyze different transport protocols in Go: raw TCP with custom framing, HTTP/2 via <code>net/http</code>, and gRPC. Evaluate latency, throughput, connection reuse, and CPU/memory cost across real scenarios like internal APIs, messaging systems, and microservices.</p> </li> <li> <p>QUIC in Go: Building Low-Latency Services with quic-go</p> <p>Explore QUIC as a next-gen transport for real-time and mobile-first systems. Introduce the <code>quic-go</code> library, demonstrate setup for secure multiplexed streams, and compare performance against HTTP/2 and TCP. Also covers connection migration and 0-RTT for fast startup.</p> </li> </ul>"},{"location":"02-networking/#low-level-and-advanced-tuning","title":"Low-Level and Advanced Tuning","text":"<ul> <li> <p>Low-Level Network Optimizations: Socket Options That Matter</p> <p>Explore advanced socket-level tuning options like disabling Nagle\u2019s algorithm (<code>TCP_NODELAY</code>), adjusting <code>SO_REUSEPORT</code>, <code>SO_RCVBUF</code>/<code>SO_SNDBUF</code>, TCP keepalives, and connection backlog (<code>SOMAXCONN</code>). Explain how Go exposes these via <code>syscall</code> and how to wrap them safely. Real-world examples included for latency-sensitive systems and high-throughput services.</p> </li> <li> <p>Tuning DNS Performance in Go Services</p> <p>DNS lookups are often overlooked as latency culprits. Learn how Go performs name resolution (cgo vs Go resolver), when to cache results, and how to use custom dialers or pre-resolved IPs to avoid flaky network paths. Includes metrics and debugging tips for real-world DNS slowdowns.</p> </li> <li> <p>Optimizing TLS for Speed: Handshake, Reuse, and Cipher Choice</p> <p>TLS adds security\u2014but it can also add overhead. Tune your Go service for fast and secure TLS: enable session resumption, choose fast cipher suites, use ALPN negotiation wisely, and minimize cert verification cost. Examples included with <code>tls.Config</code> best practices.</p> </li> <li> <p>Connection Lifecycle Observability: From Dial to Close</p> <p>Trace the full lifecycle of a connection with visibility at each stage\u2014DNS, dial, handshake, negotiation, reads/writes, and teardown. Learn how to log connection spans, trace hangs, correlate errors with performance metrics, and build custom observability into network flows.</p> </li> </ul>"},{"location":"02-networking/10k-connections/","title":"Managing 10K+ Concurrent Connections in Go","text":"Why not 100K+ or 1 Mill connection? <p>While framing the challenge in terms of \u201c100K concurrent connections\u201d is tempting, practical engineering often begins with a more grounded target: 10K to 20K stable, performant connections. This isn\u2019t a limitation of Go itself but a reflection of real-world constraints: ulimit settings, ephemeral port availability, TCP stack configuration, and the nature of the application workload all set hard boundaries.</p> <p>Cloud environments introduce their own considerations. For instance, AWS Fargate explicitly sets both the soft and hard nofile (number of open files) limit to 65,535, which provides more headroom for socket-intensive applications but still falls short of the 100K+ threshold. On EC2 instances, the practical limits depend on the base operating system and user configuration. By default, many Linux distributions impose a soft limit of 1024 and a hard limit of 65535 for nofile. Even this hard cap is lower than required to handle 100,000 open connections in a single process. Reaching higher limits requires kernel-level tuning, container runtime overrides, and multi-process strategies to distribute file descriptor load.</p> <p>A server handling simple echo logic behaves very differently from one performing CPU-bound processing, structured logging, or real-time transformation. Additionally, platform-level tunability varies\u2014Linux exposes granular control through sysctl, epoll, and reuseport, while macOS lacks many of these mechanisms. In that context, achieving and sustaining 10K+ concurrent connections with real workloads is a demanding, yet practical, benchmark.</p> <p>Handling massive concurrency in Go is often romanticized\u2014\"goroutines are cheap, just spawn them!\"\u2014but reality gets harsher as we push towards six-digit concurrency levels. Serving over 10,000 concurrent sockets isn\u2019t something you solve by scaling hardware alone\u2014it requires an architecture that works with the OS, the Go runtime, and the network stack, not against them.</p>"},{"location":"02-networking/10k-connections/#embracing-gos-concurrency-model","title":"Embracing Go\u2019s Concurrency Model","text":"<p>Go\u2019s lightweight goroutines and its powerful runtime scheduler make it an excellent choice for scaling network applications. Goroutines consume only a few kilobytes of stack space, which, in theory, makes them ideal for handling tens of thousands of concurrent connections. However, reality forces us to think beyond just spinning up goroutines. While the language\u2019s abstraction makes concurrency almost \u201cmagical,\u201d achieving true efficiency at this scale demands intentional design.</p> <p>Running a server that spawns one goroutine per connection means you\u2019re leaning heavily on the runtime scheduler to juggle thousands of concurrent execution paths. While goroutines are lightweight, they\u2019re not free\u2014each one adds to memory consumption and introduces scheduling overhead that scales with concurrency. Thus, the first design pattern that should be adopted is to ensure that each connection follows a clearly defined lifecycle and that every goroutine performs its task as efficiently as possible.</p> <p>Let\u2019s consider a basic model where we accept connections and delegate their handling to separate goroutines:</p> <pre><code>package main\n\nimport (\n    \"log\"\n    \"net\"\n    \"sync/atomic\"\n    \"time\"\n)\n\nvar activeConnections uint64\n\nfunc main() {\n    listener, err := net.Listen(\"tcp\", \":8080\")\n    if err != nil {\n        log.Fatalf(\"Error starting TCP listener: %v\", err)\n    }\n    defer listener.Close()\n\n    for {\n        conn, err := listener.Accept()\n        if err != nil {\n            log.Printf(\"Error accepting connection: %v\", err)\n            continue\n        }\n\n        atomic.AddUint64(&amp;activeConnections, 1)\n        go handleConnection(conn)\n    }\n}\n\nfunc handleConnection(conn net.Conn) {\n    defer func() {\n        conn.Close()\n        atomic.AddUint64(&amp;activeConnections, ^uint64(0)) // effectively decrements the counter\n    }()\n\n    // Imagine complex processing here\u2014an echo server example:\n    buffer := make([]byte, 1024)\n    for {\n        conn.SetDeadline(time.Now().Add(30 * time.Second)) // prevent idle hangs\n        n, err := conn.Read(buffer)\n        if err != nil {\n            log.Printf(\"Connection read error: %v\", err)\n            return\n        }\n        _, err = conn.Write(buffer[:n])\n        if err != nil {\n            log.Printf(\"Connection write error: %v\", err)\n            return\n        }\n    }\n}\n</code></pre> <p>Each connection is assigned its own goroutine. That approach works fine at low concurrency and fits Go\u2019s model well. But once you\u2019re dealing with tens of thousands of connections, the design has to account for system limits. Goroutines are cheap\u2014but not free.</p>"},{"location":"02-networking/10k-connections/#managing-concurrency-at-scale","title":"Managing Concurrency at Scale","text":"<p>It\u2019s not enough to just accept connections; you need to control what happens after. Unbounded goroutine creation leads to memory growth and increased scheduler load. To keep the system stable, concurrency must be capped\u2014typically using a semaphore or similar construct to limit how many goroutines handle active work at any given time.</p> <p>For example, you might limit the number of simultaneous active connections before spinning up a new goroutine for each incoming connection. This strategy might involve a buffered channel acting as a semaphore:</p> <pre><code>package main\n\nimport (\n    \"net\"\n)\n\nvar connLimiter = make(chan struct{}, 10000) // Max 10K concurrent conns\n\nfunc main() {\n    ln, _ := net.Listen(\"tcp\", \":8080\")\n    defer ln.Close()\n\n    for {\n        conn, _ := ln.Accept()\n\n        connLimiter &lt;- struct{}{} // Acquire slot\n        go func(c net.Conn) {\n            defer func() {\n                c.Close()\n                &lt;-connLimiter // Release slot\n            }()\n            // Dummy echo logic\n            buf := make([]byte, 1024)\n            c.Read(buf)\n            c.Write(buf)\n        }(conn)\n    }\n}\n</code></pre> <p>This pattern not only helps prevent resource exhaustion but also gracefully degrades service under high load. Adjusting these limits according to your hardware and workload characteristics is a continuous tuning process.</p> <p>Info</p> <p>We use the <code>connLimiter</code> approach here for purely illustrative purposes, as it clarifies the idea. In real life, you will most likely use errgroup to manage the goroutines amount and some <code>SIGINT,</code> and <code>SIGTERM</code> signal handling for graceful process termination.</p>"},{"location":"02-networking/10k-connections/#os-level-and-socket-tuning","title":"OS-Level and Socket Tuning","text":"<p>Before your Go application can handle more than 10,000 simultaneous connections, the operating system has to be prepared for that scale. On Linux, this usually starts with raising the limit on open file descriptors. The TCP stack also needs tuning\u2014default settings often aren\u2019t designed for high-connection workloads. Without these adjustments, the application will hit OS-level ceilings long before Go becomes the bottleneck.</p> <pre><code># Increase file descriptor limit\nulimit -n 200000\n</code></pre> <p>But it doesn\u2019t stop there. You\u2019ll also need:</p> <pre><code>sysctl -w net.core.somaxconn=65535\nsysctl -w net.ipv4.ip_local_port_range=\"10000 65535\"\nsysctl -w net.ipv4.tcp_tw_reuse=1\nsysctl -w net.ipv4.tcp_fin_timeout=15\n</code></pre> <ul> <li><code>net.core.somaxconn=65535</code>: This controls the size of the pending connection queue (the backlog) for listening sockets. A small value here will cause connection drops when many clients attempt to connect simultaneously.</li> <li><code>net.ipv4.ip_local_port_range=\"10000 65535\"</code>: Defines the ephemeral port range used for outbound connections. A wider range prevents port exhaustion when you\u2019re making many outbound connections from the same machine.</li> <li><code>net.ipv4.tcp_tw_reuse=1</code>: Allows reuse of sockets in <code>TIME_WAIT</code> state for new connections if safe. Helps reduce socket exhaustion, especially in short-lived TCP connections.</li> <li><code>net.ipv4.tcp_fin_timeout=15</code>: Reduces the time the kernel holds sockets in <code>FIN_WAIT2</code> after a connection is closed. Shorter timeout means faster resource reclamation, crucial when thousands of sockets churn per minute.</li> </ul> <p>Tuning these parameters helps prevent the OS from becoming the bottleneck as connection counts grow. On top of that, setting socket options like <code>TCP_NODELAY</code> can reduce latency by disabling Nagle\u2019s algorithm, which buffers small packets by default. In Go, these options can be applied through the net package, or more directly via the syscall package if lower-level control is needed.</p> <p>In some cases, using Go\u2019s <code>net.ListenConfig</code> allows you to inject custom control over socket creation. This is particularly useful when you need to set options at the time of listener creation:</p> <pre><code>func main() {\n    lc := net.ListenConfig{\n        Control: func(network, address string, c syscall.RawConn) error {\n            var controlErr error\n            err := c.Control(func(fd uintptr) {\n                // Enable TCP_NODELAY on the socket\n                controlErr = syscall.SetsockoptInt(int(fd), syscall.IPPROTO_TCP, syscall.TCP_NODELAY, 1)\n            })\n            if err != nil {\n                return err\n            }\n            return controlErr\n        },\n    }\n    listener, err := lc.Listen(context.Background(), \"tcp\", \":8080\")\n    if err != nil {\n        log.Fatalf(\"Error creating listener: %v\", err)\n    }\n    defer listener.Close()\n    // Accept connections in a loop\u2026\n}\n</code></pre>"},{"location":"02-networking/10k-connections/#go-scheduler-and-memory-pressure","title":"Go Scheduler and Memory Pressure","text":"<p>Spawning 10,000 goroutines might look impressive on paper, but what matters is how those goroutines behave. If they\u2019re mostly idle\u2014blocked on I/O like network or disk\u2014Go\u2019s scheduler handles them efficiently, parking and resuming with little overhead. But when goroutines actively allocate memory, spin in tight loops, or constantly contend on channels and mutexes, things get expensive. You\u2019ll start to see increased garbage collection pressure and scheduler thrashing, both of which erode performance.</p> <p>Go\u2019s garbage collector handles short-lived allocations well, but it doesn\u2019t come for free. If you\u2019re spawning goroutines that churn through memory\u2014allocating per request, per message, or worse, per loop\u2014GC pressure builds fast. The result isn\u2019t just more frequent collections, but higher latency and lost CPU cycles. Throughput drops, and the system spends more time cleaning up than doing real work.</p> <p>To manage this, you can explicitly tune the GC aggressiveness:</p> <pre><code>GOGC=50\n</code></pre> <p>Or directly within your codebase:</p> <pre><code>import \"runtime/debug\"\n\nfunc main() {\n    debug.SetGCPercent(50)\n    // rest of your application logic\n}\n</code></pre> <p>The default value for <code>GOGC</code> is 100, meaning the GC triggers when the heap size doubles compared to the previous GC cycle. Lower values (like 50) mean more frequent but shorter GC cycles, helping control memory growth at the cost of increased CPU overhead.</p> <p>Info</p> <p>In some cases, you may need an opposite \u2013 to increase the <code>GOGC</code> value, turn the GC off completely, or prefer GOMEMLIMIT=X and GOGC=off configuration. Do not make a decision before careful profiling!</p>"},{"location":"02-networking/10k-connections/#optimizing-goroutine-behavior","title":"Optimizing Goroutine Behavior","text":"<p>Consider structuring your application so that goroutines block naturally rather than actively waiting or spinning. For example, instead of polling channels in tight loops, use select statements efficiently:</p> <pre><code>for {\n    select {\n    case msg := &lt;-msgChan:\n        handleMsg(msg)\n    case &lt;-ctx.Done():\n        return\n    }\n}\n</code></pre> <p>If your goroutines must wait, prefer blocking on channels or synchronization primitives provided by Go, like mutexes or condition variables, instead of actively polling.</p>"},{"location":"02-networking/10k-connections/#pooling-and-reusing-objects","title":"Pooling and Reusing Objects","text":"<p>Another crucial technique to reduce memory allocations and GC overhead is using <code>sync.Pool</code>:</p> <pre><code>var bufPool = sync.Pool{\n    New: func() any { return make([]byte, 1024) },\n}\n\nfunc handleRequest() {\n    buf := bufPool.Get().([]byte)\n    defer bufPool.Put(buf)  // (1)\n\n    // use buffer for request handling\n}\n</code></pre> <ol> <li>Be careful here! It's strictly workflow-dependant, when you must return an object to the pool!</li> </ol> <p>Reusing objects through pools reduces memory churn. With fewer allocations, the garbage collector runs less often and with less impact. This translates directly into lower latency and more predictable performance under load.</p>"},{"location":"02-networking/10k-connections/#connection-lifecycle-management","title":"Connection Lifecycle Management","text":"<p>A connection isn\u2019t just accepted and forgotten\u2014it moves through a full lifecycle: setup, data exchange, teardown. Problems usually show up in the quiet phases. Idle connections that aren\u2019t cleaned up can tie up memory and block goroutines indefinitely. Enforcing read and write deadlines is essential. Heartbeat messages help too\u2014they give you a way to detect dead peers without waiting for the OS to time out.</p> <p>In one production case, slow client responses left goroutines blocked in reads. Over time, they built up until the system started degrading. Adding deadlines and lightweight health checks fixed the leak. Goroutines no longer lingered, and resource usage stayed flat under load.</p> <p>Each connection still runs in its own goroutine\u2014but with proper lifecycle management in place, scale doesn\u2019t come at the cost of stability.</p> <pre><code>for {\n    conn, err := ln.Accept()\n    if err != nil {\n        // handle error\n    }\n    go handle(conn)\n}\n</code></pre> <p>Inside the handler, a ticker is used to fire every few seconds, triggering a periodic heartbeat that keeps the connection active and responsive:</p> <pre><code>ticker := time.NewTicker(5 * time.Second)\ndefer ticker.Stop()\n</code></pre> <p>Before reading from the client, the server sets a read deadline\u2014if no data is received within that time, the operation fails, and the connection is cleaned up. This prevents a blocked read from stalling the goroutine indefinitely:</p> <pre><code>conn.SetReadDeadline(time.Now().Add(10 * time.Second))\n_, err := reader.ReadString('\n')\nif err != nil {\n    return // read timeout or client gone\n}\n</code></pre> <p>Likewise, before sending the heartbeat, the server sets a write deadline. If the client is unresponsive or the network is slow, the write will fail promptly, avoiding resource leakage:</p> <pre><code>select {\ncase &lt;-ticker.C:\n    conn.SetWriteDeadline(time.Now().Add(10 * time.Second))\n    conn.Write([]byte(\"ping\"))\ndefault:\n    // skip heartbeat if not due\n}\n</code></pre> <p>The loop handles incoming messages and sends periodic heartbeats, with read and write deadlines enforcing boundaries on both sides. This setup keeps each connection under active supervision. Silent failures don\u2019t linger, and the system avoids trading stability for performance.</p>"},{"location":"02-networking/10k-connections/#real-world-tuning-and-scaling-pitfalls","title":"Real-World Tuning and Scaling Pitfalls","text":"<p>Scaling to 10K+ connections is not just a matter of code\u2014it requires anticipating and mitigating potential pitfalls across many layers of the stack. Beyond addressing memory footprint, file descriptor limits, and blocking I/O, a series of high-concurrency echo server tests revealed additional performance considerations under real load.</p> <p>One experiment began with a simple line-based echo server. The baseline handler was straightforward:</p> <pre><code>func handle(conn net.Conn) {\n    defer conn.Close()\n    reader := bufio.NewReader(conn)\n\n    for {\n        line, err := reader.ReadString('\\n')\n        if err != nil {\n            fmt.Printf(\"Connection closed: %v\\n\", err)\n            return\n        }\n        conn.Write([]byte(line)) // echo\n    }\n}\n</code></pre> <p>Using a tool like <code>tcpkali</code>:</p> <pre><code>tcpkali -m $'ping\\n' -c 10000 --connect-rate=2000 --duration=60s 127.0.0.1:9000\n</code></pre> <p>The test ramped up to 10'000 concurrent connections. Over the 60-second run, it sent 2.4 MiB and received 210.3 MiB of data. Each connection averaged around 0.4 kBps, with an aggregate throughput of 29.40 Mbps downstream and 0.33 Mbps upstream. This result highlighted the server\u2019s limited responsiveness to outgoing data under sustained high concurrency, with substantial backpressure on <code>fd.Read</code>.</p>"},{"location":"02-networking/10k-connections/#instrumenting-and-benchmarking-the-server","title":"Instrumenting and Benchmarking the Server","text":"<p>Info</p> <p>We use <code>c5.2xlarge</code> (8 CPU, 16 GiB) AWS instance for all these tests.</p> <p>To better understand system behavior under high load, Go\u2019s built-in tracing facilities were enabled:</p> <pre><code>import (\n    \"runtime/trace\"\n    \"os\"\n    \"log\"\n)\n\nfunc main() {\n    f, err := os.Create(\"trace.out\")\n    if err != nil { log.Fatal(err) }\n    defer f.Close()\n\n    trace.Start(f)\n    defer trace.Stop()\n\n    // server logic ...\n}\n</code></pre> <p>After running the server and collecting traces, the command</p> <pre><code>go tool trace trace.out\n</code></pre> <p>revealed that a significant portion of runtime was spent blocked in <code>fd.Read</code> and <code>fd.Write</code>, suggesting an opportunity to balance I/O operations more effectively. Trace analysis revealed that <code>fd.Read</code> accounted for 23% of runtime, while <code>fd.Write</code> consumed 75%, indicating significant write-side backpressure during echoing. Although <code>ulimit -n</code> was set to 65535 (AWS EC2 instance's hard limit), the system still encountered bottlenecks due to I/O blocking and ephemeral port range limitations.</p>"},{"location":"02-networking/10k-connections/#reducing-write-blocking-with-buffered-writes","title":"Reducing Write Blocking with Buffered Writes","text":"<p>Connection writes were wrapped in a <code>bufio.Writer</code> with periodic flushing instead of flushing after each write. The updated snippet:</p> <pre><code>reader := bufio.NewReader(conn)\nwriter := bufio.NewWriter(conn)\ncount := 0\nconst flushInterval = 10\n\nfor {\n    line, err := reader.ReadString('\\n')\n    if err != nil {\n        return\n    }\n    writer.WriteString(line)\n    count++\n    if count &gt;= flushInterval {\n        writer.Flush()\n        count = 0\n    }\n}\n</code></pre> <p>Benchmarking with:</p> <pre><code>tcpkali -m $'ping\\n' -c 10000 --connect-rate=2000 --duration=60s 127.0.0.1:9000\n</code></pre> <p>showed dramatic improvements\u2014throughput increased from about 33.8 MiB to over 1661 MiB received and 1369 MiB sent across 10,000 connections, with per-connection bandwidth reaching 5.3 kBps. Aggregate throughput rose to 232.28 Mbps downstream and 191.41 Mbps upstream. The tracing profile confirmed more balanced I/O wait times, even under a much heavier concurrent load.</p>"},{"location":"02-networking/10k-connections/#handling-burst-loads-and-cpu-bound-workloads","title":"Handling Burst Loads and CPU-Bound Workloads","text":"<p>To evaluate the server's behavior under extreme connection pressure, a burst test was executed with 30,000 connections ramping up at 5,000 per second:</p> <pre><code>tcpkali -m $'ping\\n' -c 30000 --connect-rate=5000 --duration=60s 127.0.0.1:9000\n</code></pre> <p>The server ramped up cleanly to 30,000 concurrent connections and sustained them for the full 60 seconds. It handled a total of 2580.3 MiB sent and 1250.9 MiB received, maintaining an aggregate throughput of 360.75 Mbps upstream and 174.89 Mbps downstream. Per-channel bandwidth naturally decreased to about 1.2 kBps, but the stability across all channels and the lack of dropped connections pointed to effective load distribution and solid I/O handling even at scale.</p> <p>To simulate CPU-bound workloads, the server was modified to compute a SHA256 hash for each incoming line:</p> <pre><code>func hash(s string) string {\n    h := sha256.Sum256([]byte(s))\n    return hex.EncodeToString(h[:])\n}\n\n...\n\nfor {\n    line, err := reader.ReadString('\\n')\n    if err != nil {\n        return\n    }\n    _ = hash(line) // simulate CPU-intensive processing\n    writer.WriteString(line)\n    count++\n    if count &gt;= flushInterval {\n        writer.Flush()\n        count = 0\n    }\n}\n</code></pre> <p>In this configuration, using the same 30,000-connection setup, throughput dropped to 1068.3 MiB sent and 799.3 MiB received. Aggregate bandwidth fell to 149.35 Mbps upstream and 111.74 Mbps downstream, and per-connection bandwidth declined to around 0.7 kBps. While the server maintained full connection count and uptime, trace analysis revealed increased time spent in runtime.systemstack_switch and GC-related functions. This clearly demonstrated the impact of compute-heavy tasks on overall throughput and reinforced the need for careful balance between I/O and CPU workload when operating at high concurrency.</p>"},{"location":"02-networking/10k-connections/#summarizing-the-technical-gains","title":"Summarizing the Technical Gains","text":"<p>Benchmarking across four distinct server configurations revealed how buffering, concurrency scaling, and CPU-bound tasks influence performance under load:</p> Feature Baseline (10K, no buffer) 10K Buffered Connections 30K Buffered Connections 30K + CPU Load (SHA256) Connections handled 10,000 10,000 30,000 30,000 Data sent (60s) 2.4 MiB 1369.1 MiB 2580.3 MiB 1068.3 MiB Data received (60s) 210.3 MiB 1661.4 MiB 1250.9 MiB 799.3 MiB Per-channel bandwidth ~0.4 kBps ~5.3 kBps ~1.2 kBps ~0.7 kBps Aggregate bandwidth (\u2193/\u2191) 29.40 / 0.33 Mbps 232.28 / 191.41 Mbps 174.89 / 360.75 Mbps 111.74 / 149.35 Mbps Packet rate estimate (\u2193/\u2191) 329K / 29 pkt/s 278K / 16K pkt/s 135K / 32K pkt/s 136K / 13K pkt/s I/O characteristics Severe write backpressure Balanced read/write Efficient under scale Latency from CPU contention CPU and GC pressure Low Low Moderate High (GC + hash compute) <p>Starting from the baseline of 10,000 unbuffered connections, the server showed limited throughput\u2014just 2.4 MiB sent and 210.3 MiB received over 60 seconds\u2014with clear signs of write-side backpressure. Introducing buffered writes with the same connection count unlocked over 1369 MiB sent and 1661 MiB received, improving throughput by more than an order of magnitude and balancing I/O wait times. Scaling further to 30,000 connections maintained stability and increased overall throughput, albeit with reduced per-connection bandwidth. When SHA256 hashing was added per message, total throughput dropped significantly, confirming the expected CPU bottleneck and reinforcing the need to factor in compute latency when designing high-concurrency, I/O-heavy services.</p> <p>These profiles serve as a concrete reference for performance-aware development, where transport, memory, and compute must be co-optimized for real-world scalability.</p> <p>As you can see, achieving even 30,000 concurrent connections with reliable performance is a non-trivial task. The test results demonstrated that once a workload deviates from a trivial echo server\u2014for example, by adding logging, CPU-bound processing, or more complex read/write logic\u2014throughput and stability can degrade rapidly. Performance at scale is highly dependent on workflow characteristics, such as I/O patterns, synchronization frequency, and memory pressure.</p> <p>Taken together, these tests reinforce the need for workload-aware tuning and platform-specific adjustments when building high-performance, scalable networking systems. </p>"},{"location":"02-networking/a-bit-more-tuning/","title":"GOMAXPROCS, epoll/kqueue, and Scheduler-Level Tuning","text":"<p>Go applications operating at high concurrency levels frequently encounter performance ceilings that are not attributable to CPU saturation. These limitations often stem from runtime-level mechanics: how goroutines (G) are scheduled onto logical processors (P) via operating system threads (M), how blocking operations affect thread availability, and how the runtime interacts with kernel facilities like <code>epoll</code> or <code>kqueue</code> for I/O readiness.</p> <p>Unlike surface-level code optimization, resolving these issues requires awareness of the Go scheduler\u2019s internal design, particularly how GOMAXPROCS governs execution parallelism and how thread contention, cache locality, and syscall latency emerge under load. Misconfigured runtime settings can lead to excessive context switching, stalled P\u2019s, and degraded throughput despite available cores.</p> <p>System-level tuning\u2014through CPU affinity, thread pinning, and scheduler introspection\u2014provides a critical path to improving latency and throughput in multicore environments. When paired with precise benchmarking and observability, these adjustments allow Go services to scale more predictably and fully take advantage of modern hardware architectures.</p>"},{"location":"02-networking/a-bit-more-tuning/#understanding-gomaxprocs","title":"Understanding GOMAXPROCS","text":"<p>In Go, <code>GOMAXPROCS</code> defines the maximum number of operating system threads (M\u2019s) simultaneously executing user\u2011level Go code (G\u2019s). It\u2019s set to the developer's machine\u2019s logical CPU count by default. Under the hood, the scheduler exposes P\u2019s (processors) equal to <code>GOMAXPROCS</code>. Each P hosts a run queue of G\u2019s and binds to a single M to execute Go code.</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"runtime\"\n)\n\nfunc main() {\n    // Show current value\n    fmt.Printf(\"GOMAXPROCS = %d\\n\", runtime.GOMAXPROCS(0))\n\n    // Set to 4 and confirm\n    prev := runtime.GOMAXPROCS(4)\n    fmt.Printf(\"Changed from %d to %d\\n\", prev, runtime.GOMAXPROCS(0))\n}\n</code></pre> <p>When developers increase <code>GOMAXPROCS</code>, developers allow more P\u2019s\u2014and therefore more OS threads\u2014to run Go\u2011routines in parallel. That often boosts performance for CPU\u2011bound workloads. However, more P\u2019s also incur more context switches, more cache thrashing, and potentially more contention in shared data structures (e.g., the garbage collector\u2019s work queues). It's important to understand that blindly scaling past the sweet spot can actually degrade latency.</p>"},{"location":"02-networking/a-bit-more-tuning/#diving-into-gos-scheduler-internals","title":"Diving into Go\u2019s Scheduler Internals","text":"<p>Go\u2019s scheduler organizes three core actors: G (goroutine), M (OS thread), and P (logical processor), see more details here. When a goroutine makes a blocking syscall, its M detaches from its P, returning the P to the global scheduler so another M can pick it up. This design prevents syscalls from starving CPU\u2011bound goroutines.</p> <p>The scheduler uses work stealing: each P maintains a local run queue, and idle P\u2019s will steal work from busier peers. If developers set GOMAXPROCS too high, developers will see diminishing returns in stolen work versus the overhead of balancing those run queues.</p> <p>Enabling scheduler tracing via <code>GODEBUG</code> can reveal fine grained metrics:</p> <pre><code>GODEBUG=schedtrace=1000,scheddetail=1 go run main.go\n</code></pre> <ul> <li><code>schedtrace=1000</code> instructs the runtime to print scheduler state every 1000 milliseconds (1 second).</li> <li><code>scheddetail=1</code> enables additional information per logical processor (P), such as individual run queue lengths.</li> </ul> <p>Each printed trace includes statistics like:</p> <pre><code>SCHED 3024ms: gomaxprocs=14 idleprocs=14 threads=26 spinningthreads=0 needspinning=0 idlethreads=20 runqueue=0 gcwaiting=false nmidlelocked=1 stopwait=0 sysmonwait=false\n  P0: status=0 schedtick=173 syscalltick=3411 m=nil runqsize=0 gfreecnt=6 timerslen=0\n  ...\n  P13: status=0 schedtick=96 syscalltick=310 m=nil runqsize=0 gfreecnt=2 timerslen=0\n  M25: p=nil curg=nil mallocing=0 throwing=0 preemptoff= locks=0 dying=0 spinning=false blocked=true lockedg=nil\n  ...\n</code></pre> <p>The first line reports global scheduler state including whether garbage collection is blocking (gcwaiting), if spinning threads are needed, and idle thread counts.</p> <p>Each P line details the logical processor's scheduler activity, including the number of times it's scheduled (schedtick), system call activity (syscalltick), timers, and free goroutine slots.</p> <p>The M lines correspond to OS threads. Each line shows which goroutine\u2014if any\u2014is running on that thread, whether the thread is idle, spinning, or blocked, along with memory allocation activity and lock states.</p> <p>This view makes it easier to spot not only classic concurrency bottlenecks but also deeper issues: scheduler delays, blocking syscalls, threads that spin without doing useful work, or CPU cores that sit idle when they shouldn\u2019t. The output reveals patterns that aren\u2019t visible from logs or metrics alone.</p> <ul> <li><code>gomaxprocs=14</code>: Number of logical processors (P\u2019s).</li> <li><code>idleprocs=14</code>: All processors are idle, indicating no runnable goroutines.</li> <li><code>threads=26</code>: Number of M\u2019s (OS threads) created.</li> <li><code>spinningthreads=0</code>: No threads are actively searching for work.</li> <li><code>needspinning=0</code>: No additional spinning threads are requested by the scheduler.</li> <li><code>idlethreads=20</code>: Number of OS threads currently idle.</li> <li><code>runqueue=0</code>: Global run queue is empty.</li> <li><code>gcwaiting=false</code>: Garbage collector is not blocking execution.</li> <li><code>nmidlelocked=1</code>: One P is locked to a thread that is currently idle.</li> <li><code>stopwait=0</code>: No goroutines waiting to stop the world.</li> <li><code>sysmonwait=false</code>: The system monitor is actively running, not sleeping.</li> </ul> <p>The global run queue holds goroutines that are not bound to any specific P or that overflowed local queues. In contrast, each logical processor (P) maintains a local run queue of goroutines it is responsible for scheduling. Goroutines are preferentially enqueued locally for performance: local queues avoid lock contention and improve cache locality. It may be placed on the global queue only when a P's local queue is full, or a goroutine originates from outside a P (e.g., from a syscall).</p> <p>This dual-queue strategy reduces synchronization overhead across P\u2019s and enables efficient scheduling under high concurrency. Understanding the ratio of local vs global queue activity helps diagnose whether the system is under-provisioned, improperly balanced, or suffering from excessive cross-P migrations.</p> <p>These insights help quantify how efficiently goroutines are scheduled, how much parallelism is actually utilized, and whether the system is under- or over-provisioned in terms of logical processors. Observing these patterns under load is crucial when adjusting <code>GOMAXPROCS</code>, diagnosing tail latency, or identifying scheduler contention.</p>"},{"location":"02-networking/a-bit-more-tuning/#netpoller-deep-dive-into-epoll-on-linux-and-kqueue-on-bsd","title":"Netpoller: Deep Dive into epoll on Linux and kqueue on BSD","text":"<p>In any Go application handling high connection volumes, the network poller plays a critical behind-the-scenes role. At its core, Go uses the OS-level multiplexing facilities\u2014<code>epoll</code> on Linux and <code>kqueue</code> on BSD/macOS\u2014to monitor thousands of sockets concurrently with minimal threads. The runtime leverages these mechanisms efficiently, but understanding how and why reveals opportunities for tuning, especially under demanding loads.</p> <p>When a goroutine initiates a network operation like reading from a TCP connection, the runtime doesn't immediately block the underlying thread. Instead, it registers the file descriptor with the poller\u2014using <code>epoll_ctl</code> in edge-triggered mode or <code>EV_SET</code> with <code>EVFILT_READ</code>\u2014and parks the goroutine. The actual thread (M) becomes free to run other goroutines. When data arrives, the kernel signals the poller thread, which in turn wakes the appropriate goroutine by scheduling it onto a P\u2019s run queue. This wakeup process minimizes contention by relying on per-P notification lists and avoids runtime lock bottlenecks.</p> <p>Go uses edge-triggered notifications, which signal only on state transitions\u2014like new data becoming available. This design requires the application to drain sockets fully during each wakeup or risk missing future events. While more complex than level-triggered behavior, edge-triggered mode significantly reduces syscall overhead under load.</p> <p>Here's a simplified version of what happens under the hood during a read operation:</p> <pre><code>func pollAndRead(conn net.Conn) ([]byte, error) {\n    buf := make([]byte, 4096)\n    for {\n        n, err := conn.Read(buf)\n        if n &gt; 0 {\n            return buf[:n], nil\n        }\n        if err != nil &amp;&amp; !isTemporary(err) {\n            return nil, err\n        }\n        // Data not ready yet \u2014 goroutine will be parked until poller wakes it\n    }\n}\n</code></pre> <p>Internally, Go runs a dedicated poller thread that loops on <code>epoll_wait</code> or <code>kevent</code>, collecting batches of events (typically 512 at a time). After the call returns, the runtime processes these events, distributing wakeups across logical processors to prevent any single P from becoming a bottleneck. To further promote scheduling fairness, the poller thread may rotate across P\u2019s periodically, a behavior governed by <code>GODEBUG=netpollWaitLatency</code>.</p> <p>Go\u2019s runtime is optimized to reduce unnecessary syscalls and context switches. All file descriptors are set to non-blocking, which allows the poller thread to remain responsive. To avoid the thundering herd problem\u2014where multiple threads wake on the same socket\u2014the poller ensures only one goroutine handles a given FD event at a time.</p> <p>The design goes even further by aligning the circular event buffer with cache lines and distributing wakeups via per-P lists. These details matter at scale. With proper alignment and locality, Go reduces CPU cache contention when thousands of connections are active.</p> <p>For developers looking to inspect poller behavior, enabling tracing with <code>GODEBUG=netpoll=1</code> can surface system-level latencies and epoll activity. Additionally, the <code>GODEBUG=netpollWaitLatency=200</code> flag configures the poller\u2019s willingness to hand off to another P every 200 microseconds. That\u2019s particularly helpful in debugging idle P starvation or evaluating fairness in high-throughput systems.</p> <p>Here's a small experiment that logs event activity:</p> <pre><code>GODEBUG=netpoll=1 go run main.go\n</code></pre> <p>You\u2019ll see log lines like:</p> <pre><code>runtime: netpoll: poll returned n=3\nruntime: netpoll: waking g=102 for fd=5\n</code></pre> <p>Most developers never need to think about this machinery\u2014and they shouldn't. But these details become valuable in edge cases, like high-throughput HTTP proxies or latency-sensitive services dealing with hundreds of thousands of concurrent sockets. Tuning parameters like <code>GOMAXPROCS</code>, adjusting the event buffer size, or modifying poller wake-up intervals can yield measurable performance improvements, particularly in tail latencies.</p> <p>For example, in a system handling hundreds of thousands of concurrent HTTP/2 streams, increasing <code>GOMAXPROCS</code> while using <code>GODEBUG=netpollWaitLatency=100</code> helped reduce the 99th percentile read latency by over 15%, simply by preventing poller starvation under I/O backpressure.</p> <p>As with all low-level tuning, it's not about changing knobs blindly. It's about knowing what Go\u2019s netpoller is doing, why it\u2019s structured the way it is, and where its boundaries can be nudged for just a bit more efficiency\u2014when measurements tell you it\u2019s worth it.</p>"},{"location":"02-networking/a-bit-more-tuning/#thread-pinning-with-lockosthread-and-godebug-flags","title":"Thread Pinning with <code>LockOSThread</code> and <code>GODEBUG</code> Flags","text":"<p>Go offers tools like <code>runtime.LockOSThread()</code> to pin a goroutine to a specific OS thread, but in most real-world applications, the payoff is minimal. Benchmarks consistently show that for typical server workloads\u2014especially those that are CPU-bound\u2014Go\u2019s scheduler handles thread placement well without manual intervention. Introducing thread pinning tends to add complexity without delivering measurable gains.</p> <p>There are exceptions. In ultra-low-latency or real-time systems, pinning can help reduce jitter by avoiding thread migration. But these gains typically require isolated CPU cores, tightly controlled environments, and strict latency targets. In practice, that means bare metal. On shared infrastructure\u2014especially in cloud environments like AWS where cores are virtualized and noisy neighbors are common\u2014thread pinning rarely delivers any measurable benefit.</p> <p>If you\u2019re exploring pinning, it\u2019s not enough to assume benefit\u2014you need to benchmark it. Enabling <code>GODEBUG=schedtrace=1000,scheddetail=1</code> gives detailed insight into how goroutines are scheduled and whether contention or migration is actually a problem. Without that evidence, thread pinning is more likely to hinder than help.</p> <p>Here's how developers might pin threads cautiously:</p> <pre><code>runtime.LockOSThread()\ndefer runtime.UnlockOSThread()\n\n// perform critical latency-sensitive work here\n</code></pre> <p>Always pair such modifications with extensive metrics collection and scheduler tracing (<code>GODEBUG=schedtrace=1000,scheddetail=1</code>) to validate tangible gains over Go\u2019s robust default scheduling behavior.</p>"},{"location":"02-networking/a-bit-more-tuning/#cpu-affinity-and-external-tools","title":"CPU Affinity and External Tools","text":"<p>Using external tools like <code>taskset</code> or system calls such as <code>sched_setaffinity</code> can bind threads or processes to specific CPU cores. While theoretically beneficial for cache locality and predictable performance, extensive benchmarking consistently demonstrates limited practical value in most Go applications.</p> <p>Explicit CPU affinity management typically helps only in tightly controlled environments with:</p> <ul> <li>Real-time latency constraints (microsecond-level jitter).</li> <li>Dedicated and isolated CPUs (e.g., via Linux kernel\u2019s isolcpus).</li> <li>Avoidance of thread migration on NUMA hardware.</li> </ul> <p>Example of cautious CPU affinity usage:</p> <pre><code>func setAffinity(cpuList []int) error {\n    pid := os.Getpid()\n    var mask unix.CPUSet\n    for _, cpu := range cpuList {\n        mask.Set(cpu)\n    }\n    return unix.SchedSetaffinity(pid, &amp;mask)\n}\n\nfunc main() {\n    runtime.LockOSThread()\n    defer runtime.UnlockOSThread()\n\n    if err := setAffinity([]int{2, 3}); err != nil {\n        log.Fatalf(\"CPU affinity failed: %v\", err)\n    }\n\n    // perform critical work with confirmed benefit\n}\n</code></pre> <p>Without dedicated benchmarking and validation, these techniques may degrade performance, starve other processes, or introduce subtle latency regressions. Treat thread pinning and CPU affinity as highly specialized tools\u2014effective only after meticulous measurement confirms their benefit.</p> <p>Tuning Go at the scheduler level can unlock significant performance gains, but it demands an intimate understanding of P\u2019s, M\u2019s, and G\u2019s. Blindly upping <code>GOMAXPROCS</code> or pinning threads without measurement can backfire. the advice is to treat these knobs as surgical tools: use <code>GODEBUG</code> traces to diagnose, isolate subsystems where affinity or pinning makes sense, and always validate with benchmarks and profiles.</p> <p>Go\u2019s runtime is ever\u2011evolving. Upcoming work in preemptive scheduling and user\u2011level interrupts promises to reduce tail latency further and improve fairness. Until then, these low\u2011level levers remain some of the most powerful ways to squeeze every drop of performance from developer's Go services.</p>"},{"location":"02-networking/bench-and-load/","title":"Benchmarking and Load Testing for Networked Go Apps","text":"<p>Before you reach for a mutex-free queue or tune your goroutine pool, step back. Optimization without a baseline is just guesswork. In Go applications, performance tuning starts with understanding how your system behaves under pressure, which means benchmarking it under load.</p> <p>Load testing isn't just about pushing requests until things break. It's about simulating realistic usage patterns to extract measurable, repeatable data. That data anchors every optimization that follows.</p>"},{"location":"02-networking/bench-and-load/#test-app-simulating-fastslow-paths-and-gc-pressure","title":"Test App: Simulating Fast/Slow Paths and GC pressure","text":"<p>To benchmark meaningfully, we need endpoints that reflect different workload characteristics.</p> Show the benchmarking app <pre><code>package main\n\n// pprof-start\nimport (\n// pprof-end\n    \"flag\"\n    \"fmt\"\n    \"log\"\n    \"math/rand/v2\"\n    \"net/http\"\n// pprof-start\n    _ \"net/http/pprof\"\n// pprof-end\n    \"os\"\n    \"os/signal\"\n    \"time\"\n// pprof-start\n)\n// pprof-end\n\nvar (\n    fastDelay   = flag.Duration(\"fast-delay\", 0, \"Fixed delay for fast handler (if any)\")\n    slowMin     = flag.Duration(\"slow-min\", 1*time.Millisecond, \"Minimum delay for slow handler\")\n    slowMax     = flag.Duration(\"slow-max\", 300*time.Millisecond, \"Maximum delay for slow handler\")\n    gcMinAlloc  = flag.Int(\"gc-min-alloc\", 50, \"Minimum number of allocations in GC heavy handler\")\n    gcMaxAlloc  = flag.Int(\"gc-max-alloc\", 1000, \"Maximum number of allocations in GC heavy handler\")\n)\n\nfunc randRange(min, max int) int {\n    return rand.IntN(max-min) + min\n}\n\nfunc fastHandler(w http.ResponseWriter, r *http.Request) {\n    if *fastDelay &gt; 0 {\n        time.Sleep(*fastDelay)\n    }\n    fmt.Fprintln(w, \"fast response\")\n}\n\nfunc slowHandler(w http.ResponseWriter, r *http.Request) {\n    delayRange := int((*slowMax - *slowMin) / time.Millisecond)\n    delay := time.Duration(randRange(1, delayRange)) * time.Millisecond\n    time.Sleep(delay)\n    fmt.Fprintf(w, \"slow response with delay %d ms\\n\", delay)\n}\n\n// heavy-start\nvar longLivedData [][]byte\n\nfunc gcHeavyHandler(w http.ResponseWriter, r *http.Request) {\n    numAllocs := randRange(*gcMinAlloc, *gcMaxAlloc)\n    var data [][]byte\n    for i := 0; i &lt; numAllocs; i++ {\n        // Allocate 10KB slices. Occasionally retain a reference to simulate long-lived objects.\n        b := make([]byte, 1024*10)\n        data = append(data, b)\n        if i%100 == 0 { // every 100 allocations, keep the data alive\n            longLivedData = append(longLivedData, b)\n        }\n    }\n    fmt.Fprintf(w, \"allocated %d KB\\n\", len(data)*10)\n}\n// heavy-end\n\nfunc main() {\n    flag.Parse()\n\n    http.HandleFunc(\"/fast\", fastHandler)\n    http.HandleFunc(\"/slow\", slowHandler)\n    http.HandleFunc(\"/gc\", gcHeavyHandler)\n\n// pprof-start\n// ...\n\n    // Start pprof in a separate goroutine.\n    go func() {\n        log.Println(\"pprof listening on :6060\")\n        if err := http.ListenAndServe(\"localhost:6060\", nil); err != nil {\n            log.Fatalf(\"pprof server error: %v\", err)\n        }\n    }()\n// pprof-end\n\n    // Create a server to allow for graceful shutdown.\n    server := &amp;http.Server{Addr: \":8080\"}\n\n    go func() {\n        log.Println(\"HTTP server listening on :8080\")\n        if err := server.ListenAndServe(); err != nil &amp;&amp; err != http.ErrServerClosed {\n            log.Fatalf(\"HTTP server error: %v\", err)\n        }\n    }()\n\n    // Graceful shutdown on interrupt signal.\n    sigCh := make(chan os.Signal, 1)\n    signal.Notify(sigCh, os.Interrupt)\n    &lt;-sigCh\n    log.Println(\"Shutting down server...\")\n    if err := server.Shutdown(nil); err != nil {\n        log.Fatalf(\"Server Shutdown Failed:%+v\", err)\n    }\n    log.Println(\"Server exited\")\n}\n</code></pre> <ul> <li><code>/fast</code>: A quick response, ideal for throughput testing.</li> <li><code>/slow</code>: Simulates latency and contention.</li> <li><code>/gc</code>: Simulate GC heavy workflow.</li> <li><code>net/http/pprof</code>: Exposes runtime profiling on <code>localhost:6060</code>.</li> </ul> <p>Run it with:</p> <pre><code>go run main.go\n</code></pre>"},{"location":"02-networking/bench-and-load/#simulating-load-tools-that-reflect-reality","title":"Simulating Load: Tools That Reflect Reality","text":""},{"location":"02-networking/bench-and-load/#when-to-use-what","title":"When to Use What","text":"<p>Info</p> <p>This is by no means an exhaustive list. The ecosystem of load-testing tools is broad and constantly evolving. Tools like Apache JMeter, Locust, Artillery, and Gatling each bring their own strengths\u2014ranging from UI-driven test design to distributed execution or JVM-based scenarios. The right choice depends on your stack, test goals, and team workflow. The tools listed here are optimized for Go-based services and local-first benchmarking, but they\u2019re just a starting point.</p> <p>At a glance, <code>vegeta</code>, <code>wrk</code>, and <code>k6</code> all hammer HTTP endpoints. But they serve different roles depending on what you're testing, how much precision you need, and how complex your scenario is.</p> Tool Focus Scriptable Metrics Depth Ideal Use Case <code>vegeta</code> Constant rate load generation No (but composable) High (histogram, percentiles) Tracking latency percentiles over time; CI benchmarking <code>wrk</code> Max throughput stress tests Yes (Lua) Medium Measuring raw server capacity and concurrency limits <code>k6</code> Scenario-based simulation Yes (JavaScript) High (VU metrics, dashboards) Simulating real-world user workflows and pacing <p>Use <code>vegeta</code> when:</p> <ul> <li>You need a consistent RPS load (e.g., 100 requests/sec for the 60s).</li> <li>You're observing latency degradation under controlled pressure.</li> <li>You want structured output (histograms, percentiles) for profiling.</li> <li>You want to verify local changes before deeper profiling.</li> </ul> <p>Use <code>wrk</code> when:</p> <ul> <li>You're exploring upper-bound throughput.</li> <li>You want raw, fast load with minimal setup.</li> <li>You\u2019re profiling at high concurrency (e.g., 10k connections).</li> </ul> <p>Use <code>k6</code> when:</p> <ul> <li>You must model complex flows like login \u2192 API call \u2192 wait \u2192 logout.</li> <li>You\u2019re integrating performance tests into CI/CD.</li> <li>You want thresholds, pacing, and visual feedback.</li> </ul> <p>Each of these tools has a place in your benchmarking toolkit. Picking the right one depends on whether you're validating performance, exploring scaling thresholds, or simulating end-user behavior.</p>"},{"location":"02-networking/bench-and-load/#vegeta","title":"Vegeta","text":"<p>Vegeta is a flexible HTTP load testing tool written in Go, built for generating constant request rates. This makes it well-suited for simulating steady, sustained traffic patterns instead of sudden spikes.</p> <p>We reach for Vegeta when precision matters. It maintains exact request rates and captures detailed latency distributions, which helps track how system behavior changes under load. It\u2019s lightweight, easy to automate, and integrates cleanly into CI workflows\u2014making it a reliable option for benchmarking Go services.</p> <p>Install:</p> <pre><code>go install github.com/tsenart/vegeta@latest\n</code></pre> <p>Which endpoint(s) we are going to test:</p> <pre><code>echo \"GET http://localhost:8080/slow\" &gt; targets.txt\n</code></pre> <p>Run:</p> <pre><code>vegeta attack -rate=100 -duration=30s -targets=targets.txt | tee results.bin | vegeta report\n</code></pre> Potential output <pre><code>&gt; vegeta attack -rate=100 -duration=30s -targets=targets.txt | tee results.bin | vegeta report\nRequests      [total, rate, throughput]  3000, 100.04, 100.03\nDuration      [total, attack, wait]      29.989635542s, 29.989108333s, 527.209\u00b5s\nLatencies     [mean, 50, 95, 99, max]    524.563\u00b5s, 504.802\u00b5s, 793.997\u00b5s, 1.47362ms, 7.351541ms\nBytes In      [total, mean]              42000, 14.00\nBytes Out     [total, mean]              0, 0.00\nSuccess       [ratio]                    100.00%\nStatus Codes  [code:count]               200:3000\nError Set:\n</code></pre> <p>View percentiles:</p> <pre><code>vegeta report -type='hist[0,10ms,50ms,100ms,200ms,500ms,1s]' &lt; results.bin\n</code></pre> <p>Generate chart:</p> <pre><code>vegeta plot &lt; results.bin &gt; plot.html\n</code></pre> Testing Multiple Endpoints with Vegeta <p>Depending on your goals, there are two recommended approaches for testing both <code>/fast</code> and <code>/slow</code> endpoints in a single run.</p> <p>Option 1: Round-Robin Between Endpoints</p> <p>Create a <code>targets.txt</code> with both endpoints:</p> <pre><code>cat &gt; targets.txt &lt;&lt;EOF\nGET http://localhost:8080/fast\nGET http://localhost:8080/slow\nEOF\n</code></pre> <p>Run the test:</p> <pre><code>vegeta attack -rate=50 -duration=30s -targets=targets.txt | tee mixed-results.bin | vegeta report -type='hist[0,50ms,100ms,200ms,500ms,1s,2s]'\n</code></pre> <ul> <li>Requests are randomly distributed between the two endpoints.</li> <li>Useful for observing aggregate behavior of mixed traffic.</li> <li>Easy to set up and analyze combined performance.</li> </ul> <p>Option 2: Weighted Mix Using Multiple Vegeta Runs</p> <p>To simulate different traffic proportions (e.g., 80% fast, 20% slow):</p> <pre><code># Send 80% of requests to /fast\nvegeta attack -rate=40 -duration=30s -targets=&lt;(echo \"GET http://localhost:8080/fast\") &gt; fast.bin &amp;\n\n# Send 20% of requests to /slow\nvegeta attack -rate=10 -duration=30s -targets=&lt;(echo \"GET http://localhost:8080/slow\") &gt; slow.bin &amp;\n\nwait\n</code></pre> <p>Then merge the results and generate a report:</p> <pre><code>vegeta encode fast.bin slow.bin &gt; combined.bin\nvegeta report -type='hist[0,50ms,100ms,200ms,500ms,1s,2s]' &lt; combined.bin\n</code></pre> <ul> <li>Gives you precise control over traffic distribution.</li> <li>Better for simulating realistic traffic mixes.</li> <li>Enables per-endpoint benchmarking when analyzed separately.</li> </ul> <p>Both methods are valid\u2014choose based on whether you need simplicity or control.</p>"},{"location":"02-networking/bench-and-load/#wrk","title":"wrk","text":"<p>wrk is a high-performance HTTP benchmarking tool written in C. It's designed for raw speed and concurrency, making it ideal for stress testing your server\u2019s throughput and connection handling capacity.</p> <p>We use <code>wrk</code> when we want to push the system to its upper limits. It excels at flooding endpoints with high request volumes using multiple threads and connections. While it doesn\u2019t offer detailed percentiles like <code>vegeta</code>, it's perfect for quick saturation tests and measuring how much traffic your Go server can handle before it starts dropping requests or stalling.</p> <p>Install:</p> <pre><code>brew install wrk  # or build from source\n</code></pre> <p>Run test:</p> <pre><code>wrk -t4 -c100 -d30s http://localhost:8080/fast\n</code></pre> Potential output <pre><code>&gt; wrk -t4 -c100 -d30s http://localhost:8080/fast\nRunning 30s test @ http://localhost:8080/fast\n  4 threads and 100 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     1.29ms  255.31us   5.24ms   84.86%\n    Req/Sec    19.30k   565.16    21.93k    77.92%\n  2304779 requests in 30.00s, 287.94MB read\nRequests/sec:  76823.88\nTransfer/sec:      9.60MB\n</code></pre>"},{"location":"02-networking/bench-and-load/#k6","title":"k6","text":"<p>k6 is a modern load testing tool built around scripting realistic client behavior in JavaScript. It\u2019s designed for simulating time-based load profiles\u2014ramp-up, steady-state, ramp-down\u2014and supports custom flows, pacing, and threshold-based validation.</p> <p>We use <code>k6</code> when raw throughput isn\u2019t enough and we need to simulate how real users interact with the system. It handles chained requests, models session-like flows, and supports stage-based testing out of the box. With rich metrics and seamless CI/CD integration, <code>k6</code> helps surface regressions before they reach production.</p> <p>Install:</p> <pre><code>brew install k6\n</code></pre> <p>Script:</p> <pre><code>// script.js\nimport http from 'k6/http';\nimport { sleep } from 'k6';\n\nexport const options = {\n  stages: [\n    { duration: '10s', target: 50 },\n    { duration: '30s', target: 50 },\n    { duration: '10s', target: 0 },\n  ],\n};\n\nexport default function () {\n  http.get('http://localhost:8080/fast');\n  sleep(1);\n}\n</code></pre> <p>Run:</p> <pre><code>k6 run script.js\n</code></pre> Potential output <pre><code>&gt; k6 run script.js\n\n         /\\      Grafana   /\u203e\u203e/\n    /\\  /  \\     |\\  __   /  /\n   /  \\/    \\    | |/ /  /   \u203e\u203e\\\n  /          \\   |   (  |  (\u203e)  |\n / __________ \\  |_|\\_\\  \\_____/\n\n     execution: local\n        script: script.js\n        output: -\n\n     scenarios: (100.00%) 1 scenario, 50 max VUs, 1m20s max duration (incl. graceful stop):\n              * default: Up to 50 looping VUs for 50s over 3 stages (gracefulRampDown: 30s, gracefulStop: 30s)\n\n\n  \u2588 TOTAL RESULTS\n\n    HTTP\n    http_req_duration.......................................................: avg=495.55\u00b5s min=116\u00b5s med=449\u00b5s max=5.49ms p(90)=705\u00b5s p(95)=820.39\u00b5s\n      { expected_response:true }............................................: avg=495.55\u00b5s min=116\u00b5s med=449\u00b5s max=5.49ms p(90)=705\u00b5s p(95)=820.39\u00b5s\n    http_req_failed.........................................................: 0.00%  0 out of 2027\n    http_reqs...............................................................: 2027   40.146806/s\n\n    EXECUTION\n    iteration_duration......................................................: avg=1s       min=1s    med=1s    max=1.01s  p(90)=1s    p(95)=1s\n    iterations..............................................................: 2027   40.146806/s\n    vus.....................................................................: 3      min=3         max=50\n    vus_max.................................................................: 50     min=50        max=50\n\n    NETWORK\n    data_received...........................................................: 266 kB 5.3 kB/s\n    data_sent...............................................................: 176 kB 3.5 kB/s\n\n\n\n\nrunning (0m50.5s), 00/50 VUs, 2027 complete and 0 interrupted iterations\ndefault \u2713 [======================================] 00/50 VUs  50s\n</code></pre>"},{"location":"02-networking/bench-and-load/#profiling-networked-go-applications-with-pprof","title":"Profiling Networked Go Applications with <code>pprof</code>","text":"<p>Profiling Go applications that heavily utilize networking is crucial to identifying and resolving bottlenecks that impact performance under high-traffic scenarios. Go's built-in <code>net/http/pprof</code> package provides insights specifically beneficial for network-heavy operations. Set up continuous profiling by enabling an HTTP endpoint:</p> <pre><code>import (\n\n    _ \"net/http/pprof\"\n\n)\n\n// ...\n\n    // Start pprof in a separate goroutine.\n    go func() {\n        log.Println(\"pprof listening on :6060\")\n        if err := http.ListenAndServe(\"localhost:6060\", nil); err != nil {\n            log.Fatalf(\"pprof server error: %v\", err)\n        }\n    }()\n</code></pre> <p>It's possible to inspect the application in real time, even under heavy load. To capture a CPU profile:</p> <pre><code>go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30\n</code></pre> <p>This grabs a 30-second snapshot of CPU usage. With the <code>pprof</code> HTTP server exposed, all runtime data\u2014CPU, memory, goroutines, contention, flamegraphs\u2014is available without pausing or restarting the application.</p> <pre><code>go tool pprof -http=:7070 cpu.prof #(1)\n</code></pre> <ol> <li>the actual <code>cpu.prof</code> path will be something like <code>$HOME/pprof/pprof.net-app.samples.cpu.004.pb.gz</code></li> </ol>"},{"location":"02-networking/bench-and-load/#cpu-profiling","title":"CPU Profiling","text":"<p>Profiling a system at rest rarely tells the full story. Real bottlenecks show up under pressure\u2014when requests stack up, threads compete, and memory churn increases. CPU profiling during load reveals where execution time concentrates, often exposing slow serialization, inefficient handler logic, or contention between goroutines. These are the paths that quietly limit throughput and inflate latency when traffic scales.</p>"},{"location":"02-networking/bench-and-load/#what-to-look-for","title":"What to Look For","text":"<ul> <li>Network Serialization Hotspots: Frequent use of <code>json.Marshal</code> or similar serialization methods during network response generation.</li> <li>Syscall Overhead: Extensive syscall usage (e.g., <code>syscall.Read</code>) suggesting inefficient socket handling or excessive blocking I/O.</li> <li>GC Activity: High frequency of <code>runtime.gc</code> indicating inefficient memory management impacting response latency.</li> </ul> <p>Why This Matters: Identifying and optimizing CPU-intensive operations in networking contexts reduces latency, boosts throughput, and improves reliability during traffic spikes.</p>"},{"location":"02-networking/bench-and-load/#flamegraphs","title":"Flamegraphs","text":"<p>Flamegraphs provide a visual summary of where CPU time is spent by aggregating and collapsing stack traces into a single view. They make it easy to identify performance hotspots without digging through raw profiling data. In server applications, this often highlights issues in request handling, serialization, or blocking I/O. Under load, flamegraphs are especially useful for catching subtle inefficiencies that scale into major performance problems.</p>"},{"location":"02-networking/bench-and-load/#what-to-look-for_1","title":"What to Look For","text":"<ul> <li>Functions related to network I/O or data transfer that appear as wide blocks in a flamegraph often point to excessive time spent on serialization, buffering, or socket operations.</li> <li>Deep Call Chains Deep stacks could reveal inefficient middleware or unnecessary layers in network request handling.</li> <li>Unexpected Paths Look for unexpected serialization, reflection, or routing inefficiencies.</li> </ul> <p>Why This Matters: Flamegraphs simplify diagnosing complex inefficiencies visually, leading to quicker optimization and reduced downtime.</p>"},{"location":"02-networking/bench-and-load/#managing-garbage-collection-gc-pressure","title":"Managing Garbage Collection (GC) Pressure","text":"<p>Memory profiling helps identify where your application is wasting heap space, especially in network-heavy code paths. Common issues include repeated allocation of response buffers, temporary objects, or excessive use of slices and maps. These patterns often go unnoticed until they trigger GC pressure or latency under load. Profiling with <code>pprof</code> follows the same basic steps as CPU profiling, making it easy to integrate into your existing workflow.</p> <pre><code>go tool pprof http://localhost:6060/debug/pprof/heap\n</code></pre> <p>Then, again, you can view results interactively.</p> <pre><code>go tool pprof -http=:7070 mem.prof #(1)\n</code></pre> <ol> <li>the actual <code>mem.prof</code> path will be something like <code>$HOME/pprof/pprof.net-app.alloc_objects.alloc_space.inuse_objects.inuse_space.003.pb.gz</code></li> </ol>"},{"location":"02-networking/bench-and-load/#what-to-look-for_2","title":"What to Look For","text":"<ul> <li>Frequent Temporary Buffers: High frequency of allocations in network buffers, such as repeatedly creating byte slices for each request.</li> <li>Persistent Network Objects: Accumulation of long-lived network connections or sessions.</li> <li>Excessive Serialization Overhead: High object creation rate due to repeated encoding/decoding of network payloads.</li> </ul> <p>Example: Optimizing buffer reuse using <code>sync.Pool</code> greatly reduces GC pressure during high-volume network operations.</p> <p>Why This Matters: Reducing memory churn from network activities improves response times and minimizes latency spikes caused by GC.</p>"},{"location":"02-networking/bench-and-load/#identifying-cpu-bottlenecks","title":"Identifying CPU Bottlenecks","text":"<p>Networked applications often hit CPU limits first when pushed under sustained load. Profiling helps surface where time is actually being spent and what\u2019s getting in the way.</p>"},{"location":"02-networking/bench-and-load/#what-to-look-for_3","title":"What to Look For","text":"<ul> <li>Latency Rising While Throughput Stalls: A sign the CPU is saturated, often from request processing or serialization overhead.</li> <li>Scheduler Overhead (runtime.schedule, mcall): Too many goroutines can overwhelm the scheduler, especially when each connection gets its own handler.</li> <li>Lock Contention: Repeated locking on shared network state or blocking channel operations slows down throughput and limits parallelism.</li> </ul> <p>For example, if profiling shows excessive time spent in TLS handshake routines, the fix might involve moving handshakes off the hot path or reducing handshake frequency with connection reuse.</p> <p>Why it matters: CPU bottlenecks cap your ability to scale. Fixing them is often the difference between a system that handles 5K clients and one that handles 50K.</p>"},{"location":"02-networking/bench-and-load/#practicle-example-of-profiling-networked-go-applications-with-pprof","title":"Practicle example of Profiling Networked Go Applications with <code>pprof</code>","text":"<p>To illustrate these concepts practically, our demo application integrates profiling and benchmarking tools and provides comprehensive profiling and load testing scenarios. The demo covers identifying performance bottlenecks, analyzing flame graphs, and benchmarking under various simulated network conditions.</p> <p>Due to its significant size, Practicle example of Profiling Networked Go Applications with <code>prof</code> is a separate article.</p>"},{"location":"02-networking/bench-and-load/#benchmarking-as-a-feedback-loop","title":"Benchmarking as a Feedback Loop","text":"<p>A single load test run means little in isolation. But if you treat benchmarking as part of your development cycle\u2014before and after changes\u2014you start building a performance narrative. You can see exactly how a change impacted throughput or whether it traded latency for memory overhead.</p> <p>The Go standard library gives you <code>testing.B</code> for microbenchmarks. Combine profiling with robust integration testing as part of your CI/CD pipeline using tools like <code>Vegeta</code> and <code>k6</code>. This practice ensures early detection of regressions, continuous validation of performance enhancements, and reliable application performance maintenance under realistic production conditions.</p>"},{"location":"02-networking/connection_observability/","title":"Connection Lifecycle Observability: From Dial to Close","text":"<p>Many observability systems expose only high-level HTTP metrics, but deeper insight comes from explicitly instrumenting each stage \u2014 DNS resolution, dialing, handshake, negotiation, reads/writes, and teardown. Observing the full lifecycle of a network connection provides the clarity needed to diagnose latency issues, identify failures, and relate resource usage to external behavior.</p>"},{"location":"02-networking/connection_observability/#dns-resolution","title":"DNS Resolution","text":"<p>Every outbound connection begins with a name resolution step unless an IP is already known. DNS latency and failures often dominate connection setup time and can vary significantly depending on caching and resolver performance.</p> <p>Capturing DNS resolution duration, errors and resulting address provides visibility into one of the least predictable phases. At this stage, it is valuable to:</p> <ul> <li>measure the time from query issuance to receiving an answer</li> <li>log the resolved IP addresses</li> <li>surface transient and permanent failures distinctly</li> </ul> <p>In Go, this can be achieved by wrapping the <code>net.Resolver</code>.</p> <pre><code>import (\n    \"context\"\n    \"log\"\n    \"net\"\n    \"time\"\n)\n\nfunc resolveWithTracing(ctx context.Context, hostname string) ([]string, error) {\n    start := time.Now()\n    ips, err := net.DefaultResolver.LookupHost(ctx, hostname)\n    elapsed := time.Since(start)\n\n    log.Printf(\"dns: host=%s duration=%s ips=%v err=%v\", hostname, elapsed, ips, err)\n    return ips, err\n}\n</code></pre> <p>This explicit measurement avoids relying on opaque metrics exposed by the OS resolver or libraries.</p>"},{"location":"02-networking/connection_observability/#dialing","title":"Dialing","text":"<p>After obtaining an address, the next phase is dialing \u2014 establishing a TCP (or other transport) connection. Here, round-trip latency to the target, route stability, and ephemeral port exhaustion can all surface.</p> <p>Observing the dial phase often involves intercepting <code>net.Dialer</code>.</p> <pre><code>func dialWithTracing(ctx context.Context, network, addr string) (net.Conn, error) {\n    var d net.Dialer\n    start := time.Now()\n    conn, err := d.DialContext(ctx, network, addr)\n    elapsed := time.Since(start)\n\n    log.Printf(\"dial: addr=%s duration=%s err=%v\", addr, elapsed, err)\n    return conn, err\n}\n</code></pre> <p>Why trace here? Dialing failures can indicate downstream unavailability, but also local issues like SYN flood protection(1) or bad routing. Without a per-dial timestamp and error trace, identifying the locus of failure is guesswork.</p> <ol> <li>When dialing fails, sometimes it\u2019s not because of the network or the server being down, but because the local machine refuses to open more connections due to resource exhaustion or protective rate limiting.</li> </ol>"},{"location":"02-networking/connection_observability/#handshake-and-negotiation","title":"Handshake and Negotiation","text":"<p>For secure connections, the next stage \u2014 cryptographic handshake \u2014 dominates. TLS negotiation can involve multiple round-trips, certificate validation, and cipher negotiation. Measuring this stage separately is necessary because it isolates pure network latency (dial) from cryptographic and policy enforcement costs (handshake).</p> <p>The <code>crypto/tls</code> library in Go allows instrumentation of the handshake explicitly.</p> <pre><code>import (\n    \"crypto/tls\"\n)\n\nfunc handshakeWithTracing(conn net.Conn, config *tls.Config) (*tls.Conn, error) {\n    tlsConn := tls.Client(conn, config)\n    start := time.Now()\n    err := tlsConn.Handshake()\n    elapsed := time.Since(start)\n\n    log.Printf(\"handshake: duration=%s err=%v\", elapsed, err)\n    return tlsConn, err\n}\n</code></pre> <p>A common misconception is that slow TLS handshakes always reflect bad network conditions; in practice, slow certificate validation or OCSP/CRL checks are frequently to blame. Separating these phases helps pinpoint the cause.</p>"},{"location":"02-networking/connection_observability/#application-reads-and-writes","title":"Application Reads and Writes","text":"<p>Once the connection is established and negotiated, application-level traffic proceeds as reads and writes. Observability at this stage is often the least precise yet most critical for correlating client-perceived latency to backend processing.</p> <p>Instrumenting reads and writes directly yields fine-grained latency and throughput metrics. Wrapping a connection is a common strategy.</p> <pre><code>type tracedConn struct {\n    net.Conn\n}\n\nfunc (c *tracedConn) Read(b []byte) (int, error) {\n    start := time.Now()\n    n, err := c.Conn.Read(b)\n    elapsed := time.Since(start)\n\n    log.Printf(\"read: bytes=%d duration=%s err=%v\", n, elapsed, err)\n    return n, err\n}\n\nfunc (c *tracedConn) Write(b []byte) (int, error) {\n    start := time.Now()\n    n, err := c.Conn.Write(b)\n    elapsed := time.Since(start)\n\n    log.Printf(\"write: bytes=%d duration=%s err=%v\", n, elapsed, err)\n    return n, err\n}\n</code></pre> <p>Why measure at this granularity? Reads and writes can block for reasons unrelated to connection establishment \u2014 buffer backpressure, TCP window exhaustion, or application pauses. Tracing them reveals whether observed slowness is due to I/O contention or upstream latency.</p> <p>Warning</p> <p>Such a granularity level can affect your system performance. See Why reduce logs? for more details.</p>"},{"location":"02-networking/connection_observability/#teardown","title":"Teardown","text":"<p>Finally, connection closure often receives little attention, yet it can affect resource usage and connection pool behavior. Observing how long a connection takes to close and whether any errors surface during <code>Close()</code> is also useful. For TCP, the closure can be delayed by the OS until all <code>FIN</code>/<code>ACK</code> sequences complete, and sockets can linger in <code>TIME_WAIT</code>(1). Explicitly logging teardown metrics helps identify resource leaks.</p> <ol> <li>Waiting for enough time to pass to be sure that all remaining packets on the connection have expired.</li> </ol> <pre><code>func closeWithTracing(c net.Conn) error {\n    start := time.Now()\n    err := c.Close()\n    elapsed := time.Since(start)\n\n    log.Printf(\"close: duration=%s err=%v\", elapsed, err)\n    return err\n}\n</code></pre>"},{"location":"02-networking/connection_observability/#correlating-spans-and-errors","title":"Correlating Spans and Errors","text":"<p>Capturing individual phase durations is not enough unless correlated into a coherent trace. Using a context-aware tracing library allows attaching phase spans to a single logical transaction.</p> <p>For example, with OpenTelemetry:</p> <pre><code>import (\n    \"go.opentelemetry.io/otel/trace\"\n)\n\nfunc tracePhase(ctx context.Context, tracer trace.Tracer, phase string, fn func() error) error {\n    ctx, span := tracer.Start(ctx, phase)\n    defer span.End()\n    return fn()\n}\n</code></pre> <p>Wrapping each phase in a span enables stitching the timeline together and correlating specific errors with their originating stage. This level of detail is important when diagnosing production outages where symptoms often manifest far from their root cause.</p>"},{"location":"02-networking/connection_observability/#detecting-and-explaining-hangs","title":"Detecting and Explaining Hangs","text":"<p>One of the more insidious problems in distributed systems is a hung connection \u2014 no forward progress but no immediate failure. Granular observability can distinguish between a hang during DNS, an unresponsive handshake, a blocked write due to full buffers, or a delayed FIN. For example, if the dial phase logs complete but handshake never finishes, attention should focus on certificate validation, MTU black holes, or middlebox interference. By maintaining per-phase timeouts and alerts based on historical baselines, such conditions can be detected and explained rather than simply categorized as \"slow.\" This explains not just what failed, but why it failed.</p>"},{"location":"02-networking/connection_observability/#beyond-logs-metrics-and-structured-events","title":"Beyond Logs: Metrics and Structured Events","text":"<p>Most likely you will need to reduce the volume of logs without losing essential information is critical for operating at scale. High-frequency, detailed logging of connection lifecycle events incurs I/O, CPU, and storage overhead. Without control mechanisms, it can overwhelm log aggregation systems, inflate costs, and even obscure the signal with noise. Log verbosity must therefore be managed carefully \u2014 keeping enough detail for diagnosis but avoiding excessive volume.</p> <p>Zap is used in these examples to demonstrate strategies for runtime configurability and structured logging, but the same principles apply to any capable logging library. The ability to change levels dynamically, emit structured data, and sample events makes it easier to implement these strategies.</p>"},{"location":"02-networking/connection_observability/#why-reduce-logs","title":"Why reduce logs?","text":"<p>Logging at fine granularity across thousands of connections per second can quickly generate terabytes of data, with much of it being repetitive and low-value. High log throughput can also cause contention with the application\u2019s main execution paths, affecting latency. Furthermore, unbounded log growth burdens operators, complicates incident triage, and increases retention costs.</p>"},{"location":"02-networking/connection_observability/#techniques-to-control-log-volume","title":"Techniques to control log volume","text":"<ul> <li>Configurable log levels: adjust verbosity at runtime depending on the situation \u2014 keeping production at <code>INFO</code> or <code>WARN</code> by default, switching to <code>DEBUG</code> or <code>TRACE</code> during investigation.</li> <li>Sampling: log only a subset of events under high load or log every Nth connection. Zap supports samplers that enforce rate limits and randomness.</li> <li>Metrics-first, logs-for-anomalies: promote phase durations and counters to metrics (e.g., Prometheus) and emit logs only when thresholds or percentiles are breached.</li> <li>Aggregate phase data: collect all phase timings and results per connection, then emit a single structured log event at the end instead of logging each phase as a separate line.</li> </ul> <p>By combining these approaches, it is possible to retain meaningful observability with manageable overhead. Zap\u2019s API simply illustrates how to implement these patterns effectively \u2014 the key takeaway is that the design itself, not the library choice, determines observability quality and cost.</p> <p>Here are a few illustrative code snippets for these techniques:</p>"},{"location":"02-networking/connection_observability/#configurable-log-level","title":"Configurable log level","text":"<pre><code>atomicLevel := zap.NewAtomicLevel()\nlogger := zap.New(zapcore.NewCore(\n    zapcore.NewJSONEncoder(zap.NewProductionEncoderConfig()),\n    zapcore.AddSync(os.Stdout),\n    atomicLevel,\n))\n\n// Adjust at runtime\natomicLevel.SetLevel(zap.WarnLevel)\n</code></pre>"},{"location":"02-networking/connection_observability/#sampling-logs","title":"Sampling logs","text":"<pre><code>core := zapcore.NewSamplerWithOptions(\n    logger.Core(),\n    time.Second,\n    100, // first 100 per second\n    10,  // thereafter\n)\nlogger := zap.New(core)\n</code></pre>"},{"location":"02-networking/connection_observability/#aggregate-per-connection-log","title":"Aggregate per-connection log","text":"<pre><code>phases := map[string]time.Duration{\n    \"dns\": 10 * time.Millisecond,\n    \"dial\": 40 * time.Millisecond,\n    \"handshake\": 50 * time.Millisecond,\n}\nlogger.Info(\"connection completed\", zap.Any(\"phases\", phases))\n</code></pre> <p>While log statements and spans provide valuable information, a production-grade observability system benefits greatly from structured metrics collection and structured logging.</p> <p>Prometheus is well-suited for collecting and querying time-series metrics from each phase, exposing them to dashboards and alerting rules. Instead of embedding phase durations in unstructured logs, it is advisable to design and expose clearly named metrics like <code>connection_dns_duration_seconds</code>, <code>connection_dial_errors_total</code>, and similar counters. These names are proposed examples that follow Prometheus naming conventions and should be defined explicitly in the implementation to ensure clarity and consistency.</p> <p>For structured logging, libraries such as Uber\u2019s <code>zap</code> offer fast, JSON-encoded logs that integrate cleanly with log aggregation systems like ELK or Splunk. Rather than free-form <code>Printf</code>, one can emit structured key-value pairs:</p> <pre><code>import (\n    \"go.uber.org/zap\"\n)\n\nvar logger, _ = zap.NewProduction()\n\nlogger.Info(\"dns\",\n    zap.String(\"host\", hostname),\n    zap.Duration(\"duration\", elapsed),\n    zap.Strings(\"ips\", ips),\n    zap.Error(err),\n)\n</code></pre> <p>Structured logs reduce the burden on downstream parsers and make correlating events between services more reliable. Combining these techniques \u2014 phase-level instrumentation, Prometheus metrics, and structured logs \u2014 transforms opaque failures into actionable insight. With this level of granularity and consistency, ambiguous symptoms become precise diagnoses, ensuring that both developers and operators can pinpoint not just that something is wrong, but exactly where and why.</p>"},{"location":"02-networking/dns_performance/","title":"Tuning DNS Performance in Go Services","text":"<p>DNS resolution tends to fly under the radar, but it can still slow down Go applications. Even brief delays in lookups can stack up in distributed or microservice architectures where components frequently communicate. Understanding how Go resolves DNS under the hood \u2014 and how to adjust it \u2014 can make your service more responsive and reliable.</p>"},{"location":"02-networking/dns_performance/#how-dns-resolution-works-in-go-cgo-vs-native-resolver","title":"How DNS Resolution Works in Go: cgo vs. Native Resolver","text":"<p>Go supports two different ways of handling DNS queries: the built-in pure-Go resolver and the cgo-based resolver.</p> <p>The <code>pure-Go</code> resolver is fully self-contained and avoids using any external DNS libraries. It reads its configuration from <code>/etc/resolv.conf</code> (on Unix-like systems) and talks directly to the configured nameservers. This makes it simple and generally performant, though it sometimes struggles to handle more exotic or highly customized DNS environments.</p> <p>In contrast, the cgo-based resolver delegates DNS resolution to the operating system\u2019s own resolver (through the C standard library, <code>libc</code>). This gives better compatibility with complicated or custom DNS environments\u2014like those involving LDAP or multicast DNS\u2014but it also comes with a tradeoff. The cgo resolver adds overhead due to calls into external C libraries, and it can sometimes lead to issues around thread safety or unpredictable latency spikes.</p> <p>It's possible to explicitly configure Go to prefer the pure-Go resolver using an environment variable:</p> <pre><code>export GODEBUG=netdns=go\n</code></pre> <p>Alternatively, force the use of cgo resolver:</p> <pre><code>export GODEBUG=netdns=cgo\n</code></pre>"},{"location":"02-networking/dns_performance/#runtime-dependencies","title":"Runtime Dependencies","text":"<p>Enabling cgo changes how the Go binary interacts with the operating system. With cgo turned on, the binary no longer stands alone \u2014 it links dynamically to <code>libc</code> and the system loader, which <code>ldd</code> reveals in its output.</p> <pre><code>$ ldd ./app-cgo\n    linux-vdso.so.1 (0x0000fa34ddbad000)\n    libc.so.6 =&gt; /lib/aarch64-linux-gnu/libc.so.6 (0x0000fa34dd9b0000)\n    /lib/ld-linux-aarch64.so.1 (0x0000fa34ddb70000)\n</code></pre> <p>A cgo-enabled binary relies on the system\u2019s C runtime (<code>libc.so.6</code>) and the dynamic loader (<code>ld-linux</code>). Without these shared libraries available on the host, the binary won\u2019t start \u2014 which makes it unsuitable for stripped-down environments like scratch containers.</p> <p>By contrast, a pure-Go binary is completely self-contained and statically linked. If you run <code>ldd</code> on it, you\u2019ll simply see:</p> <pre><code>$ ldd ./app-pure\n    not a dynamic executable\n</code></pre> <p>This shows that all the code the binary needs is already baked in, with no dependency on shared libraries at runtime. Because of this, pure-Go builds are a good fit for minimal containers or bare environments without a C runtime, offering better portability and fewer operational surprises. The downside is that these binaries can\u2019t take advantage of system-level resolver features that require cgo and the host\u2019s <code>libc</code>.</p>"},{"location":"02-networking/dns_performance/#dns-caching-why-and-when","title":"DNS Caching: Why and When","text":"<p>Caching DNS results prevents the application from sending the same queries over and over, which can eliminate a noticeable amount of latency. Each lookup incurs at least one network round-trip, and when this happens at scale, the cumulative cost can become a serious drag on service performance.</p> <p>Many operating systems and hosting environments already include some form of DNS caching. On Windows, the <code>DNS Client</code> service keeps a local cache; on macOS, <code>mDNSResponder</code> handles this; and on most Linux systems, <code>systemd-resolved</code> or <code>nscd</code> often provides a caching layer. In cloud environments, DNS queries are often routed through a nearby caching resolver inside the same data center. These mechanisms do help reduce latency, but they operate transparently to the application and offer little visibility or control over TTLs and policies. On Linux, if <code>systemd-resolved</code> isn\u2019t enabled, no caching happens at all unless you configure it explicitly.</p> <p>Since Go itself doesn\u2019t include any DNS caching, you have to implement it yourself if you want fine-grained, application-level control. One option is to run your own caching DNS server nearby; another is to embed a lightweight cache directly in your code, using a third-party library.</p> <p>Example using go-cache for a simple DNS cache:</p> <pre><code>import (\n    \"github.com/patrickmn/go-cache\"\n    \"net\"\n    \"time\"\n)\n\nvar dnsCache = cache.New(5*time.Minute, 10*time.Minute)\n\nfunc LookupWithCache(host string) ([]net.IP, error) {\n    if cachedIPs, found := dnsCache.Get(host); found {\n        return cachedIPs.([]net.IP), nil\n    }\n\n    ips, err := net.LookupIP(host)\n    if err != nil {\n        return nil, err\n    }\n    dnsCache.Set(host, ips, cache.DefaultExpiration)\n    return ips, nil\n}\n</code></pre> <p>Overdoing DNS caching has its downsides \u2014 it can leave you serving stale records and make your service fragile if upstream addresses change. It\u2019s worth tuning your cache expiration times so they reflect how often the domains you rely on actually change.</p>"},{"location":"02-networking/dns_performance/#using-custom-dialers-and-pre-resolved-ips","title":"Using Custom Dialers and Pre-resolved IPs","text":"<p>In latency-sensitive services, it often makes sense to resolve DNS up front or use a custom dialer to control resolution explicitly. Every call to <code>net.Dial</code> or <code>net.DialContext</code> with a hostname triggers a lookup, which can involve syscalls, context switches, and sometimes even a network round-trip if the cache is cold. At high throughput, this overhead adds up.</p> <p>To eliminate this overhead, you can resolve hostnames during startup and save the resulting IPs. This approach is particularly effective when dealing with a fixed set of endpoints that rarely change.</p> <pre><code>var serviceAddr string\n\nfunc init() {\n    ips, err := net.LookupIP(\"api.example.com\")\n    if err != nil || len(ips) == 0 {\n        panic(\"Unable to resolve api.example.com\")\n    }\n    serviceAddr = ips[0].String() // in real code, consider picking an IP randomly, prefer IPv4 if needed, or iterate over ips with checks as appropriate\n}\n</code></pre> <p>One drawback is that it fixes the IP for the lifetime of the process, so if the endpoint changes its address, connections may start failing. To handle this gracefully, you can run a background goroutine that refreshes the resolved IP at regular intervals.</p> <p>Custom dialers take it one step further: they allow you to control how DNS resolution and socket establishment occur on a per-connection basis. This will enable you to force connections through a specific resolver, hardwire pre-resolved IP addresses, or even bypass DNS completely by dialing IP addresses directly.</p> <pre><code>import (\n    \"net\"\n    \"context\"\n    \"time\"\n)\n\nvar dialer = &amp;net.Dialer{\n    Timeout:   5 * time.Second,\n    KeepAlive: 30 * time.Second,\n    Resolver: &amp;net.Resolver{\n        PreferGo: true,\n        Dial: func(ctx context.Context, network, address string) (net.Conn, error) {\n            return net.Dial(network, \"8.8.8.8:53\")\n        },\n    },\n}\n\nfunc ConnectWithCustomDialer(ctx context.Context, address string) (net.Conn, error) {\n    return dialer.DialContext(ctx, \"tcp\", address)\n}\n</code></pre> <p>Custom dialers can also bypass unreliable system resolvers or direct DNS queries through a dedicated, faster nameserver. They give you precise control over how resolution happens, but that control comes with added complexity \u2014 you\u2019ll need to handle fallback and refresh logic yourself.</p>"},{"location":"02-networking/dns_performance/#metrics-and-debugging-real-world-dns-slowdowns","title":"Metrics and Debugging Real-world DNS Slowdowns","text":"<p>Identifying and troubleshooting DNS-induced latency requires insightful metrics and targeted debugging techniques. Measuring DNS is not just about seeing how fast it is \u2014 it helps answer deeper questions: are lookups hitting OS or provider caches? is the network path to the DNS server flaky? are specific nameservers slower than others? are IPv6 and IPv4 behaving differently? Without visibility, DNS issues can silently degrade performance and reliability.</p>"},{"location":"02-networking/dns_performance/#metrics","title":"Metrics","text":"<p>Make sure to track DNS resolution times in your service metrics so you can see how much time lookups actually add to each request. The simplest way is to wrap your DNS calls with a timer that starts just before the lookup and records the duration after it completes. Over time, these measurements help you identify trends, spot intermittent slowness, and correlate DNS delays with other parts of your system.</p> <pre><code>start := time.Now()\nips, err := net.LookupIP(\"example.com\")\nduration := time.Since(start)\n\nrecordDNSLookupDuration(\"example.com\", duration)\n</code></pre> <p>Looking at high-percentile latencies \u2014 like the 95th or 99th \u2014 can reveal sporadic slowdowns or flaky DNS behavior that averages might hide.</p>"},{"location":"02-networking/dns_performance/#debugging-tips","title":"Debugging Tips","text":"<p>When facing unexplained latency spikes, leverage Go\u2019s built-in debug mode:</p> <pre><code>export GODEBUG=netdns=2\n</code></pre> <p>Enabling this produces detailed DNS query logs that show exactly how each request is handled from start to finish. You can see when a server responds slowly, when lookups fail and trigger retries, or when the runtime unexpectedly falls back to the cgo resolver. Such detailed insight makes it much easier to pinpoint elusive DNS problems that standard metrics often miss.</p>"},{"location":"02-networking/dns_performance/#advanced-dns-performance-tips","title":"Advanced DNS Performance Tips","text":"<p>Running a local DNS caching resolver, such as <code>dnsmasq</code> or <code>Unbound</code>, close to your service can eliminate the extra latency of external lookups. If security and privacy are concerns, enabling DNS-over-HTTPS (DoH) or DNS-over-TLS (DoT) is also an option, though it comes with some additional latency due to encryption. Finally, reviewing and tuning your <code>/etc/resolv.conf</code> \u2014 adjusting retry counts and timeout settings \u2014 helps ensure the resolver behaves predictably under load or failure conditions.</p>"},{"location":"02-networking/efficient-net-use/","title":"Efficient Use of <code>net/http</code>, <code>net.Conn</code>, and UDP in High-Traffic Go Services","text":"<p>When we first start building high-traffic services in Go, we often lean heavily on <code>net/http</code>. It\u2019s stable, ergonomic, and remarkably capable for 80% of use cases. But as soon as traffic spikes or latency budgets shrink, the cracks begin to show.</p> <p>It\u2019s not that <code>net/http</code> is broken\u2014it\u2019s just that the defaults are tuned for convenience, not for performance under stress. And as we scale backend services to handle millions of requests per second, understanding what happens underneath the abstraction becomes the difference between meeting SLOs and fire-fighting in production.</p> <p>This article is a walkthrough of how to make networked Go services truly efficient\u2014what works, what breaks, and how to go beyond idiomatic usage. We\u2019ll start with <code>net/http</code>, drop into raw <code>net.Conn</code>, and finish with real-world patterns for handling UDP in latency-sensitive systems.</p>"},{"location":"02-networking/efficient-net-use/#the-hidden-complexity-behind-a-simple-http-call","title":"The Hidden Complexity Behind a Simple HTTP Call","text":"<p>Let\u2019s begin where most Go developers do: a simple <code>http.Client</code>.</p> <pre><code>client := &amp;http.Client{\n    Timeout: 5 * time.Second,\n}\n\nresp, err := client.Get(\"http://localhost:8080/data\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer resp.Body.Close()\n</code></pre> <p>This looks harmless. It gets the job done, and in most local tests, it performs reasonably well. But in production, at scale, this innocent-looking code can trigger a surprising range of issues: leaked connections, memory spikes, blocked goroutines, and mysterious latency cliffs.</p> <p>One of the most common issues is forgetting to fully read <code>resp.Body</code> before closing it. Go\u2019s HTTP client won\u2019t reuse connections unless the body is drained. And under load, that means you're constantly opening new TCP connections\u2014slamming the kernel with ephemeral ports, exhausting file descriptors, and triggering throttling.</p> <p>Here\u2019s the safe pattern:</p> <pre><code>io.Copy(io.Discard, resp.Body)\ndefer resp.Body.Close()\n</code></pre>"},{"location":"02-networking/efficient-net-use/#transport-tuning-when-defaults-arent-enough","title":"Transport Tuning: When Defaults Aren\u2019t Enough","text":"<p>It\u2019s easy to overlook how much global state hides behind <code>http.DefaultTransport</code>. If you spin up multiple <code>http.Client</code> instances across your app without customizing the transport, you're probably reusing a shared global pool without realizing it.</p> <p>This leads to unpredictable behavior under load: idle connections get evicted too quickly, or keep-alive connections linger longer than they should. The fix? Build a tuned <code>Transport</code> that matches your concurrency profile.</p>"},{"location":"02-networking/efficient-net-use/#custom-httptransport-fields-to-tune","title":"Custom <code>http.Transport</code> Fields to Tune","text":"<p>All the following settings are part of the <code>http.Transport</code> struct:</p> <pre><code>transport := &amp;http.Transport{\n    MaxIdleConns:          1000,\n    MaxConnsPerHost:       100,\n    IdleConnTimeout:       90 * time.Second,\n    ExpectContinueTimeout: 0,\n    DialContext: (&amp;net.Dialer{\n        Timeout:   5 * time.Second,\n        KeepAlive: 30 * time.Second,\n    }).DialContext,\n}\n\nclient := &amp;http.Client{\n    Transport: transport,\n    Timeout:   2 * time.Second,\n}\n</code></pre>"},{"location":"02-networking/efficient-net-use/#more-advanced-optimization-tricks","title":"More Advanced Optimization Tricks","text":"<p>These are all tied to key settings in the <code>http.Transport</code>, <code>http.Client</code>, and <code>http.Server</code> structs, or custom wrappers built on top of them:</p>"},{"location":"02-networking/efficient-net-use/#set-expectcontinuetimeout-carefully","title":"Set <code>ExpectContinueTimeout</code> Carefully","text":"<p>If our clients send large POST requests and the server doesn\u2019t support <code>100-continue</code> properly, we can reduce or eliminate this delay:</p> <pre><code>transport := &amp;http.Transport{\n    ...\n    ExpectContinueTimeout: 0, // if not needed, skip the wait entirely\n    ...\n}\n</code></pre>"},{"location":"02-networking/efficient-net-use/#constrain-maxconnsperhost","title":"Constrain <code>MaxConnsPerHost</code>","text":"<p>Go\u2019s default HTTP client will open an unbounded number of connections to a host. That\u2019s fine until one of your downstreams can\u2019t handle it.</p> <pre><code>transport := &amp;http.Transport{\n    ...\n    MaxConnsPerHost: 100,\n    ...\n}\n</code></pre> <p>This prevents stampedes during spikes and avoids exhausting resources on your backend services.</p>"},{"location":"02-networking/efficient-net-use/#use-small-httpclienttimeout","title":"Use Small <code>http.Client.Timeout</code>","text":"<p>A common mistake is setting a very high timeout (e.g., 30s) for safety. But long timeouts hold onto goroutines, buffers, and sockets under pressure. Prefer tighter control:</p> <pre><code>client := &amp;http.Client{\n    Timeout: 2 * time.Second,\n}\n</code></pre> <p>Instead of relying on big timeouts, use retries with backoff (e.g., with go-retryablehttp) to improve resiliency under partial failure.</p>"},{"location":"02-networking/efficient-net-use/#explicitly-set-readbuffersize-and-writebuffersize-in-httpserver","title":"Explicitly Set <code>ReadBufferSize</code> and <code>WriteBufferSize</code> in <code>http.Server</code>","text":"<p>Go's <code>http.Server</code> does not expose <code>ReadBufferSize</code> and <code>WriteBufferSize</code> directly, but when you need to reduce GC pressure and improve syscall efficiency under load, you can pre-size the buffers in custom <code>Conn</code> wrappers. 4KB\u20138KB is a balanced value for most workloads: it's large enough to handle small headers and bodies efficiently without wasting memory. For example, 4KB covers almost all typical HTTP headers and small JSON payloads.</p> <p>You can implement this using <code>bufio.NewReaderSize</code> and <code>NewWriterSize</code> in a wrapped connection that plugs into a custom <code>net.Listener</code> and <code>http.Server.ConnContext</code>.</p> <p>If you're using <code>fasthttp</code>, you can configure buffer sizes explicitly:</p> <pre><code>server := &amp;fasthttp.Server{\n    ReadBufferSize:  4096,\n    WriteBufferSize: 4096,\n    ...\n}\n</code></pre> <p>This avoids dynamic allocations on each request and leads to more predictable memory usage and cache locality under high throughput.</p>"},{"location":"02-networking/efficient-net-use/#use-bufioreaderpeek-for-efficient-framing","title":"Use <code>bufio.Reader.Peek()</code> for Efficient Framing","text":"<p>When implementing a framed protocol over TCP, like length-prefixed binary messages, naively calling <code>Read()</code> in a loop can lead to fragmented reads and unnecessary syscalls. This adds up, especially under load. Using <code>Peek()</code> gives you a look into the buffered data without advancing the read position, making it easier to detect message boundaries without triggering extra reads. It\u2019s a practical technique in streaming systems or multiplexed connections where tight control over framing is critical.</p> <pre><code>header, _ := reader.Peek(8) // Peek without advancing the buffer\n</code></pre>"},{"location":"02-networking/efficient-net-use/#force-fresh-dns-lookups-with-custom-dialers","title":"Force Fresh DNS Lookups with Custom Dialers","text":"<p>Go\u2019s built-in DNS caching lasts for the lifetime of the process. In dynamic environments, like Kubernetes, this can become a problem when service IPs change but clients keep reusing stale ones. To avoid this, you can force fresh DNS lookups by creating a new net.Dialer per request or rotating the HTTP client periodically.</p> <p>But you can bypass Go\u2019s internal DNS cache when needed:</p> <pre><code>DialContext: func(ctx context.Context, network, addr string) (net.Conn, error) {\n    return (&amp;net.Dialer{}).DialContext(ctx, network, addr)\n},\n</code></pre> <p>This ensures a fresh DNS lookup per request. While this adds minor overhead, it's necessary in failover-sensitive environments.</p>"},{"location":"02-networking/efficient-net-use/#use-syncpool-for-readerswriters","title":"Use <code>sync.Pool</code> for Readers/Writers","text":"<p>Most people use <code>sync.Pool</code> to reuse <code>[]byte</code> buffers, but for services that process many requests per second, allocating <code>bufio.Reader</code> and <code>bufio.Writer</code> objects per connection adds up. These objects also maintain their own buffers, so recycling them reduces pressure on both heap allocations and garbage collection.</p> <pre><code>var readerPool = sync.Pool{\n    New: func() interface{} {\n        return bufio.NewReaderSize(nil, 4096)\n    },\n}\n\nfunc getReader(conn net.Conn) *bufio.Reader {\n    r := readerPool.Get().(*bufio.Reader)\n    r.Reset(conn)\n    return r\n}\n</code></pre> <p>This practice significantly reduces allocation churn and improves latency consistency, especially in systems processing thousands of connections concurrently.</p>"},{"location":"02-networking/efficient-net-use/#dont-share-httpclient-across-multiple-hosts","title":"Don\u2019t Share <code>http.Client</code> Across Multiple Hosts","text":"<p>While it might seem efficient to reuse a single <code>http.Client</code>, each target host maintains its own internal connection pool within the underlying <code>http.Transport</code>. If you use the same client for multiple base URLs, you end up mixing connection reuse and causing head-of-line blocking across unrelated services. Worse, DNS caching and socket exhaustion become harder to track.</p> <p>Instead, create a dedicated <code>http.Client</code> for each upstream service you interact with. This improves connection reuse, avoids cross-talk between services, and usually makes behavior more predictable, especially in environments like service meshes or when dealing with multiple external APIs.</p>"},{"location":"02-networking/efficient-net-use/#use-conncontext-and-connstate-hooks-for-debugging","title":"Use <code>ConnContext</code> and <code>ConnState</code> Hooks for Debugging","text":"<p>These hooks are useful for tracking the lifecycle of each connection\u2014especially when debugging issues like memory leaks, stuck connections, or resource exhaustion in production. The <code>ConnState</code> callback gives visibility into transitions such as <code>StateNew</code>, <code>StateActive</code>, <code>StateIdle</code>, and <code>StateHijacked</code>, allowing you to log, trace, or apply custom handling per connection state.</p> <p>By monitoring these events, you can detect when connections hang, fail to close, or unexpectedly idle out. It also helps when correlating behavior with client IPs or network zones.</p> <pre><code>ConnState: func(conn net.Conn, state http.ConnState) {\n    log.Printf(\"conn %v changed state to %v\", conn.RemoteAddr(), state)\n},\n</code></pre>"},{"location":"02-networking/efficient-net-use/#dropping-the-abstraction-when-to-use-netconn","title":"Dropping the Abstraction: When to Use <code>net.Conn</code>","text":"<p>As we get closer to the limits of what the Go standard library can offer, it\u2019s worth knowing that there are high-performance alternatives built specifically for event-driven, low-latency workloads. Projects like <code>cloudwego/netpoll</code> and <code>tidwall/evio</code> offer powerful tools for maximizing performance beyond what\u2019s achievable with <code>net.Conn</code> alone.</p> <ul> <li> <p><code>cloudwego/netpoll</code> is an epoll-based network library designed for building massive concurrent network services with minimal GC overhead. It uses event-based I/O to eliminate goroutine-per-connection costs, ideal for scenarios like RPC proxies, internal service meshes, or high-frequency messaging systems.</p> </li> <li> <p><code>tidwall/evio</code> provides a fast, non-blocking event loop for Go based on the reactor pattern. It\u2019s well-suited for protocols where latency matters more than per-connection state complexity, such as custom TCP, UDP protocols, or lightweight gateways.</p> </li> </ul> <p>If you're building systems where throughput or connection count exceeds hundreds of thousands, or where tail latency is critical, it's worth exploring these libraries. They come with trade-offs\u2014most notably, less standardization and more manual lifecycle management\u2014but in return, they give you fine-grained control over performance-critical paths.</p> <p>Sometimes, even a tuned HTTP stack isn't enough.</p> <p>In cases like internal binary protocols or services dealing with hundreds of thousands of requests per second, we may find we're paying for HTTP semantics we don't use. Dropping to <code>net.Conn</code> gives us full control\u2014no pooling surprises, no hidden keep-alives, just a raw socket.</p> <pre><code>ln, err := net.Listen(\"tcp\", \":9000\")\n...\n</code></pre> <p>This lets us take over the connection lifecycle, buffering, and concurrency fully. It also opens up opportunities to reduce GC impact via buffer reuse:</p> <pre><code>var bufPool = sync.Pool{\n    New: func() interface{} {\n        return make([]byte, 4096)\n    },\n}\n</code></pre> <p>Enabling TCP_NODELAY is useful in latency-sensitive systems:</p> <pre><code>tcpConn := conn.(*net.TCPConn)\ntcpConn.SetNoDelay(true)\n</code></pre>"},{"location":"02-networking/efficient-net-use/#beyond-tcp-why-udp-matters","title":"Beyond TCP: Why UDP Matters","text":"<p>TCP could be too heavy for workloads like log firehose ingestion, telemetry beacons, or heartbeat messages. We can turn to UDP for low-latency, connectionless data delivery:</p> <pre><code>conn, _ := net.ListenUDP(\"udp\", &amp;net.UDPAddr{Port: 9999})\n...\n</code></pre> <p>This skips handshakes and reuses the socket efficiently. But remember\u2014UDP offers no ordering, reliability, or built-in session tracking. It works best in high-volume, low-consequence pipelines.</p>"},{"location":"02-networking/efficient-net-use/#choosing-the-right-tool","title":"Choosing the Right Tool","text":"<p>Our networking strategy should reflect traffic shape and protocol expectations:</p> Scenario Preferred Tool REST/gRPC, general APIs <code>net/http</code> HTTP under load Tuned <code>http.Transport</code> Custom TCP protocol <code>net.Conn</code> Framed binary data <code>net.Conn</code> + buffer mgmt Fire-and-forget telemetry <code>UDPConn</code> Latency-sensitive game updates <code>UDP</code> <p>At scale, network performance is never just about the network. It's also about memory pressure, context lifecycles, kernel behavior, and socket hygiene. We can go far with the Go standard library, but when systems push back, we need to push deeper.</p> <p>The good news? Go gives us the tools. We just need to use them wisely.</p> <p>If you're experimenting with framed protocols, zero-copy parsing, or custom benchmarking setups, there's a lot more to explore. Let's keep going.</p>"},{"location":"02-networking/gc-endpoint-profiling/","title":"Practical Example: Profiling Networked Go Applications with <code>pprof</code>","text":"<p>This section walks through a demo application instrumented with benchmarking tools and runtime profiling to ground profiling concepts in a real-world context. It covers identifying performance bottlenecks, interpreting flame graphs, and analyzing system behavior under various simulated network conditions.</p>"},{"location":"02-networking/gc-endpoint-profiling/#cpu-profiling-in-networked-apps","title":"CPU Profiling in Networked Apps","text":"<p>The demo application is intentionally designed to be as simple as possible to highlight key profiling concepts without unnecessary complexity. While the code and patterns used in the demo are basic, the profiling insights gained here are highly applicable to more complex, production-grade applications.</p> <p>To enable continuous profiling under load, we expose <code>pprof</code> via a dedicated HTTP endpoint:</p> <pre><code>import (\n\n    _ \"net/http/pprof\"\n\n)\n\n// ...\n\n    // Start pprof in a separate goroutine.\n    go func() {\n        log.Println(\"pprof listening on :6060\")\n        if err := http.ListenAndServe(\"localhost:6060\", nil); err != nil {\n            log.Fatalf(\"pprof server error: %v\", err)\n        }\n    }()\n</code></pre> full <code>net-app</code>'s source code <pre><code>package main\n\n// pprof-start\nimport (\n// pprof-end\n    \"flag\"\n    \"fmt\"\n    \"log\"\n    \"math/rand/v2\"\n    \"net/http\"\n// pprof-start\n    _ \"net/http/pprof\"\n// pprof-end\n    \"os\"\n    \"os/signal\"\n    \"time\"\n// pprof-start\n)\n// pprof-end\n\nvar (\n    fastDelay   = flag.Duration(\"fast-delay\", 0, \"Fixed delay for fast handler (if any)\")\n    slowMin     = flag.Duration(\"slow-min\", 1*time.Millisecond, \"Minimum delay for slow handler\")\n    slowMax     = flag.Duration(\"slow-max\", 300*time.Millisecond, \"Maximum delay for slow handler\")\n    gcMinAlloc  = flag.Int(\"gc-min-alloc\", 50, \"Minimum number of allocations in GC heavy handler\")\n    gcMaxAlloc  = flag.Int(\"gc-max-alloc\", 1000, \"Maximum number of allocations in GC heavy handler\")\n)\n\nfunc randRange(min, max int) int {\n    return rand.IntN(max-min) + min\n}\n\nfunc fastHandler(w http.ResponseWriter, r *http.Request) {\n    if *fastDelay &gt; 0 {\n        time.Sleep(*fastDelay)\n    }\n    fmt.Fprintln(w, \"fast response\")\n}\n\nfunc slowHandler(w http.ResponseWriter, r *http.Request) {\n    delayRange := int((*slowMax - *slowMin) / time.Millisecond)\n    delay := time.Duration(randRange(1, delayRange)) * time.Millisecond\n    time.Sleep(delay)\n    fmt.Fprintf(w, \"slow response with delay %d ms\\n\", delay)\n}\n\n// heavy-start\nvar longLivedData [][]byte\n\nfunc gcHeavyHandler(w http.ResponseWriter, r *http.Request) {\n    numAllocs := randRange(*gcMinAlloc, *gcMaxAlloc)\n    var data [][]byte\n    for i := 0; i &lt; numAllocs; i++ {\n        // Allocate 10KB slices. Occasionally retain a reference to simulate long-lived objects.\n        b := make([]byte, 1024*10)\n        data = append(data, b)\n        if i%100 == 0 { // every 100 allocations, keep the data alive\n            longLivedData = append(longLivedData, b)\n        }\n    }\n    fmt.Fprintf(w, \"allocated %d KB\\n\", len(data)*10)\n}\n// heavy-end\n\nfunc main() {\n    flag.Parse()\n\n    http.HandleFunc(\"/fast\", fastHandler)\n    http.HandleFunc(\"/slow\", slowHandler)\n    http.HandleFunc(\"/gc\", gcHeavyHandler)\n\n// pprof-start\n// ...\n\n    // Start pprof in a separate goroutine.\n    go func() {\n        log.Println(\"pprof listening on :6060\")\n        if err := http.ListenAndServe(\"localhost:6060\", nil); err != nil {\n            log.Fatalf(\"pprof server error: %v\", err)\n        }\n    }()\n// pprof-end\n\n    // Create a server to allow for graceful shutdown.\n    server := &amp;http.Server{Addr: \":8080\"}\n\n    go func() {\n        log.Println(\"HTTP server listening on :8080\")\n        if err := server.ListenAndServe(); err != nil &amp;&amp; err != http.ErrServerClosed {\n            log.Fatalf(\"HTTP server error: %v\", err)\n        }\n    }()\n\n    // Graceful shutdown on interrupt signal.\n    sigCh := make(chan os.Signal, 1)\n    signal.Notify(sigCh, os.Interrupt)\n    &lt;-sigCh\n    log.Println(\"Shutting down server...\")\n    if err := server.Shutdown(nil); err != nil {\n        log.Fatalf(\"Server Shutdown Failed:%+v\", err)\n    }\n    log.Println(\"Server exited\")\n}\n</code></pre> <p>The next step will be to establish a connection with the profiled app and collect samples:</p> <pre><code>go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30\n</code></pre> <p>View results interactively:</p> <pre><code>go tool pprof -http=:7070 cpu.prof # (1)\n</code></pre> <ol> <li>the actual <code>cpu.prof</code> path will be something like <code>$HOME/pprof/pprof.net-app.samples.cpu.004.pb.gz</code></li> </ol> <p>or you can save the profiling graph as an <code>svg</code> image.</p>"},{"location":"02-networking/gc-endpoint-profiling/#cpu-profiling-walkthrough-load-on-the-gc-endpoint","title":"CPU Profiling Walkthrough: Load on the <code>/gc</code> Endpoint","text":"<p>We profiled the application during a 30-second load test targeting the <code>/gc</code> endpoint to see what happens under memory pressure. This handler was intentionally designed to trigger allocations and force garbage collection, which makes it a great candidate for observing runtime behavior under stress.</p> <p>We used Go\u2019s built-in profiler to capture a CPU trace:</p> CPU profiling trace for the <code>/gc</code> endpoint <p></p> <p>This gave us 3.02 seconds of sampled CPU activity out of 30 seconds of wall-clock time\u2014a useful window into what the runtime and application were doing under pressure.</p>"},{"location":"02-networking/gc-endpoint-profiling/#where-the-time-went","title":"Where the Time Went","text":""},{"location":"02-networking/gc-endpoint-profiling/#http-stack-dominates-the-surface","title":"HTTP Stack Dominates the Surface","text":"<p>As expected, the majority of CPU time was spent on request handling:</p> <ul> <li><code>http.(*conn).serve</code> accounted for nearly 58% of sampled time</li> <li><code>http.serverHandler.ServeHTTP</code> appeared prominently as well</li> </ul> <p>This aligns with the fact that we were sustaining constant traffic. The Go HTTP stack is doing the bulk of the work, managing connections and dispatching requests.</p>"},{"location":"02-networking/gc-endpoint-profiling/#garbage-collection-overhead-is-clearly-visible","title":"Garbage Collection Overhead is Clearly Visible","text":"<p>A large portion of CPU time was spent inside the garbage collector:</p> <ul> <li><code>runtime.gcDrain</code>, <code>runtime.scanobject</code>, and <code>runtime.gcBgMarkWorker</code> were all active</li> <li>Combined with memory-related functions like <code>runtime.mallocgc</code>, these accounted for roughly 20% of total CPU time</li> </ul> <p>This confirms that <code>gcHeavyHandler</code> is achieving its goal. What we care about is whether this kind of allocation pressure leaks into real-world handlers. If it does, we\u2019re paying for it in latency and CPU churn.</p>"},{"location":"02-networking/gc-endpoint-profiling/#io-and-syscalls-take-a-big-slice","title":"I/O and Syscalls Take a Big Slice","text":"<p>We also saw high syscall activity\u2014especially from:</p> <ul> <li><code>syscall.syscall</code> (linked to <code>poll</code>, <code>Read</code>, and <code>Write</code>)</li> <li><code>bufio.Writer.Flush</code> and <code>http.response.finishRequest</code></li> </ul> <p>These functions reflect the cost of writing responses back to clients. For simple handlers, this is expected. But if your handler logic is lightweight and most of the time is spent just flushing data over TCP, it\u2019s worth asking whether the payloads or buffer strategies could be optimized.</p>"},{"location":"02-networking/gc-endpoint-profiling/#scheduler-activity-is-non-trivial","title":"Scheduler Activity Is Non-Trivial","text":"<p>Functions like <code>runtime.schedule</code>, <code>mcall</code>, and <code>findRunnable</code> were also on the board. These are Go runtime internals responsible for managing goroutines. Seeing them isn\u2019t unusual during high-concurrency tests\u2014but if they dominate, it often points to excessive goroutine churn or blocking behavior.</p>"},{"location":"02-networking/gc-endpoint-profiling/#memory-profiling-retained-heap-from-the-gc-endpoint","title":"Memory Profiling: Retained Heap from the <code>/gc</code> Endpoint","text":"<p>We also captured a memory profile to complement the CPU view while hammering the <code>/gc</code> endpoint. This profile used the <code>inuse_space</code> metric, which shows how much heap memory is actively retained by each function at the time of capture.</p> <p>We triggered the profile with:</p> <pre><code>go tool pprof -http=:7070 http://localhost:6060/debug/pprof/heap\n</code></pre> Memory profiling for the <code>/gc</code> endpoint <p></p> <p>At the time of capture, the application retained 649MB of heap memory, and almost all of it\u201499.46%\u2014was attributed to a single function: <code>gcHeavyHandler</code>. This was expected. The handler simulates allocation pressure by creating 10KB slices in a tight loop. Every 100th slice is added to a global variable to simulate long-lived memory.</p> <p>Here\u2019s what the handler does:</p> <pre><code>var longLivedData [][]byte\n\nfunc gcHeavyHandler(w http.ResponseWriter, r *http.Request) {\n    numAllocs := randRange(*gcMinAlloc, *gcMaxAlloc)\n    var data [][]byte\n    for i := 0; i &lt; numAllocs; i++ {\n        // Allocate 10KB slices. Occasionally retain a reference to simulate long-lived objects.\n        b := make([]byte, 1024*10)\n        data = append(data, b)\n        if i%100 == 0 { // every 100 allocations, keep the data alive\n            longLivedData = append(longLivedData, b)\n        }\n    }\n    fmt.Fprintf(w, \"allocated %d KB\\n\", len(data)*10)\n}\n</code></pre> <p>The flamegraph confirmed what we expected:</p> <ul> <li><code>gcHeavyHandler</code> accounted for nearly all memory in use.</li> <li>The path traced cleanly from the HTTP connection, through the Go router stack, into the handler logic.</li> <li>No significant allocations came from elsewhere\u2014this was a focused, controlled memory pressure scenario.</li> </ul> <p>This type of profile is valuable because it reveals what is still being held in memory, not just what was allocated. This view is often the most revealing for diagnosing leaks, retained buffers, or forgotten references.</p>"},{"location":"02-networking/gc-endpoint-profiling/#summary-cpu-and-memory-profiling-of-the-gc-endpoint","title":"Summary: CPU and Memory Profiling of the <code>/gc</code> Endpoint","text":"<p>The <code>/gc</code> endpoint was intentionally built to simulate high allocation pressure and GC activity. Profiling this handler under load gave us a clean, focused view of how the Go runtime behaves when pushed to its memory limits.</p> <p>From the CPU profile, we saw that:</p> <ul> <li>As expected, most of the time was spent in the HTTP handler path during sustained load.</li> <li>Nearly 20% of CPU samples were attributed to memory allocation and garbage collection.</li> <li>Syscall activity was high, mostly from writing responses.</li> <li>The Go scheduler was moderately active, managing the concurrent goroutines handling traffic.</li> </ul> <p>From the memory profile, we captured 649MB of live heap usage, with 99.46% of it retained by <code>gcHeavyHandler</code>. This matched our expectations: the handler deliberately retains every 100th 10KB allocation to simulate long-lived data.</p> <p>Together, these profiles give us confidence that the <code>/gc</code> endpoint behaves as intended under synthetic pressure:</p> <ul> <li>It creates meaningful CPU and memory load.</li> <li>It exposes the cost of sustained allocations and GC cycles.</li> <li>It provides a predictable environment for testing optimizations or GC tuning strategies.</li> </ul>"},{"location":"02-networking/long-lived-connections/","title":"Memory Management and Leak Prevention in Long-Lived Connections","text":"<p>Long-lived connections\u2014such as WebSockets or TCP streams\u2014are critical for real-time systems but also prone to gradual degradation. When connections persist, any failure to clean up buffers, goroutines, or timeouts can quietly consume memory over time. These leaks often evade unit tests or staging environments but surface under sustained load in production.</p> <p>This article focuses on memory management strategies tailored to long-lived connections in Go. It outlines patterns that cause leaks, techniques for enforcing resource bounds, and tools to identify hidden retention through profiling.</p>"},{"location":"02-networking/long-lived-connections/#identifying-common-leak-patterns","title":"Identifying Common Leak Patterns","text":"<p>In garbage-collected languages like Go, memory leaks typically involve lingering references\u2014objects that are no longer needed but remain reachable. The most common culprits in connection-heavy services include goroutines that don\u2019t exit, buffered channels that accumulate data, and slices that retain large backing arrays.</p>"},{"location":"02-networking/long-lived-connections/#goroutine-leaks","title":"Goroutine Leaks","text":"<p>Handlers for persistent connections often run in their own goroutines. If the control flow within a handler blocks indefinitely\u2014whether due to I/O operations, nested goroutines, or external dependencies\u2014those goroutines can remain active even after the connection is no longer useful.</p> <pre><code>func handleWS(conn *websocket.Conn) {\n    for {\n        _, message, err := conn.ReadMessage()\n        if err != nil {\n            break\n        }\n        process(message)\n    }\n}\n\nhttp.HandleFunc(\"/ws\", func(w http.ResponseWriter, r *http.Request) {\n    ws, err := upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        return\n    }\n    go handleWS(ws)\n})\n</code></pre> <p>Here, if <code>process(message)</code> internally spawns goroutines without proper cancellation, or if <code>conn.ReadMessage()</code> blocks indefinitely after a network interruption, the handler goroutine can hang forever, retaining references to stacks and heap objects. Blocking reads prevent the loop from exiting, and unbounded goroutine spawning within <code>process</code> can accumulate if upstream errors aren\u2019t handled. Now multiply by 10,000 connections.</p>"},{"location":"02-networking/long-lived-connections/#buffer-and-channel-accumulation","title":"Buffer and Channel Accumulation","text":"<p>Buffered channels and pooled buffers offer performance advantages, but misuse can lead to retained memory that outlives its usefulness. A typical example involves <code>sync.Pool</code> combined with I/O:</p> <pre><code>var bufferPool = sync.Pool{\n    New: func() interface{} { return make([]byte, 4096) },\n}\n\nfunc handle(conn net.Conn) {\n    buf := bufferPool.Get().([]byte)\n    defer bufferPool.Put(buf)\n\n    for {\n        n, err := conn.Read(buf)\n        if err != nil {\n            return\n        }\n\n        data := make([]byte, n)\n        copy(data, buf[:n])\n        go process(data)\n    }\n}\n</code></pre> <p>This version correctly isolates the active portion of the buffer using a copy. Problems arise when the copy is skipped:</p> <pre><code>data := buf[:n]\ngo process(data)\n</code></pre> <p>Although <code>data</code> appears small, it still points to the original 4 KB buffer. If <code>process</code> stores that slice in a log queue, cache, or channel, the entire backing array remains in memory. Over time, this pattern can hold onto hundreds of megabytes of heap space across thousands of connections.</p> <p>To prevent this, always create a new slice with just the required data length before handing it off to any code that might retain it. Copying a slice may seem inefficient, but it ensures the larger buffer is no longer indirectly referenced.</p>"},{"location":"02-networking/long-lived-connections/#enforcing-readwrite-deadlines","title":"Enforcing Read/Write Deadlines","text":"<p>Network I/O without deadlines introduces an unbounded wait time. If a client stalls or a network failure interrupts a connection, read and write operations may block indefinitely. In a high-connection environment, even a few such blocked goroutines can accumulate and exhaust memory over time.</p> <p>Deadlines solve this by imposing a strict upper bound on how long any read or write can take. Once the deadline passes, the operation returns with a timeout error, allowing the connection handler to proceed with cleanup.</p>"},{"location":"02-networking/long-lived-connections/#setting-deadlines","title":"Setting Deadlines","text":"<pre><code>const timeout = 30 * time.Second\n\nfunc handle(conn net.Conn) {\n    defer conn.Close()\n\n    buffer := make([]byte, 4096) // 4 KB buffer; size depends on protocol and usage\n\n    for {\n        conn.SetReadDeadline(time.Now().Add(timeout))\n        n, err := conn.Read(buffer)\n        if err != nil {\n            if netErr, ok := err.(net.Error); ok &amp;&amp; netErr.Timeout() {\n                return\n            }\n            return\n        }\n\n        conn.SetWriteDeadline(time.Now().Add(timeout))\n        _, err = conn.Write(buffer[:n])\n        if err != nil {\n            return\n        }\n    }\n}\n</code></pre> <p>This approach ensures that each read and write completes\u2014or fails\u2014within a known time window. It prevents handlers from hanging due to slow or unresponsive peers and contributes directly to keeping goroutine count and memory usage stable under load.</p>"},{"location":"02-networking/long-lived-connections/#context-based-cancellation","title":"Context-Based Cancellation","text":"<p>For more coordinated shutdowns, contexts provide a way to propagate cancellation signals across multiple goroutines and resources:</p> <pre><code>func handle(ctx context.Context, conn net.Conn) {\n    defer conn.Close()\n\n    ctx, cancel := context.WithTimeout(ctx, 30*time.Second)\n    defer cancel()\n\n    done := make(chan struct{})\n    go func() {\n        select {\n        case &lt;-ctx.Done():\n            conn.Close()\n        case &lt;-done:\n        }\n    }()\n\n    // perform read/write as before\n    close(done)\n}\n</code></pre> <p>With this pattern, the handler exits cleanly even if the read or write blocks. Closing the connection from a context cancellation path ensures dependent routines terminate in a timely manner.</p>"},{"location":"02-networking/long-lived-connections/#managing-backpressure","title":"Managing Backpressure","text":"<p>When input arrives faster than it can be processed or sent downstream, backpressure is necessary to avoid unbounded memory growth. Systems that ingest data without applying pressure controls can suffer from memory spikes, GC churn, or latency cliffs under load.</p>"},{"location":"02-networking/long-lived-connections/#rate-limiting-and-queuing","title":"Rate Limiting and Queuing","text":"<p>Rate limiters constrain processing throughput to match downstream capacity. Token-bucket implementations are common and provide burst-friendly rate control </p> <pre><code>type RateLimiter struct {\n    tokens chan struct{}\n}\n\nfunc NewRateLimiter(rate int) *RateLimiter {\n    rl := &amp;RateLimiter{tokens: make(chan struct{}, rate)}\n    for i := 0; i &lt; rate; i++ {\n        rl.tokens &lt;- struct{}{}\n    }\n    go func() {\n        ticker := time.NewTicker(time.Second)\n        for range ticker.C {\n            select {\n            case rl.tokens &lt;- struct{}{}:\n            default:\n            }\n        }\n    }()\n    return rl\n}\n\nfunc (rl *RateLimiter) Acquire() {\n    &lt;-rl.tokens\n}\n</code></pre> <p>This limiter can be used per connection or across the system:</p> <pre><code>rl := NewRateLimiter(100)\nfor {\n    rl.Acquire()\n    // read/process/send\n}\n</code></pre> <p>By limiting processing rates, the system avoids overwhelming internal queues or consumers. When capacity is reached, the limiter naturally applies backpressure by blocking.</p>"},{"location":"02-networking/long-lived-connections/#flow-control-via-tcp","title":"Flow Control via TCP","text":"<p>For TCP streams, it\u2019s often better to leverage kernel-level flow control rather than building large user-space buffers. This is especially important when sending data to slow or unpredictable clients.</p> <pre><code>type framedConn struct {\n    net.Conn\n    mw *bufio.Writer\n}\n\nfunc (f *framedConn) WriteFrame(data []byte) error {\n    if err := binary.Write(f.mw, binary.BigEndian, uint32(len(data))); err != nil {\n        return err\n    }\n    if _, err := f.mw.Write(data); err != nil {\n        return err\n    }\n    return f.mw.Flush()\n}\n</code></pre> <p>By flushing early and using small buffers, the application shifts pressure back to the TCP stack. If the peer can\u2019t keep up, send calls will block instead of buffering excessive data in memory.</p> <p>Warning</p> <p>While relying on TCP\u2019s built-in flow control simplifies the memory model and offloads queuing to the kernel, this approach comes with tradeoffs. Flushing small buffers aggressively forces the application to send many small TCP segments. This increases system call frequency, consumes more CPU per byte, and fragments data on the wire. In high-throughput systems or over high-latency links, this can underutilize the TCP congestion window and cap throughput well below the link\u2019s capacity.</p> <p>Another risk lies in how TCP flow control applies uniformly to the socket, without application-level context. A slow-reading client can cause Write calls to block indefinitely, holding up goroutines and potentially stalling outbound flows. In fan-out scenarios\u2014like broadcasting to many WebSocket clients\u2014this means one slow recipient can back up others unless additional timeout or buffering logic is applied. While TCP ensures fairness and correctness, it doesn\u2019t help with per-client pacing, prioritization, or partial delivery strategies that might be required in real-world systems.</p> <p>For low-concurrency or control-plane connections, TCP backpressure alone might be sufficient. But at scale, especially when dealing with mixed client speeds, it\u2019s often necessary to combine TCP-level backpressure with bounded user-space queues, write timeouts, or selective message drops to keep latency predictable and memory usage under control.</p> <p>Persistent connections introduce long-lived resources that must be managed explicitly. Without deadlines, bounded queues, and cleanup coordination, even well-intentioned code can gradually degrade. Apply these patterns consistently and validate them under load with memory profiles.</p> <p>Memory leaks are easier to prevent than to detect. Defensive design around goroutines, buffers, and timeouts ensures services remain stable even under sustained connection load.</p>"},{"location":"02-networking/low-level-optimizations/","title":"Low-Level Network Optimizations: Socket Options That Matter","text":"<p>Socket settings can limit both throughput and latency when the system is under load. The defaults are designed for safety and compatibility, not for any particular workload. In practice they often become the bottleneck before CPU or memory do. Go lets you reach the underlying file descriptors through <code>syscall</code>, so you can change key socket options without giving up its concurrency model or the standard library.</p>"},{"location":"02-networking/low-level-optimizations/#disabling-nagles-algorithm-tcp_nodelay","title":"Disabling Nagle\u2019s Algorithm: <code>TCP_NODELAY</code>","text":"<p>Nagle\u2019s algorithm exists to make TCP more efficient. Every tiny packet you send carries headers that add up to a lot of wasted bandwidth if left unchecked. Nagle fixes that by holding back small writes until it can batch them into a full segment, cutting down on overhead and network chatter. That trade-off \u2014 bandwidth at the expense of latency \u2014 is usually fine, which is why it\u2019s on by default. But if your application sends lots of small, time-critical messages, like a game server or a trading system, waiting even a few milliseconds for the buffer to fill can hurt.</p> <pre><code>sequenceDiagram\n    participant App as Application\n    participant TCP as TCP Stack\n    participant Net as Network\n\n    App-&gt;&gt;TCP: Send 1 byte\n    note right of TCP: Buffering (no ACK received)\n    App-&gt;&gt;TCP: Send 1 byte\n    note right of TCP: Still buffering...\n\n    TCP--&gt;&gt;Net: Send 2 bytes (batched)\n    Net--&gt;&gt;TCP: ACK received\n\n    App-&gt;&gt;TCP: Send 1 byte\n    TCP--&gt;&gt;Net: Immediately send (ACK received, buffer clear)</code></pre> <p>Nagle\u2019s algorithm trades latency for efficiency by holding back small packets until there\u2019s more data to send or an acknowledgment comes back. That delay is fine for bulk transfers but a problem for anything that needs fast, small messages. Setting <code>TCP_NODELAY</code> turns it off so data goes out immediately. This is critical for workloads like gaming, trading, real-time video, and other interactive systems where you can\u2019t afford to wait.</p> <p>In Go, you can turn off Nagle\u2019s algorithm with <code>TCP_NODELAY</code>:</p> <pre><code>func SetTCPNoDelay(conn *net.TCPConn) error {\n    return conn.SetNoDelay(true)\n}\n</code></pre>"},{"location":"02-networking/low-level-optimizations/#so_reuseport-for-scalability","title":"SO_REUSEPORT for Scalability","text":"<p><code>SO_REUSEPORT</code> lets multiple sockets on the same machine bind to the same port and accept connections at the same time. Instead of funneling all incoming connections through one socket, the kernel distributes new connections across all of them, so each socket gets its own share of the load. This is useful when running several worker processes or threads that each accept connections independently, because it removes the need for user-space coordination and avoids contention on a single accept queue. It also makes better use of multiple CPU cores by letting each process or thread handle its own queue of connections directly.</p> <p>Typical scenarios for <code>SO_REUSEPORT</code>:</p> <ul> <li>High-performance web servers where multiple worker processes call bind() on the same port. The kernel distributes incoming connection requests across the accept() queues of all bound sockets, typically using a hash of the 4-tuple or round-robin, eliminating the need for a user-space dispatcher.</li> <li>Multi-threaded or multi-process servers that accept connections in parallel. When combined with Go\u2019s GOMAXPROCS, each thread or process can call accept() independently on its own file descriptor, avoiding lock contention on a single queue and fully utilizing all CPU cores.</li> <li>Fault-tolerant designs where multiple processes bind to the same port to increase resilience. If one process exits or is killed, the others continue to service connections without interruption, because each maintains its own independent accept() queue.</li> </ul> <p>In Go, SO_REUSEPORT isn\u2019t exposed in the standard library, but it can be set via syscall when creating the socket. This is done with <code>syscall.SetsockoptInt</code>, which operates on the socket\u2019s file descriptor. You pass the protocol level (<code>SOL_SOCKET</code>), the option (<code>SO_REUSEPORT</code>), and the value (<code>1</code> to enable). This must happen before calling <code>bind()</code>, so it\u2019s typically placed in the Control callback of a <code>net.ListenConfig</code>, which runs before the socket is bound.</p> <pre><code>listenerConfig := &amp;net.ListenConfig{\n    Control: func(network, address string, c syscall.RawConn) error {\n        return c.Control(func(fd uintptr) {\n            syscall.SetsockoptInt(int(fd), syscall.SOL_SOCKET, syscall.SO_REUSEPORT, 1)\n        })\n    },\n}\nlistener, err := listenerConfig.Listen(context.Background(), \"tcp\", \":8080\")\n</code></pre>"},{"location":"02-networking/low-level-optimizations/#tuning-socket-buffer-sizes-so_rcvbuf-and-so_sndbuf","title":"Tuning Socket Buffer Sizes: <code>SO_RCVBUF</code> and <code>SO_SNDBUF</code>","text":"<p>Socket buffer sizes \u2014 <code>SO_RCVBUF</code> for receiving and <code>SO_SNDBUF</code> for sending \u2014 directly affect throughput and the number of system calls. These buffers hold incoming and outgoing data in the kernel, smoothing out bursts and letting the application read and write at its own pace.</p> <p>When buffers are too small, they fill up quickly and the kernel keeps waking the application to read or write data. That extra churn increases CPU usage and limits how much data you can push through the connection. If the buffers are too large, they just waste memory and let packets pile up in the queue longer than needed, which adds latency and hurts responsiveness when the system is busy. The point is to make the buffers big enough to keep the link busy but not so big that they turn into a backlog.</p> <p>This is how you can adjust the buffer:</p> <pre><code>func SetSocketBuffers(conn *net.TCPConn, recvBuf, sendBuf int) error {\n    if err := conn.SetReadBuffer(recvBuf); err != nil {\n        return err\n    }\n    return conn.SetWriteBuffer(sendBuf)\n}\n</code></pre> <p>Typical default sizes for <code>SO_RCVBUF</code> and <code>SO_SNDBUF</code> on Linux systems range from 128 KB to 256 KB, depending on the kernel version and system configuration. These defaults are chosen to provide a balance between minimizing latency and avoiding excessive memory usage. For high-bandwidth applications or connections with high latency (e.g., long-haul TCP connections), increasing these buffer sizes\u2014sometimes to several megabytes\u2014can significantly improve throughput by allowing more in-flight data without blocking.</p> <p>The right buffer size depends on the RTT, the link bandwidth, and the size of the messages your application sends. A common rule of thumb is to match the buffer to the bandwidth\u2013delay product (BDP), which is just <code>bandwidth \u00d7 RTT</code>. That way, the connection can keep the pipe full without stalling.</p> <p>To figure out the right buffer sizes, you need to test under realistic load. Tools like iperf3 are good for measuring raw throughput, and app-specific profiling (pprof, netstat, custom metrics) helps spot where things actually get stuck. Increase the buffer sizes step by step during load tests and watch where the gains level off \u2014 that\u2019s usually a good place to stop.</p> <p>Warning</p> <p>The optimal settings depend on how your system actually runs, so you need to measure them under load. Guessing or copying values from elsewhere usually doesn\u2019t work \u2014 you have to test and adjust until it performs the way you need.</p>"},{"location":"02-networking/low-level-optimizations/#tcp-keepalives-for-reliability","title":"TCP Keepalives for Reliability","text":"<p>TCP keepalive probes detect dead peer connections, freeing resources promptly. Keepalives prevent hanging connections in long-lived services.</p> <p>Default keepalive settings on most platforms are conservative\u2014idle time of 2 hours, 10 probes at 75-second intervals\u2014intended for general-purpose environments. For server-side applications requiring faster failure detection (e.g., reverse proxies, microservices over unreliable links), significantly more aggressive settings are common: 30 seconds idle, 3 probes at 10-second intervals.</p> <p>However, aggressive tuning increases traffic and risks false positives on congested links. A recommended approach is to balance early detection and network conditions. Typical tuned values\u2014idle 30\u201360s, interval 10\u201315s, probes 3\u20135\u2014are not specified by any RFC or standard, but come from operational practice and vendor guidance. For example, PostgreSQL, Kubernetes, and AWS all recommend values in this range to align with cloud load balancer timeouts and service SLAs. These numbers are derived from empirical experience to minimize both false positives and long detection delays, as discussed in sources like PostgreSQL documentation, AWS networking blogs, and Kubernetes issue discussions.</p> <p>These can be adjusted system-wide using <code>sysctl</code> or per-connection on some platforms (e.g., Linux) via platform-specific socket options or libraries that expose <code>TCP_KEEPIDLE</code>, <code>TCP_KEEPINTVL</code>, and <code>TCP_KEEPCNT</code>. In Go, <code>SetKeepAlivePeriod</code> only controls the idle time; deeper tuning may require cgo or raw syscalls.</p> <p>In Go, enabling and tuning keepalives:</p> <pre><code>func SetKeepAlive(conn *net.TCPConn, idleTime time.Duration) error {\n    if err := conn.SetKeepAlive(true); err != nil {\n        return err\n    }\n    return conn.SetKeepAlivePeriod(idleTime)\n}\n</code></pre> <p>The <code>SetKeepAlivePeriod</code> method in Go controls only the idle time before the first keepalive probe is sent. On Linux, this corresponds to the <code>TCP_KEEPIDLE</code> parameter; on macOS and BSD, it maps to <code>TCP_KEEPALIVE</code>. However, it does not affect the interval between subsequent probes (<code>TCP_KEEPINTVL</code>) or the number of allowed failed probes (<code>TCP_KEEPCNT</code>). These two must be set separately using raw syscalls or cgo bindings if finer-grained control is required. Without tuning <code>KEEPINTVL</code> and <code>KEEPCNT</code>, a dead connection may take several minutes to detect, even if <code>SetKeepAlivePeriod</code> is low.</p> <p>Warning</p> <p>Do not change the default values without testing them properly. The right settings depend entirely on your workload, the network path, and how the application behaves under real conditions. Different configurations can produce very different results, and what works elsewhere might hurt here. You\u2019ll find plenty of blog posts and advice with suggested numbers, but those are just starting points. The only reliable way to figure out what works in your environment is to test and measure it yourself.</p>"},{"location":"02-networking/low-level-optimizations/#connection-backlog-somaxconn","title":"Connection Backlog: SOMAXCONN","text":"<p>The connection backlog (<code>SOMAXCONN</code>) defines how many pending connections can queue up for acceptance. When the backlog queue fills up, additional connection attempts are refused by the kernel (usually resulting in <code>ECONNREFUSED</code>(1) or dropped SYN packets) until space becomes available, which can cause clients to see connection errors under heavy load.</p> <ol> <li>A <code>connect()</code> on a stream socket found no one listening on the remote address. See Linux man pages for more details.</li> </ol> <p>The default value of <code>SOMAXCONN</code> on Linux is typically 128 or 4096, depending on the kernel version and distribution. These defaults are chosen to strike a balance between memory use and handling normal connection rates, but they may be too low for high-traffic servers or services experiencing connection bursts.</p> <p>A bigger backlog helps when the server gets hit with a burst of connections \u2014 like after a failover or during peak \u2014 so clients don\u2019t get dropped right away. Raising <code>SOMAXCONN</code> lets the kernel queue more connections while the app catches up and keeps the service reachable under load.</p> <p>Changing <code>SOMAXCONN</code> is a system-level setting. On Linux you usually adjust it through kernel parameters.</p> <pre><code># System-level setting\nsysctl -w net.core.somaxconn=4096\n</code></pre>"},{"location":"02-networking/low-level-optimizations/#safely-wrapping-syscalls-in-go","title":"Safely Wrapping Syscalls in Go","text":"<p>Working with socket options through syscalls means dealing directly with file descriptors. These calls need to happen before the socket is bound or used, which makes the timing important and easy to get wrong. If you set an option too late, the kernel ignores it, or worse, you get hard-to-reproduce bugs. Since you\u2019re bypassing the Go runtime, you\u2019re also responsible for checking errors and making sure the file descriptor stays in a valid state. <code>syscall.RawConn</code> exists to help with this \u2014 it gives you a controlled hook to run your code against the socket at exactly the right point during setup.</p> <pre><code>listenerConfig := net.ListenConfig{\n    Control: func(network, address string, c syscall.RawConn) error {\n        return c.Control(func(fd uintptr) {\n            // Custom syscall here; safely handled means it executes within the correct context,\n            // before the socket is handed over to the Go runtime, with proper error checking and synchronization\n        })\n    },\n}\n</code></pre> <p>The socket options are set during creation, at the right time, and in one place. That keeps them from being forgotten or misapplied later.</p>"},{"location":"02-networking/low-level-optimizations/#real-world-considerations","title":"Real-World Considerations","text":"<p>Tuning sockets is always a trade-off \u2014 lower latency, higher throughput, better reliability, and reasonable resource use don\u2019t usually come together for free. You need to understand your workload, change one thing at a time, and test it under real load. Without monitoring and instrumentation, you\u2019re just guessing, so measure everything and make sure the changes actually help.</p>"},{"location":"02-networking/networking-internals/","title":"Go Networking Internals","text":"<p>Go\u2019s networking model is deceptively simple on the surface\u2014spawn a goroutine, accept a connection, read from it, and write a response. But behind this apparent ease is a highly optimized and finely tuned runtime that handles tens or hundreds of thousands of connections with minimal OS overhead. In this deep dive, we\u2019ll walk through the mechanisms that make this possible: from goroutines and the scheduler to how Go interacts with OS-level pollers like <code>epoll</code>, <code>kqueue</code>, and IOCP.</p>"},{"location":"02-networking/networking-internals/#goroutines-and-the-runtime-scheduler","title":"Goroutines and the Runtime Scheduler","text":"<p>Goroutines are lightweight user-space threads managed by the Go runtime. They\u2019re cheap to create (a few kilobytes of stack) and can scale to millions. But they\u2019re not magic\u2014they rely on the runtime scheduler to multiplex execution across a limited number of OS threads.</p> <p>Go\u2019s scheduler is based on an M:N model:</p> <ul> <li>M (Machine): Represents an OS thread.</li> <li>G (Goroutine): Represents the actual task or coroutine.</li> <li>P (Processor): Represents the context for scheduling (holding run queues, caches).</li> </ul> <p>Each P can execute one G at a time using an M. There are as many Ps as GOMAXPROCS. If a goroutine blocks on I/O, another runnable G may park and reuse the thread.</p> <pre><code>stateDiagram-v2\n    [*] --&gt; New : goroutine declared\n    New --&gt; Runnable : go func() invoked\n    Runnable --&gt; Running : scheduled on an available P\n    Running --&gt; Waiting : blocking syscall, channel op, etc.\n    Waiting --&gt; Runnable : event ready, rescheduled\n    Running --&gt; Terminated : function exits or panics\n    Waiting --&gt; Terminated : canceled or panicked\n    Terminated --&gt; [*]\n\n    state \"Go Scheduler\\n(GOMAXPROCS = N)\" as Scheduler {\n        [*] --&gt; P1\n        [*] --&gt; P2\n        ...\n        [*] --&gt; PN\n\n        P1 --&gt; ScheduleGoroutine1 : pick from global/runq\n        P2 --&gt; ScheduleGoroutine2\n        PN --&gt; ScheduleGoroutineN\n    }\n\n    note right of Runnable\n        Ps (Processors) pick Runnable goroutines\n        based on availability up to GOMAXPROCS\n    end note\n\n    note right of Scheduler\n        GOMAXPROCS determines how many Ps\n        can execute goroutines in parallel.\n    end note</code></pre>"},{"location":"02-networking/networking-internals/#blocking-io-in-goroutines-what-really-happens","title":"Blocking I/O in Goroutines: What Really Happens?","text":"<p>Suppose a goroutine calls <code>conn.Read()</code>. This looks blocking\u2014but only from the goroutine's perspective. Internally, Go\u2019s runtime intercepts the call and uses a mechanism known as the netpoller.</p> <p>On Unix-based systems, Go uses readiness-based polling (<code>epoll</code> on Linux, <code>kqueue</code> on macOS/BSD). When a goroutine performs a syscall like <code>read(fd)</code>, the runtime checks whether the file descriptor is ready. If not:</p> <ol> <li>The goroutine is parked.</li> <li>The file descriptor is registered with the poller.</li> <li>The OS thread is released to run other work.</li> <li>When the fd becomes ready, the poller wakes up, and the runtime marks the goroutine as runnable.</li> </ol> <pre><code>flowchart TD\n    A[\"Goroutine: conn.Read()\"] --&gt; B[netpoller checks FD]\n    B --&gt; C{FD ready?}\n    C -- No --&gt; D[Park goroutine]\n    D --&gt; E[FD registered with epoll]\n    E --&gt; F[epoll_wait blocks]\n    F --&gt; G[FD ready]\n    G --&gt; H[Wake goroutine]\n    H --&gt; I[Re-schedule]\n    C -- Yes --&gt; H</code></pre> <p>This system enables Go to serve a massive number of clients concurrently, using a small number of threads, avoiding the overhead of traditional thread-per-connection models.</p>"},{"location":"02-networking/networking-internals/#internals-of-the-net-package","title":"Internals of the <code>net</code> Package","text":"<p>Let\u2019s take a look at what happens behind <code>net.Listen(\"tcp\", \":8080\")</code> and <code>conn.Read()</code>.</p> <ul> <li><code>net.Listen</code> calls into <code>net.ListenTCP</code>, which constructs a <code>netFD</code> struct wrapping the socket.</li> <li>The socket is marked non-blocking via <code>syscall.SetNonblock(fd, true)</code>.</li> <li><code>Accept</code> and <code>Read</code> methods on <code>netFD</code> are layered on top of syscalls, but routed through internal pollers and wrapped with logic to yield and resume goroutines.</li> </ul> <p>Here\u2019s a rough diagram of the call chain:</p> <pre><code>flowchart TD\n    A[net.Listen] --&gt; B[ListenTCP] --&gt; C[listenFD]\n    C --&gt; D[\"pollDesc (register with netpoll)\"]\n    D --&gt; E[runtime-integrated non-blocking syscall wrappers]</code></pre> <p>This architecture makes the blocking calls from the developer\u2019s perspective translate into non-blocking interactions with the kernel.</p>"},{"location":"02-networking/networking-internals/#the-netpoller-polling-with-epollkqueueiocp","title":"The Netpoller: Polling with Epoll/Kqueue/IOCP","text":"<p>The netpoller is a runtime subsystem that integrates low-level polling mechanisms with Go\u2019s scheduling system. Each fd has an associated <code>pollDesc</code>, which helps coordinate goroutine suspension and resumption.</p> <p>The poller operates in a dedicated thread (or threads) that loop over OS wait primitives:</p> <ul> <li>epoll_wait (Linux)</li> <li>kqueue (macOS/BSD)</li> <li>IOCP (Windows)</li> </ul> <p>When an I/O event fires, the poller finds the associated <code>pollDesc</code>, identifies the parked goroutine, and puts it back into the run queue.</p> <p>In the Go source, relevant files include:</p> <ul> <li>runtime/netpoll_epoll.go</li> <li>runtime/netpoll_kqueue.go</li> <li>runtime/netpoll_windows.go</li> </ul> <p>The Go poller is readiness-based (not completion-based, except for Windows IOCP). It handles:</p> <ul> <li>fd registration</li> <li>waking goroutines on readiness</li> <li>integration with the run queue (P-local or global)</li> </ul>"},{"location":"02-networking/networking-internals/#example-high-performance-tcp-echo-server","title":"Example: High-Performance TCP Echo Server","text":"<p>Let's break down a simple Go TCP echo server and map each part to Go\u2019s internal networking and scheduling mechanisms \u2014 including <code>netFD</code>, <code>poll.FD</code>, and goroutines.</p> Simple Echo server source code <pre><code>package main\n\nimport (\n    \"bufio\"\n    \"fmt\"\n    \"net\"\n    \"time\"\n)\n\nfunc main() {\n    // Start listening on TCP port 9000\n    listener, err := net.Listen(\"tcp\", \":9000\")\n    if err != nil {\n        panic(err) // Exit if the port can't be bound\n    }\n    fmt.Println(\"Echo server listening on :9000\")\n\n    // Accept incoming connections in a loop\n    for {\n        conn, err := listener.Accept() // Accept new client connection\n        if err != nil {\n            fmt.Printf(\"Accept error: %v\\n\", err)\n            continue // Skip this iteration on error\n        }\n\n        // Handle the connection in a new goroutine for concurrency\n        go handle(conn)\n    }\n}\n\n// handle echoes data back to the client line-by-line\nfunc handle(conn net.Conn) {\n    defer conn.Close() // Ensure connection is closed on exit\n\n    reader := bufio.NewReader(conn) // Wrap connection with buffered reader\n\n    for {\n        // Set a read deadline to avoid hanging goroutines if client disappears\n        conn.SetReadDeadline(time.Now().Add(5 * 60 * time.Second)) // 5 minutes timeout\n\n        // Read input until newline character\n        line, err := reader.ReadString('\\n')\n        if err != nil {\n            fmt.Printf(\"Connection closed: %v\\n\", err)\n            return // Exit on read error (e.g. client disconnect)\n        }\n\n        // Echo the received line back to the client\n        _, err = conn.Write([]byte(line))\n        if err != nil {\n            fmt.Printf(\"Write error: %v\\n\", err)\n            return // Exit on write error\n        }\n    }\n}\n</code></pre>"},{"location":"02-networking/networking-internals/#imports-and-setup","title":"Imports and Setup","text":"<pre><code>import (\n    \"bufio\"\n    \"fmt\"\n    \"net\"\n    \"time\"\n    \"sync/atomic\"\n)\n</code></pre> <p>Internals Involved:</p> <ul> <li>The <code>net</code> package abstracts system-level networking.</li> <li>Under the hood:<ul> <li>Uses <code>netFD</code> (internal, private struct)</li> <li>Wraps <code>poll.FD</code> for non-blocking I/O</li> <li>Uses OS features like <code>epoll</code>, <code>kqueue</code>, or <code>IOCP</code> for event notification</li> </ul> </li> </ul>"},{"location":"02-networking/networking-internals/#listener-setup","title":"Listener Setup","text":"<pre><code>listener, err := net.Listen(\"tcp\", \":9000\")\nif err != nil {\n    panic(err)\n}\nfmt.Println(\"Echo server listening on :9000\")\n</code></pre> <p>Internals Involved:</p> <ul> <li><code>net.Listen()</code> returns a <code>TCPListener</code><ul> <li>Internally calls <code>syscall.socket</code>, <code>bind</code>, <code>listen</code></li> <li>Associates a <code>netFD</code> with the socket</li> </ul> </li> <li>The listener uses Go\u2019s internal poller to enable non-blocking <code>Accept</code></li> </ul>"},{"location":"02-networking/networking-internals/#accept-loop-and-goroutine-scheduling","title":"Accept Loop and Goroutine Scheduling","text":"<pre><code>for {\n    conn, err := listener.Accept()\n    if err != nil {\n        fmt.Printf(\"Accept error: %v\\n\", err)\n        continue\n    }\n    go handle(conn)\n}\n</code></pre> <p>Internals Involved:</p> <ul> <li><code>listener.Accept()</code> \u2192 <code>netFD.Accept()</code> \u2192 <code>poll.FD.Accept()</code> \u2192 <code>syscall.accept</code><ul> <li>Non-blocking, waits via Go's poller (<code>runtime_pollWait</code>)</li> </ul> </li> <li><code>go handle(conn)</code> spawns a goroutine (G)<ul> <li>Scheduled onto a P (Processor)</li> <li><code>P</code> is part of Go\u2019s M:N scheduler governed by <code>GOMAXPROCS</code></li> </ul> </li> </ul>"},{"location":"02-networking/networking-internals/#connection-handler","title":"Connection Handler","text":"<pre><code>func handle(conn net.Conn) {\n    defer conn.Close()\n\n    reader := bufio.NewReader(conn)\n\n    for {\n        conn.SetReadDeadline(time.Now().Add(5 * 60 * time.Second))\n\n        line, err := reader.ReadString('\\n')\n        if err != nil {\n            fmt.Printf(\"Connection closed: %v\\n\", err)\n            return\n        }\n\n        _, err = conn.Write([]byte(line))\n        if err != nil {\n            fmt.Printf(\"Write error: %v\\n\", err)\n            return\n        }\n    }\n}\n</code></pre> <p>Internals Involved:</p> <ul> <li><code>bufio.NewReader(conn)</code> wraps the <code>net.Conn</code>, which is backed by <code>*TCPConn</code> and <code>netFD</code>.</li> <li><code>ReadString()</code> calls <code>conn.Read()</code> under the hood:<ul> <li><code>netFD.Read()</code> \u2192 <code>poll.FD.Read()</code> \u2192 <code>syscall.Read()</code></li> <li>Uses <code>runtime_pollWait</code> to yield the goroutine if data isn't ready</li> </ul> </li> <li><code>SetReadDeadline</code> sets a timeout by integrating with the runtime's network poller to prevent indefinite blocking.</li> <li><code>conn.Write()</code> \u2192 <code>netFD.Write()</code> \u2192 <code>poll.FD.Write()</code> \u2192 <code>syscall.write</code></li> </ul>"},{"location":"02-networking/networking-internals/#internal-flow-diagram","title":"Internal Flow Diagram","text":"<pre><code>sequenceDiagram\n    participant L as Listener Goroutine\n    participant N as netFD\n    participant P as Go Poller\n    participant S as syscall layer\n    participant H as Handler Goroutine\n\n    L-&gt;&gt;N: Accept()\n    N-&gt;&gt;P: Wait for connection (runtime_pollWait)\n    P-&gt;&gt;S: syscall.accept\n    S--&gt;&gt;L: Return net.Conn\n    L-&gt;&gt;H: go handle(conn)\n\n    H-&gt;&gt;N: Read()\n    N-&gt;&gt;P: Wait for data (runtime_pollWait)\n    P-&gt;&gt;S: syscall.read\n    S--&gt;&gt;H: Return data\n    H-&gt;&gt;N: Write()\n    N-&gt;&gt;P: Check readiness\n    P-&gt;&gt;S: syscall.write\n    S--&gt;&gt;H: Confirm write</code></pre> <p>This model scales well as long as you:</p> <ul> <li>Ensure your <code>ulimit -n</code> is high enough</li> <li>Avoid shared state and contention</li> <li>Tune your GOMAXPROCS for your workload</li> </ul>"},{"location":"02-networking/networking-internals/#observations-at-scale","title":"Observations at Scale","text":"<p>As connections scale up (see how it may look like here):</p> <ul> <li>Per-connection memory and GC pressure grow</li> <li>Frequent goroutine context switching may introduce latency</li> <li>Coordinating channels, timeouts, and backpressure adds complexity</li> </ul> <p>Some mitigation strategies:</p> <ul> <li>Use <code>sync.Pool</code> for buffer reuse</li> <li>Minimize GC pauses (avoid per-request allocations)</li> <li>Prefer <code>netpoll</code>-friendly designs (avoid long CPU-bound goroutines)</li> </ul> <p>Go\u2019s model trades OS-level multiplexing for user-space scheduling and event-driven I/O coordination. It\u2019s not a silver bullet\u2014but when used correctly, it offers a robust platform for building scalable network services. Understanding these internals helps you avoid common traps, optimize at the right layer, and build systems that behave predictably under load.</p>"},{"location":"02-networking/quic-in-go/","title":"QUIC in Go: Building Low-Latency Services with quic-go","text":"<p>QUIC has emerged as a robust protocol, solving many inherent limitations of traditional TCP connections. QUIC combines encryption, multiplexing, and connection migration into a unified protocol, designed to optimize web performance, particularly in real-time and mobile-first applications. In Go, quic-go is the main QUIC implementation and serves as a practical base for building efficient, low-latency network services with built-in encryption and stream multiplexing.</p>"},{"location":"02-networking/quic-in-go/#understanding-quic","title":"Understanding QUIC","text":"<p>Originally developed at Google and later standardized by the IETF, QUIC rethinks the transport layer to overcome longstanding TCP limitations:</p> <ul> <li>Head-of-line blocking: TCP delivers a single ordered byte stream, so packet loss stalls everything behind it. QUIC splits data into independent streams, allowing others to proceed even when one is delayed.</li> <li>Per-packet encryption and header protection: QUIC applies encryption at the packet level, including selective header protection tied to packet numbers\u2014something DTLS\u2019s record-based framing can\u2019t support.</li> <li>Built-in transport mechanisms: QUIC handles stream multiplexing, flow control, and retransmissions as part of the protocol. DTLS, by contrast, only secures datagrams and leaves reliability and ordering to the application.</li> <li>Connection ID abstraction: QUIC identifies sessions using connection IDs rather than IP and port tuples, allowing connections to persist across network changes. DTLS provides no such abstraction, making mobility difficult to implement.</li> </ul>"},{"location":"02-networking/quic-in-go/#quic-vs-tcp-key-differences","title":"QUIC vs. TCP: Key Differences","text":"<p>QUIC takes a fundamentally different approach from TCP. While TCP is built directly on IP and requires a connection-oriented handshake before data can flow, QUIC runs over UDP and handles its own connection logic, reducing setup overhead and improving startup latency. This architectural choice allows QUIC to provide multiplexed, independent streams that effectively eliminate the head-of-line blocking issue commonly experienced with TCP, where the delay or loss of one packet stalls subsequent packets.</p> <p>QUIC integrates TLS 1.3 directly into its transport layer, eliminating the layered negotiation seen in TCP+TLS. This design streamlines the handshake process and enables 0-RTT data, where repeat connections can begin transmitting encrypted payloads immediately\u2014something TCP simply doesn\u2019t support.</p> <p>Another key distinction is how connections are identified. TCP connections are bound to a specific IP and port, so any change in network interface results in a broken connection. QUIC avoids this by using connection IDs that remain stable across address changes, allowing sessions to continue uninterrupted when a device moves between networks\u2014critical for mobile and latency-sensitive use cases.</p>"},{"location":"02-networking/quic-in-go/#is-quic-based-on-dtls","title":"Is QUIC Based on DTLS?","text":"<p>Although QUIC and DTLS both use TLS cryptographic primitives over UDP, QUIC does not build on DTLS. Instead, QUIC incorporates TLS 1.3 directly into its transport layer, inheriting only the cryptographic handshake\u2014not the record framing or protocol structure of DTLS.</p> <p>QUIC defines its own packet encoding, multiplexing, retransmission, and encryption formats. It wraps TLS handshake messages within QUIC packets and tightly couples encryption state with transport features like packet numbers and stream IDs. In contrast, DTLS operates as a secured datagram layer atop UDP, providing encryption and authentication but leaving transport semantics\u2014such as retransmit, ordering, or flow control\u2014to higher layers.</p> <p>The reasons for QUIC rejecting DTLS as its security base include:</p> <ul> <li>Tighter integration of handshake and transport: QUIC merges TLS negotiation with transport state setup, enabling 0\u2011RTT reuse and 1\u2011RTT setup in fewer round trips. DTLS\u2019s layered model introduces higher latency.</li> <li>Fine-grained encryption control: QUIC encrypts packet headers and payloads per-packet, bound to packet number and header offset. This is impossible with DTLS\u2019s coarse record layer.</li> <li>Native transport features: QUIC implements multiplexed, independent streams, per-stream flow control, and resilient retransmission logic. DTLS treats reliability and ordering as the application's responsibility.</li> <li>Connection migration capability: QUIC uses connection IDs decoupled from IP/port endpoints, enabling smooth network-interface switches. DTLS lacks this architectural property.</li> </ul> <p>In summary, QUIC uses TLS 1.3 for cryptographic handshake but eschews DTLS entirely, replacing it with a tightly integrated transport protocol. This design empowers QUIC to offer secure, low-latency, multiplexed, and mobile-friendly connections that DTLS\u2014optimized for secure datagram channels\u2014cannot match.</p>"},{"location":"02-networking/quic-in-go/#introducing-quic-go","title":"Introducing quic-go","text":"<p>quic-go implements the core IETF QUIC specification and supports most features required for production use, including TLS 1.3 integration, 0-RTT, stream multiplexing, and flow control. While some advanced capabilities like active connection migration are not yet implemented.</p>"},{"location":"02-networking/quic-in-go/#getting-started-with-quic-go","title":"Getting Started with quic-go","text":"<p>To start using <code>quic-go</code>, include it via Go modules:</p> <pre><code>go get github.com/quic-go/quic-go\n</code></pre>"},{"location":"02-networking/quic-in-go/#basic-quic-server","title":"Basic QUIC Server","text":"<p>A basic QUIC server setup in Go is conceptually similar to writing a traditional TCP server using the <code>net</code> package, but with several important distinctions.</p> <p>The initialization phase still involves listening on an address, but uses <code>quic.ListenAddr()</code> instead of <code>net.Listen()</code>. Unlike TCP, QUIC operates over UDP and requires a TLS configuration from the start, as all QUIC connections are encrypted by design. There\u2019s no need to manually wrap connections in TLS\u2014QUIC handles encryption as part of the protocol.</p> <pre><code>    listener, err := quic.ListenAddr(\"localhost:4242\", generateTLSConfig(), nil)\n    if err != nil {\n        log.Fatal(err)\n    }\n    fmt.Println(\"QUIC server listening on localhost:4242\")\n\n    for {\n        conn, err := listener.Accept(context.Background())\n        if err != nil {\n            log.Println(\"Accept error:\", err)\n            continue\n        }\n        go handleConn(conn)\n    }\n</code></pre> <p>After accepting a connection, handling diverges more significantly from the traditional <code>net.Conn</code> model. A single QUIC connection supports multiple independent streams, each functioning like a lightweight, ordered, bidirectional byte stream. These are accepted and handled independently, allowing concurrent interactions over a single connection without head-of-line blocking.</p> <pre><code>    defer conn.CloseWithError(0, \"bye\")\n\n    for {\n        stream, err := conn.AcceptStream(context.Background())\n        if err != nil {\n            return\n        }\n\n        go func(s quic.Stream) {\n            defer s.Close()\n\n            data, err := io.ReadAll(s)\n            if len(data) &gt; 0 {\n                log.Printf(\"Received: %s\", string(data))\n            }\n            if err != nil &amp;&amp; err != io.EOF {\n                if appErr, ok := err.(*quic.ApplicationError); !ok || appErr.ErrorCode != 0 {\n                    log.Println(\"read error:\", err)\n                }\n            }\n        }(stream)\n    }\n</code></pre> <p>This separation of initialization and per-stream handling is one of QUIC's most powerful features. With TCP, one connection equals one stream. With QUIC, one connection can carry dozens of concurrent, fully independent streams with isolated flow control and recovery behavior, allowing high-efficiency communication patterns with minimal latency.</p>"},{"location":"02-networking/quic-in-go/#multiplexed-streams","title":"Multiplexed Streams","text":"<p>QUIC inherently supports stream multiplexing, enabling simultaneous bidirectional communication without additional connection overhead. Streams operate independently, preventing head-of-line blocking, thus enhancing throughput and reducing latency.</p> <pre><code>stream, err := session.OpenStreamSync(context.Background())\nif err != nil {\n    log.Fatal(err)\n}\n_, err = stream.Write([]byte(\"Hello QUIC!\"))\n</code></pre>"},{"location":"02-networking/quic-in-go/#performance-quic-vs-http2-and-tcp","title":"Performance: QUIC vs. HTTP/2 and TCP","text":"<p>In performance benchmarks, QUIC frequently outperforms traditional HTTP/2 over TCP, particularly on lossy networks common in mobile environments. QUIC recovers faster from packet loss due to multiplexed streams and built-in congestion control algorithms like Cubic and BBR, integrated directly into the quic-go library.</p>"},{"location":"02-networking/quic-in-go/#connection-migration","title":"Connection Migration","text":"<p>One significant advantage of the QUIC protocol is its support for seamless connection migration(1), designed to allow mobile devices to maintain connections while switching networks (e.g., from Wi-Fi to cellular). This is enabled by connection IDs, which abstract away the client's IP address and port, allowing the server to continue communication even if the client's network path changes.</p> <ol> <li>See 9.2. Initiating Connection Migration and 9.3. Responding to Connection Migration sections from RFC 9000.</li> </ol> <p>However, in practice, connection migration depends on the specific implementation. In quic-go (as of v0.52.0), full active migration is not yet implemented. Here's what is supported:</p> <ul> <li>NAT rebinding works: If a client's IP or port changes due to NAT behavior or DHCP renewal, and the same connection ID is used, quic-go will continue the session without requiring a new connection. This is passive migration and requires no explicit action from the client.</li> <li>Interface switching (active migration) is not supported: Switching network interfaces\u2014such as moving from Wi-Fi to LTE\u2014requires sending packets from a new path and validating it with PATH_CHALLENGE and PATH_RESPONSE frames. The protocol defines this behavior, but quic-go does not implement it.</li> </ul>"},{"location":"02-networking/quic-in-go/#resilience-in-practice","title":"Resilience in Practice","text":"<p>QUIC is particularly effective in networks where reliability is difficult to guarantee\u2014mobile clients on LTE, IoT nodes on lossy links, or any edge system moving between access points. Rather than relying on techniques like forward error correction (dropped during IETF standardization), QUIC builds resilience into the core transport through a combination of stream isolation, fine-grained recovery, and flexible routing.</p> <p>QUIC connections are multiplexed: each stream runs independently with a separate flow control and delivery state. Packet loss on one stream doesn\u2019t interfere with others, avoiding the head-of-line blocking inherent in TCP. This alone gives QUIC a noticeable advantage in throughput on degraded links.</p> <p>Loss recovery in QUIC is driven by packet-level acknowledgments and RTT-based timers\u2014no reliance on TCP-style retransmission logic. Lost packets are detected faster, and retransmissions are scoped to the specific frames involved.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Server\n\n    Note over Client, Server: RTT = 100ms\n\n    Client-&gt;&gt;Server: Packet #1 (STREAM)\n    Client-&gt;&gt;Server: Packet #2 (STREAM)\n    Client--xServer: Packet #3 (lost)\n    Client-&gt;&gt;Server: Packet #4 (STREAM)\n\n    Note over Server: Server receives #1, #2, #4 (but not #3)\n\n    Server--&gt;&gt;Client: ACK [1-2, 4] after 100ms\n\n    Note over Client: Missing #3 is inferred lost\\nonce #4 is ACKed before it\n\n    Client-&gt;&gt;Server: Packet #5 (Retransmit lost frame from #3)\n    Server--&gt;&gt;Client: ACK [5]</code></pre>"},{"location":"02-networking/quic-in-go/#how-quic-acks-differ-from-tcp","title":"How QUIC ACKs Differ from TCP","text":"<p>While TCP includes support for SACK (Selective Acknowledgment), there are critical distinctions that make QUIC\u2019s loss recovery more robust and deterministic:</p> <ul> <li>SACK in TCP is optional and negotiated during handshake. QUIC\u2019s ACK ranges are always enabled and part of the core protocol.</li> <li>TCP acknowledges bytes, while QUIC acknowledges whole packets by number.</li> <li>Retransmissions in TCP are often byte-specific and tied to stream position. In QUIC, entire packets are retransmitted.</li> <li>QUIC ACKs are encrypted, making them tamper-resistant and resilient to on-path interference.</li> </ul> Feature TCP QUIC ACK granularity Byte-level Packet-level ACK range support Optional (via SACK) Mandatory Loss detection Duplicate ACKs, timers Gaps + RTT-based timers Retransmission granularity Partial stream bytes Full packet Encryption of ACKs No Yes"},{"location":"02-networking/quic-in-go/#connection-migration-and-middlebox-resilience","title":"Connection Migration and Middlebox Resilience","text":"<p>When a device moves between networks (e.g., Wi-Fi to LTE), QUIC's use of connection IDs allows it to maintain continuity. Connections aren't bound to IP-port pairs and don\u2019t require a full reconnect. Although <code>quic-go</code> doesn\u2019t yet support active migration, passive rebinding already works in practice.</p> <p>Because QUIC encrypts its transport metadata, it\u2019s also more robust against middlebox interference. Encrypted packet numbers, ACKs, and control frames reduce the risk of unintended behavior by on-path devices, which can degrade TCP performance.</p>"},{"location":"02-networking/quic-in-go/#congestion-control-flexibility","title":"Congestion Control Flexibility","text":"<p>Finally, QUIC enables pluggable congestion control. The protocol doesn\u2019t prescribe one algorithm\u2014BBR, Cubic, and custom logic are all possible at the application layer. This allows fine-tuning behavior for different latency and throughput tradeoffs.</p>"},{"location":"02-networking/quic-in-go/#0-rtt-connections","title":"0-RTT Connections","text":"<p>QUIC supports 0-RTT handshakes, allowing clients to send application data during the initial handshake on repeat connections. This reduces startup latency significantly. However, because 0-RTT data can be replayed by an attacker, it must be used carefully\u2014typically limited to idempotent operations and trusted clients.</p> <ol> <li>A replay attack occurs when an attacker captures valid network data\u2014such as a request or handshake\u2014and maliciously retransmits it to trick the server into executing it again. In the context of 0-RTT, since early data is sent before the handshake completes, it can be replayed by an adversary on a different connection, potentially causing duplicated actions (like double-purchasing or unauthorized state changes). This is why 0-RTT data must be idempotent or explicitly protected against replay.</li> </ol> <pre><code>    earlyConn, err := quic.DialAddrEarly(context.Background(), \"localhost:4242\", tlsConf, nil)\n</code></pre> <p>0-RTT is particularly beneficial for latency-sensitive applications like gaming, VoIP, and real-time financial data feeds.</p> Show the complete 0-RTT Server/Client examples 0-RTT Server <pre><code>package main\n\nimport (\n    \"context\"\n    \"crypto/tls\"\n    \"log\"\n    \"time\"\n\n    \"github.com/quic-go/quic-go\"\n)\n\nfunc main() {\n    sessionCache := tls.NewLRUClientSessionCache(128)\n    tlsConf := &amp;tls.Config{\n        InsecureSkipVerify: true,\n        EnableActiveMigration: true,\n        ClientSessionCache: sessionCache,\n        NextProtos:         []string{\"quic-0rtt-example\"},\n    }\n\n    // 1. Establish initial connection for session priming\n    conn, err := quic.DialAddr(context.Background(), \"localhost:4242\", tlsConf, nil)\n    if err != nil {\n        log.Fatal(err)\n    }\n    conn.CloseWithError(0, \"primed\")\n    time.Sleep(500 * time.Millisecond)\n\n    // 2. 0-RTT connection\n    // DialAddrEarly-start\n    earlyConn, err := quic.DialAddrEarly(context.Background(), \"localhost:4242\", tlsConf, nil)\n    // DialAddrEarly-end\n    if err != nil {\n        log.Fatal(\"0-RTT failed:\", err)\n    }\n    stream, err := earlyConn.OpenStreamSync(context.Background())\n    if err != nil {\n        log.Fatal(\"stream open failed:\", err)\n    }\n\n    message := \"Hello over 0-RTT\"\n    if _, err := stream.Write([]byte(message)); err != nil {\n        log.Fatal(\"write failed:\", err)\n    }\n    // Wait briefly to ensure data is sent before closing\n    time.Sleep(200 * time.Millisecond)\n\n    stream.Close()\n    earlyConn.CloseWithError(0, \"done\")\n    log.Println(\"0-RTT client sent:\", message)\n}\n</code></pre> 0-RTT Client <pre><code>package main\n\nimport (\n    \"context\"\n    \"crypto/tls\"\n    \"log\"\n    \"time\"\n\n    \"github.com/quic-go/quic-go\"\n)\n\nfunc main() {\n    sessionCache := tls.NewLRUClientSessionCache(128)\n    tlsConf := &amp;tls.Config{\n        InsecureSkipVerify: true,\n        EnableActiveMigration: true,\n        ClientSessionCache: sessionCache,\n        NextProtos:         []string{\"quic-0rtt-example\"},\n    }\n\n    // 1. Establish initial connection for session priming\n    conn, err := quic.DialAddr(context.Background(), \"localhost:4242\", tlsConf, nil)\n    if err != nil {\n        log.Fatal(err)\n    }\n    conn.CloseWithError(0, \"primed\")\n    time.Sleep(500 * time.Millisecond)\n\n    // 2. 0-RTT connection\n    // DialAddrEarly-start\n    earlyConn, err := quic.DialAddrEarly(context.Background(), \"localhost:4242\", tlsConf, nil)\n    // DialAddrEarly-end\n    if err != nil {\n        log.Fatal(\"0-RTT failed:\", err)\n    }\n    stream, err := earlyConn.OpenStreamSync(context.Background())\n    if err != nil {\n        log.Fatal(\"stream open failed:\", err)\n    }\n\n    message := \"Hello over 0-RTT\"\n    if _, err := stream.Write([]byte(message)); err != nil {\n        log.Fatal(\"write failed:\", err)\n    }\n    // Wait briefly to ensure data is sent before closing\n    time.Sleep(200 * time.Millisecond)\n\n    stream.Close()\n    earlyConn.CloseWithError(0, \"done\")\n    log.Println(\"0-RTT client sent:\", message)\n}\n</code></pre> <p>Expected Output</p> <p>When the server is started and the client is executed immediately afterward, you should see:</p> <p>Server Console <pre><code>QUIC server listening on localhost:4242\n2025/06/23 16:33:00 Received: Hello over 0-RTT\n</code></pre></p> <p>Client Console <pre><code>0-RTT client sent: Hello over 0-RTT\n</code></pre></p> <p>This confirms that early data was transmitted and accepted during the 0-RTT phase of a resumed session, without waiting for the full handshake to complete.</p>"},{"location":"02-networking/quic-in-go/#final-thoughts-on-quic-with-go","title":"Final Thoughts on QUIC with Go","text":"<p>QUIC is a transformative protocol with significant design advantages over TCP and HTTP/2, especially in the context of mobile-first and real-time systems. Its ability to multiplex streams without head-of-line blocking, reduce handshake latency through 0-RTT, and recover gracefully from packet loss makes it particularly effective in environments with unstable connectivity\u2014such as LTE, Wi-Fi roaming, or satellite uplinks.</p> <p>While the Go ecosystem benefits from quic-go as a mature userspace implementation, it's important to understand current limitations. Most notably, quic-go does not yet support full active connection migration as described in RFC 9000. Although it handles NAT rebinding passively\u2014maintaining sessions across address changes within the same network\u2014it lacks path validation and interface-switching logic required for full multi-homing or roaming support.</p> <p>In parallel, a Linux kernel implementation of QUIC is under active development, aiming to provide native support for the protocol alongside TCP and UDP. This effort, led by Lucien Xin, proposes a complete QUIC stack inside the kernel, including kTLS integration and socket-based API compatibility. If adopted, this would unlock new performance ceilings for QUIC under high load, bypassing userspace copy overhead and reducing syscall costs for data plane operations.</p> <p>In short, QUIC\u2019s architecture is well-positioned to outperform legacy transports\u2014especially in variable network conditions. While quic-go already enables many of these benefits, it\u2019s worth keeping in mind what\u2019s implemented today vs. what\u2019s defined by the spec. As ecosystem support deepens\u2014from kernel integration to advanced path management\u2014QUIC\u2019s full potential will become more accessible to systems operating at the edge of latency, reliability, and mobility.</p> <p>Using QUIC via the quic-go library gives developers access to a transport layer designed for modern network demands. Its built-in stream multiplexing, fast connection setup with 0-RTT, and ability to handle network path changes make it a strong fit for real-time systems and mobile applications where latency and reliability are critical.</p>"},{"location":"02-networking/resilient-connection-handling/","title":"Building Resilient Connection Handling with Load Shedding and Backpressure","text":"<p>In high-throughput services, connection floods and sudden spikes can saturate resources, leading to latency spikes or complete system collapse. This article dives into the low-level mechanisms\u2014circuit breakers, load shedding (passive and active), backpressure via channel buffering and timeouts\u2014and shows how to degrade or reject requests gracefully when pressure mounts.</p>"},{"location":"02-networking/resilient-connection-handling/#circuit-breakers-failure-isolation","title":"Circuit Breakers: Failure Isolation","text":"<p>Circuit breakers guard downstream dependencies by short\u2011circuiting calls when error rates or latencies exceed thresholds. Without them, a slow or failing service causes client goroutines to pile up, consuming all threads or connections and triggering cascading failure. This mechanism isolates failing services, preventing them from affecting the overall system stability. A circuit breaker continuously monitors response times and error rates, intelligently managing request flow and allowing the system to adapt to changing conditions automatically.</p>"},{"location":"02-networking/resilient-connection-handling/#what-it-does","title":"What It Does","text":"<p>A circuit breaker maintains three states:</p> <ul> <li>Closed: Requests flow through. Failures are counted over a rolling window.</li> <li>Open: Calls immediately return an error; no requests go to the target.</li> <li>Half-Open: A limited number of test requests are allowed; success transitions back to Closed, failure re-opens.</li> </ul> <pre><code>stateDiagram-v2\n    [*] --&gt; Closed\n    Closed --&gt; Open : errorRate &gt; threshold\n    Open --&gt; HalfOpen : resetTimeout expires\n    HalfOpen --&gt; Closed : testSuccess &gt;= threshold\n    HalfOpen --&gt; Open : testFailure</code></pre>"},{"location":"02-networking/resilient-connection-handling/#why-it-matters","title":"Why It Matters","text":"<p>Without circuit breakers, services depending on slow or failing components will eventually experience thread exhaustion, request queue buildup, and degraded tail latencies. Circuit breakers introduce bounded failure response by proactively rejecting requests once a dependency is known to be unstable. This reduces the impact surface of a single failure and increases system recoverability. During the Half-Open phase, only limited traffic probes the system, minimizing the risk of amplifying an unstable recovery. Circuit breakers are especially critical in distributed systems where fault domains span across network and service boundaries. They also serve as a feedback mechanism, signaling operational anomalies without requiring centralized alerting.</p>"},{"location":"02-networking/resilient-connection-handling/#implementation-sketch","title":"Implementation Sketch","text":"<p>There are many ways to implement a Circuit Breaker, each varying in complexity and precision. Some designs use fixed time windows, others rely on exponential backoff, or combine error rates with latency thresholds. In this article, we\u2019ll focus on a simple, practical approach: a sliding window with discrete time buckets for failure tracking, combined with a straightforward three-state machine to control call flow and recovery.</p> The Sketch <pre><code>flowchart TD\nsubgraph SlidingWindow [\"Sliding Window (last N intervals)\"]\n    B0((Bucket 0))\n    B1((Bucket 1))\n    B2((Bucket 2))\n    B3((Bucket 3))\n    B4((Bucket 4))\nend\n\nB0 -.-&gt; Tick1[\"Tick(): move idx + reset bucket\"]\nTick1 --&gt; B1\nB1 -.-&gt; Tick2[\"Tick()\"]\nTick2 --&gt; B2\nB2 -.-&gt; Tick3[\"Tick()\"]\nTick3 --&gt; B3\nB3 -.-&gt; Tick4[\"Tick()\"]\nTick4 --&gt; B4\nB4 -.-&gt; Tick5[\"Tick()\"]\nTick5 --&gt; B0\n\nB0 -.-&gt; SumFailures[\"Sum all failures\"]\n\nSumFailures --&gt;|Failures &gt;= errorThreshold| OpenCircuit[\"Circuit Opens\"]\n\nOpenCircuit --&gt; WaitReset[\"Wait resetTimeout\"]\nWaitReset --&gt; HalfOpen[\"Move to Half-Open state\"]\n\nsubgraph HalfOpenPhase [\"Half-Open Phase\"]\n    TryCall1(\"Try Call 1\")\n    TryCall2(\"Try Call 2\")\n    TryCall3(\"Try Call 3\")\nend\n\nHalfOpen --&gt; SuccessCheck[\"Check Successes\"]\nSuccessCheck --&gt;|Enough successes| CloseCircuit[\"Circuit Closes\"]\nSuccessCheck --&gt;|Failure during trial| ReopenCircuit[\"Circuit Re-Opens\"]\n\nReopenCircuit --&gt; WaitReset</code></pre> <p>First, we need a lightweight way to track how many failures have occurred recently. Instead of maintaining an unbounded history, we use a sliding window with fixed-size time buckets:</p> <pre><code>type slidingWindow struct {\n    buckets []int32\n    size    int\n    idx     int\n    mu      sync.Mutex\n}\n</code></pre> <p>Each bucket counts events for a short time slice. As time moves forward, we rotate to the next bucket and reset it, ensuring old data naturally fades away. Here's the core movement logic:</p> <pre><code>func (w *slidingWindow) Tick() {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    w.idx = (w.idx + 1) % w.size\n    atomic.StoreInt32(&amp;w.buckets[w.idx], 0)\n}\n</code></pre> <p>Summing across all buckets gives us the rolling view of recent failures.</p> <p>Rather than scattering magic numbers like 0, 1, and 2 across the codebase, we introduce named states using Go's <code>iota</code>:</p> <pre><code>type CircuitState int32\n\nconst (\n    StateClosed CircuitState = iota\n    StateOpen\n    StateHalfOpen\n)\n</code></pre> <p>Each state represents a clear behavior: in <code>Closed</code>, calls flow normally; in <code>Open</code>, calls are blocked to protect the system; in <code>Half-Open</code>, limited trial calls are allowed.</p> <p>The <code>CircuitBreaker</code> struct ties everything together, holding the sliding window, state, thresholds, and counters for tracking in-flight operations:</p> <pre><code>type CircuitBreaker struct {\n    failures              *slidingWindow\n    errorThresh           int\n    successThresh         int32\n    interval              time.Duration\n    resetTimeout          time.Duration\n    halfOpenMaxConcurrent int32\n\n    state          CircuitState\n    lastOpen       time.Time\n    successes      int32\n    inFlightTrials int32\n}\n</code></pre> <p>Initialization includes kicking off a background ticker to advance the sliding window:</p> <pre><code>func NewCircuitBreaker(errThresh int, succThresh int, interval, reset time.Duration, halfOpenMax int32) *CircuitBreaker {\n    cb := &amp;CircuitBreaker{\n        failures:              newWindow(60),\n        errorThresh:           errThresh,\n        successThresh:         int32(succThresh),\n        interval:              interval,\n        resetTimeout:          reset,\n        halfOpenMaxConcurrent: halfOpenMax,\n    }\n    go func() {\n        ticker := time.NewTicker(interval)\n        for range ticker.C {\n            cb.failures.Tick()\n        }\n    }()\n    return cb\n}\n</code></pre> <p>The <code>Allow()</code> method decides whether an incoming call should proceed:</p> <pre><code>func (cb *CircuitBreaker) Allow() bool {\n    switch atomic.LoadInt32((*int32)(&amp;cb.state)) {\n    case int32(StateClosed):\n        return true\n    case int32(StateOpen):\n        if time.Since(cb.lastOpen) &gt;= cb.resetTimeout {\n            atomic.StoreInt32((*int32)(&amp;cb.state), int32(StateHalfOpen))\n            atomic.StoreInt32(&amp;cb.successes, 0)\n            atomic.StoreInt32(&amp;cb.inFlightTrials, 0)\n            return true\n        }\n        return false\n    case int32(StateHalfOpen):\n        if atomic.LoadInt32(&amp;cb.inFlightTrials) &gt;= cb.halfOpenMaxConcurrent {\n            return false\n        }\n        atomic.AddInt32(&amp;cb.inFlightTrials, 1)\n        return true\n    }\n    return true\n}\n</code></pre> <p>This ensures that after an open timeout, only a controlled number of trial requests are permitted.</p> <p>After each call, we report its outcome so the breaker can adjust:</p> <pre><code>func (cb *CircuitBreaker) Report(success bool) {\n    if !cb.Allow() {\n        return\n    }\n    defer func() {\n        if atomic.LoadInt32((*int32)(&amp;cb.state)) == int32(StateHalfOpen) {\n            atomic.AddInt32(&amp;cb.inFlightTrials, -1)\n        }\n    }()\n\n    switch atomic.LoadInt32((*int32)(&amp;cb.state)) {\n    case int32(StateClosed):\n        if !success {\n            cb.failures.Inc()\n            if int(cb.failures.Sum()) &gt;= cb.errorThresh {\n                atomic.StoreInt32((*int32)(&amp;cb.state), int32(StateOpen))\n                cb.lastOpen = time.Now()\n            }\n        }\n\n    case int32(StateHalfOpen):\n        if success {\n            if atomic.AddInt32(&amp;cb.successes, 1) &gt;= cb.successThresh {\n                atomic.StoreInt32((*int32)(&amp;cb.state), int32(StateClosed))\n            }\n        } else {\n            atomic.StoreInt32((*int32)(&amp;cb.state), int32(StateOpen))\n            cb.lastOpen = time.Now()\n        }\n    }\n}\n</code></pre> <p>Failures during normal operation cause the circuit to Open. Successes during Half-Open gradually rebuild trust, closing the circuit when enough healthy calls succeed.</p> <p>Putting it all together:</p> <pre><code>breaker := NewCircuitBreaker(\n    10,              // open after 10 failures\n    5,               // close after 5 half-open successes\n    time.Second,     // tick every second\n    10*time.Second,  // remain open for 10 seconds\n    3,               // allow up to 3 trial calls\n)\n\nif breaker.Allow() {\n    success := callRemoteService()\n    breaker.Report(success)\n}\n</code></pre> <p>This approach protects systems under distress, recovers cautiously, and maintains throughput where possible.</p>"},{"location":"02-networking/resilient-connection-handling/#load-shedding-passive-vs-active","title":"Load Shedding: Passive vs Active","text":"<p>Load shedding refers to the practice of shedding, or dropping, excess load in order to protect system integrity. It becomes a necessity when demand exceeds the sustainable capacity of a service, particularly under conditions of degraded performance or partial failure. By rejecting less important work, a system can focus on fulfilling critical requests and maintaining stability. Load shedding can be implemented either passively\u2014relying on queues and resource limits\u2014or actively\u2014based on observed performance metrics. The balance between these two methods determines the trade-off between simplicity, responsiveness, and accuracy in overload scenarios.</p>"},{"location":"02-networking/resilient-connection-handling/#passive-load-shedding","title":"Passive Load Shedding","text":"<p>Passive load shedding is a minimalistic but highly effective mechanism that relies on the natural limits of bounded queues to regulate request flow. When a bounded buffer or channel reaches its capacity, any additional incoming request is either blocked or dropped. This approach places no computational overhead on the system and doesn't require runtime telemetry or complex decision-making logic. It serves as a coarse-grained, first-line defense against unbounded load by defining strict queue limits and enforcing backpressure implicitly. Passive shedding is particularly suitable for latency-sensitive systems that prefer quick rejection over queue buildup.</p> <pre><code>flowchart TD\n    A[Incoming Connection] --&gt; B{Channel Full?}\n    B -- No --&gt; C[Enqueue Request]\n    B -- Yes --&gt; D[Drop Connection]</code></pre> The Sketch <p>In this implementation, we use a buffered channel to introduce a hard upper limit on how many requests the system will queue for processing. When new connections arrive, they are either enqueued immediately if there\u2019s available buffer space, or dropped without processing if the channel is full. This style of passive load shedding is simple, deterministic, and highly effective for services where it is better to reject excess load early rather than risk cascading failures deeper inside the system. It provides a natural form of admission control without adding complex queuing, retries, or explicit rejection signaling. <pre><code>// A buffered channel of size N implements passive load shedding.\n// When full, new requests are silently dropped (connection closed).\nrequests := make(chan *Request, 1000)\n\n// acceptLoop continuously accepts new connections and enqueues them\n// if there is capacity; otherwise, it drops excess load immediately.\nfunc acceptLoop(ln net.Listener) {\n    for {\n        conn, err := ln.Accept()\n        if err != nil {\n            continue // transient accept error, skip\n        }\n        req := &amp;Request{conn: conn}\n\n        select {\n        case requests &lt;- req:\n            // Request accepted and queued for processing.\n        default:\n            // Channel full: drop request immediately to avoid overload.\n            conn.Close()\n        }\n    }\n}\n</code></pre></p>"},{"location":"02-networking/resilient-connection-handling/#why-it-matters_1","title":"Why It Matters","text":"<p>Passive load shedding leverages natural constraints in bounded resources to apply backpressure at the system edges. When queues are full, rejecting new work avoids exacerbating downstream bottlenecks or amplifying queuing delays. This method is low-overhead and deterministic\u2014services either have space to process or reject immediately. However, it lacks sensitivity to CPU or memory pressure, making it best suited as a safety valve rather than a comprehensive control strategy. Passive shedding also plays a key role in fail-fast systems where speed of rejection is preferable to prolonged degradation. It simplifies overload protection without external observability dependencies.</p>"},{"location":"02-networking/resilient-connection-handling/#active-load-shedding","title":"Active Load Shedding","text":"<p>Active load shedding introduces a higher degree of intelligence and responsiveness by integrating system telemetry\u2014such as CPU load, memory usage, request latencies, or custom business KPIs\u2014into the decision-making process. Rather than reacting only when queues overflow, active shedding proactively evaluates system health and begins dropping or deferring traffic based on dynamic thresholds. This allows services to stay ahead of resource exhaustion, make more fine-grained decisions, and prioritize critical workloads. Active shedding is more computationally expensive and complex than passive techniques, but offers higher precision and adaptability, especially in bursty or unpredictable environments.</p> <pre><code>flowchart TD\n    A[Incoming Request] --&gt; B{CPU Load &gt; Threshold?}\n    B -- Yes --&gt; C[Reject Request]\n    B -- No --&gt; D[Accept and Process]</code></pre> The Sketch <p>In this design, active load shedding is driven by real-time system metrics \u2014 specifically CPU usage. The shedder object monitors CPU load at a regular interval and flips a global shedding flag when the load exceeds a defined threshold. When the flag is active, new incoming connections are proactively rejected, even if the internal queues could technically still accept them. This approach allows the system to respond dynamically to environmental pressure, rather than passively waiting for internal backlogs to accumulate. It\u2019s particularly effective for services where CPU saturation is a leading indicator of imminent degradation.</p> <pre><code>// shedder monitors system CPU load and decides whether to shed incoming requests.\ntype shedder struct {\n    maxCPU    float64        // CPU usage threshold to start shedding\n    checkFreq time.Duration  // frequency to check CPU load\n}\n\n// ShouldShed checks current CPU usage against the configured maximum.\nfunc (s *shedder) ShouldShed() bool {\n    cpu := getCPULoad()\n    return cpu &gt; s.maxCPU\n}\n\n// startMonitor periodically evaluates CPU load and updates the global shedding flag.\nfunc (s *shedder) startMonitor() {\n    ticker := time.NewTicker(s.checkFreq)\n    for range ticker.C {\n        if s.ShouldShed() {\n            atomic.StoreInt32(&amp;shedding, 1) // enter shedding mode\n        } else {\n            atomic.StoreInt32(&amp;shedding, 0) // exit shedding mode\n        }\n    }\n}\n\n// During request acceptance, the shedding flag is checked to actively reject overload.\nif atomic.LoadInt32(&amp;shedding) == 1 {\n    conn.Close() // actively reject new connection\n} else {\n    enqueue(conn) // accept and process normally\n}\n</code></pre>"},{"location":"02-networking/resilient-connection-handling/#why-it-matters_2","title":"Why It Matters","text":"<p>Active shedding enables services to respond to nuanced overload conditions by inspecting real-time system health signals. Unlike passive strategies, it doesn't wait for queues to overflow but anticipates risk based on dynamic telemetry. This leads to earlier rejection and more graceful degradation. Because it incorporates CPU usage, latency, and error rate into decision logic, active shedding is especially effective in CPU-bound workloads or mixed-load services. However, it requires careful calibration to avoid false positives and oscillation. When tuned properly, active shedding reduces latency tail spikes and increases overall system fairness under contention.</p>"},{"location":"02-networking/resilient-connection-handling/#backpressure-strategies","title":"Backpressure Strategies","text":"<p>Backpressure is a fundamental control mechanism in concurrent systems that prevents fast producers from overwhelming slower consumers. By imposing limits on how much work can be queued or in-flight, backpressure ensures that system throughput remains stable and predictable. It acts as a contract between producers and consumers: \"only send more when there's capacity to handle it.\" Effective backpressure strategies protect both local and remote components from runaway memory growth, scheduling contention, and thrashing. In Go, backpressure is often implemented using buffered channels, context cancellation, and timeouts, each offering a different degree of strictness and complexity.</p>"},{"location":"02-networking/resilient-connection-handling/#buffered-channel-backpressure","title":"Buffered Channel Backpressure","text":"<p>Buffered channels are the most direct form of backpressure in Go. They provide a queue with fixed capacity that blocks the sender once full, naturally throttling the producer to match the consumer's pace. This backpressure is enforced by the Go runtime without requiring additional logic, making it a convenient choice for simple pipelines and high-throughput services. Properly sizing the channel is essential to balance throughput and latency: too small leads to frequent stalls; too large risks uneven latency and poor garbage collection performance. Buffered channels are best used when traffic volume is consistent and processing times are predictable.</p> <pre><code>sequenceDiagram\n    participant Producer\n    participant Buffer\n    participant Consumer\n    Producer-&gt;&gt;Buffer: Send Request\n    Buffer--&gt;&gt;Producer: Blocks if full\n    Buffer-&gt;&gt;Consumer: Process Request</code></pre> The Sketch <p>In this model, a buffered channel acts as a natural backpressure mechanism. Producers (in this case, connection handlers) push requests into the requests channel. As long as there\u2019s available buffer space, enqueueing is non-blocking and fast. However, once the channel fills up, the producer blocks automatically until a consumer reads from the channel and frees up space. This design elegantly slows down intake when processing can\u2019t keep up, preventing memory bloat or CPU exhaustion without requiring explicit shedding logic.</p> <pre><code>// requests is a buffered channel that provides natural backpressure.\n// When full, producers block until space becomes available.\nrequests := make(chan *Request, 500)\n\n// Producer loop reads incoming connections and enqueues them.\n// Blocks automatically when the channel is full, applying backpressure upstream.\nfor conn := range incomingConns {\n    req := &amp;Request{conn: conn}\n    requests &lt;- req // blocks when buffer reaches 500\n}\n</code></pre>"},{"location":"02-networking/resilient-connection-handling/#why-it-matters_3","title":"Why It Matters","text":"<p>Buffered channels enforce backpressure at the point of communication, ensuring that a producer cannot outpace the consumer beyond a predefined capacity. This prevents unbounded memory growth and protects downstream systems from congestion collapse. When the buffer is full, producers block until space becomes available, creating a natural throttling mechanism that requires no coordination protocol or central scheduler. This behavior aligns producer throughput with consumer availability, smoothing bursts and avoiding CPU starvation caused by unbounded goroutine creation. Moreover, because this mechanism is handled by the Go runtime, it adds minimal overhead and is easy to reason about in concurrent pipelines. However, incorrect buffer sizing can lead to head-of-line blocking, increased latency jitter, or premature rejection upstream, so sizing decisions must be based on empirical throughput metrics and latency tolerance.</p>"},{"location":"02-networking/resilient-connection-handling/#timeouts-and-context-cancellation","title":"Timeouts and Context Cancellation","text":"<p>Context cancellation and timeouts allow developers to specify explicit upper bounds on how long operations should block or wait. In overload conditions, timeouts prevent indefinite contention for shared resources and help preserve service-level objectives (SLOs) by bounding tail latencies. By layering timeout-based logic onto blocking calls, services can fail early when overwhelmed and avoid accumulating stale work. Context propagation also enables coordinated deadline enforcement across distributed systems, ensuring that latency targets are respected end-to-end. This method is particularly effective in systems with real-time constraints or those requiring precise error handling under partial failure.</p> <pre><code>flowchart TD\n    A[Send Request] --&gt; B{Timeout Exceeded?}\n    B -- No --&gt; C[Enqueue in Channel]\n    B -- Yes --&gt; D[Cancel or Drop Request]</code></pre> The Sketch <p>In this approach, timeouts and context cancellation are used to bound how long a request can wait to enter the system. If the requests channel is immediately ready, the request is accepted and queued for processing.  If the channel remains full beyond the timeout (50 milliseconds in this case), the context fires, and the request is dropped explicitly by closing the underlying connection. This technique ensures that no request waits indefinitely, giving the system tight control over tail latencies and preventing hidden buildup under load.</p> <pre><code>// Set up a context with a strict timeout to bound enqueue latency.\nctx, cancel := context.WithTimeout(context.Background(), 50*time.Millisecond)\ndefer cancel()\n\n// Attempt to enqueue the request with timeout protection.\nselect {\ncase requests &lt;- req:\n    // Request accepted into the processing queue.\ncase &lt;-ctx.Done():\n    // Timeout exceeded before enqueue succeeded; drop or fallback.\n    req.conn.Close()\n}\n</code></pre> <p>The choice of timeout duration (e.g., 50ms vs 200ms) has a significant impact on system behavior under load.</p> <ul> <li>Shorter timeouts (like 50ms) favor fairness \u2014 ensuring that no single request hogs system resources while waiting. This helps the system reject overload quickly and keeps end-to-end latency predictable, but it can slightly reduce overall throughput if temporary congestion is frequent.</li> <li>Longer timeouts (like 200ms) favor throughput \u2014 allowing temporary spikes in load to be absorbed if downstream recovery is fast enough. However, longer waits can increase tail latencies, cause uneven request handling, and potentially exhaust resources during sustained overload.</li> </ul> <p>Tuning the timeout is a tradeoff between protecting system responsiveness versus maximizing work completion rate. For most high-volume services, shorter timeouts combined with passive load shedding typically lead to better stability and user experience.</p>"},{"location":"02-networking/resilient-connection-handling/#why-it-matters_4","title":"Why It Matters","text":"<p>Timeouts and context cancellation provide deterministic bounds on request lifecycle and system resource usage, which are essential in high-concurrency environments. Without these constraints, blocked operations can accumulate, leading to goroutine leaks, memory exhaustion, or increased tail latency as contention builds up. Timeouts allow systems to discard stale work that is unlikely to succeed within acceptable SLA thresholds, preserving responsiveness under load. Context cancellation enables hierarchical deadline propagation across service boundaries, ensuring consistent behavior and simplifying distributed timeout management. Additionally, early termination of blocked operations improves throughput under saturation by allowing retry-capable clients to shift load to healthier replicas or degrade gracefully. This pattern is critical in environments with strict latency objectives or dynamic load patterns, where predictable failure is preferable to delayed or non-deterministic success.</p>"},{"location":"02-networking/resilient-connection-handling/#dynamic-buffer-sizing","title":"Dynamic Buffer Sizing","text":"<p>Dynamic buffer sizing adds elasticity to the backpressure model, allowing services to adapt their buffering capacity to current load conditions. This approach is valuable in workloads that exhibit high variability or in systems that must handle periodic bursts without shedding. Implementations often rely on resizable queues or buffer pools, sometimes coordinated with autoscaling signals or performance feedback loops. Although more complex than fixed-size buffers, dynamic sizing can reduce latency spikes and resource contention by matching capacity to demand more closely. Careful concurrency management and race-avoidance techniques are essential to maintain safety in dynamic resizing logic.</p> <pre><code>flowchart TD\n    A[Incoming Requests] --&gt; B{Buffer Usage High?}\n    B -- Yes --&gt; C[Increase Buffer Size]\n    C --&gt; D[Reconfigure Channel or Queue]\n    D --&gt; E[Continue Processing]\n\n    B -- No --&gt; F{Buffer Usage Low?}\n    F -- Yes --&gt; G[Decrease Buffer Size]\n    G --&gt; D\n\n    F -- No --&gt; E</code></pre> The Sketch <p>In this model, the system keeps a tight feedback loop between workload intensity and resource provisioning. If the incoming request rate overwhelms the buffer (for example, reaching over 80% usage), the buffer automatically grows \u2014 doubling its capacity up to a maximum ceiling. On the other hand, if demand drops and the buffer remains underutilized (say below 20%), it shrinks conservatively to free memory.</p> <p>This technique is especially valuable in environments where traffic patterns are unpredictable \u2014 giving your service better burst tolerance without permanently oversizing infrastructure.</p> <p>Because resizing operations involve draining and recreating channels, all access is safely guarded with a mutex (mu) to avoid data races or inconsistency between producers and consumers.</p> <pre><code>// DynamicBuffer wraps a buffered channel and automatically resizes it\n// based on usage thresholds. This enables better elasticity under varying load.\ntype DynamicBuffer struct {\n    mu        sync.Mutex\n    ch        chan Request     // underlying buffered channel\n    minSize   int              // minimum buffer capacity\n    maxSize   int              // maximum buffer capacity\n    growPct   float64          // grow if usage exceeds this fraction\n    shrinkPct float64          // shrink if usage falls below this fraction\n}\n\n// NewDynamicBuffer initializes a dynamic buffer with initial capacity and growth rules.\n// It also starts a background monitor that periodically evaluates whether resizing is needed.\nfunc NewDynamicBuffer(initial, min, max int, growPct, shrinkPct float64) *DynamicBuffer {\n    db := &amp;DynamicBuffer{\n        ch:        make(chan Request, initial),\n        minSize:   min,\n        maxSize:   max,\n        growPct:   growPct,\n        shrinkPct: shrinkPct,\n    }\n    go db.monitor()\n    return db\n}\n\n// Enqueue adds a request into the channel.\n// If the channel is full, this call blocks until space is available.\nfunc (db *DynamicBuffer) Enqueue(req Request) {\n    db.mu.Lock()\n    ch := db.ch\n    db.mu.Unlock()\n\n    ch &lt;- req\n}\n\n// Dequeue retrieves a request from the channel or aborts if the context expires.\n// This ensures consumers can cancel work if needed without hanging indefinitely.\nfunc (db *DynamicBuffer) Dequeue(ctx context.Context) (Request, bool) {\n    select {\n    case req := &lt;-db.ch:\n        return req, true\n    case &lt;-ctx.Done():\n        return Request{}, false\n    }\n}\n\n// monitor runs periodically, evaluating the channel's fill ratio,\n// and triggers resizing if usage crosses configured thresholds.\nfunc (db *DynamicBuffer) monitor() {\n    ticker := time.NewTicker(1 * time.Second)\n    for range ticker.C {\n        db.mu.Lock()\n\n        oldCh := db.ch\n        cap := cap(oldCh)\n        length := len(oldCh)\n        usage := float64(length) / float64(cap)\n\n        var newSize int\n        if usage &gt; db.growPct &amp;&amp; cap &lt; db.maxSize {\n            // If heavily loaded, double the buffer size, but cap it at maxSize\n            newSize = min(db.maxSize, cap*2)\n        } else if usage &lt; db.shrinkPct &amp;&amp; cap &gt; db.minSize {\n            // If lightly loaded, shrink the buffer to half, but not below minSize\n            newSize = max(db.minSize, cap/2)\n        }\n\n        if newSize != 0 {\n            // Create a new channel with the updated size and drain old requests into it\n            newCh := make(chan Request, newSize)\n            for len(oldCh) &gt; 0 {\n                newCh &lt;- &lt;-oldCh\n            }\n            db.ch = newCh\n        }\n\n        db.mu.Unlock()\n    }\n}\n</code></pre>"},{"location":"02-networking/resilient-connection-handling/#why-it-matters_5","title":"Why It Matters","text":"<p>Dynamic buffer sizing complements reactive backpressure with proactive adaptability. By tuning buffer capacity based on observed queue utilization, systems can respond early to sustained pressure without overcommitting memory or deferring rejection decisions until saturation. This elasticity helps smooth out latency spikes during transient load surges and prevents over-allocation during idle periods, preserving headroom for other critical components. Unlike fixed-size buffers that force developers to trade off between burst tolerance and memory efficiency, dynamically sized buffers evolve with workload shape\u2014absorbing shocks without degrading steady-state performance. When integrated with metrics, autoscaling triggers, or performance-aware feedback loops, they become a foundational tool for achieving predictable behavior under unpredictable demand.</p>"},{"location":"02-networking/resilient-connection-handling/#graceful-rejection-and-degradation","title":"Graceful Rejection and Degradation","text":"<p>Graceful rejection and degradation ensure that when overload conditions occur, the service doesn't simply fail but instead provides fallback behavior that preserves core functionality or communicates the system's status clearly to clients. These mechanisms are essential for maintaining user experience and system operability under stress. Rejection involves explicitly refusing to handle a request, often with guidance on when to retry, while degradation refers to reducing the scope or fidelity of a response. Together, they offer a layered resilience model that prioritizes transparency, usability, and continued availability of critical paths.</p>"},{"location":"02-networking/resilient-connection-handling/#http-level-rejection","title":"HTTP-Level Rejection","text":"<p>Returning well-formed HTTP error responses allows systems to signal overload without leaving clients in limbo. The use of standard status codes, such as <code>503 Service Unavailable</code>, provides clear semantics for retry logic and enables intermediate systems\u2014like load balancers and proxies\u2014to react appropriately. The <code>Retry-After</code> header suggests a delay for future attempts, reducing immediate retry storms. These rejections form the outer perimeter of overload defense, filtering requests at the earliest possible point to reduce system strain. When combined with structured observability, HTTP-level rejections help diagnose performance regressions and load hotspots.</p> <pre><code>flowchart TD\n    A[Request Received] --&gt; B{Overloaded?}\n    B -- Yes --&gt; C[Return 503 + Retry-After]\n    B -- No --&gt; D[Process Request]</code></pre> The Sketch <p>This example shows how to implement graceful overload rejection at the HTTP layer. Instead of letting requests pile up in queues or consume server threads during overload, the handler checks system health via isOverloaded(). If the server is under pressure, it returns a 503 Service Unavailable response with a Retry-After header. This explicitly asks clients to wait before retrying, which is especially useful for well-behaved HTTP clients, load balancers, or reverse proxies that honor such signals.</p> <p>By rejecting early and clearly, you reduce backend strain, avoid cascading timeouts, and preserve responsiveness for healthy traffic.</p> <pre><code>// This HTTP handler implements basic overload protection at the protocol level.\n// When the system is under pressure, it responds with a 503 and Retry-After header,\n// signaling clients to back off temporarily rather than retry aggressively.\n\nhttp.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n    if isOverloaded() {\n        // Inform the client that the server is temporarily unavailable.\n        w.Header().Set(\"Retry-After\", \"5\") // suggest waiting 5 seconds before retrying\n        w.WriteHeader(http.StatusServiceUnavailable)\n        _, _ = w.Write([]byte(\"Service is temporarily overloaded. Please try again later.\"))\n        return\n    }\n\n    // Otherwise, proceed with request handling\n    process(r.Context(), w)\n})\n</code></pre>"},{"location":"02-networking/resilient-connection-handling/#why-it-matters_6","title":"Why It Matters","text":"<p>Explicitly rejecting requests with HTTP status codes, particularly 503 Service Unavailable, ensures that overload conditions are surfaced in a protocol-compliant and client-visible manner. This avoids opaque timeouts or TCP resets that are hard to diagnose and can trigger inefficient retry behavior. By including headers like Retry-After, services communicate expected recovery windows and encourage exponential backoff, reducing the risk of synchronized retry storms. This pattern is especially effective at the system perimeter\u2014APIs, gateways, and load balancers\u2014where early rejection can deflect pressure from internal systems. Additionally, structured rejection improves observability, making it easier to correlate client behavior with internal resource constraints. It also enables intermediate systems (e.g., CDNs or edge proxies) to absorb or delay traffic intelligently, providing an additional buffer layer.</p>"},{"location":"02-networking/resilient-connection-handling/#feature-degradation","title":"Feature Degradation","text":"<p>Feature degradation allows services to selectively conserve resources by disabling or simplifying non-essential behavior under load. Instead of failing entirely, the system returns a leaner response\u2014such as omitting analytics, personalization, or dynamic content\u2014while preserving critical functionality. This approach helps maintain perceived uptime and minimizes business impact, especially in customer-facing applications where total failure is unacceptable. Degradation also reduces the computational and I/O footprint per request, freeing up headroom for other traffic classes. Strategically designed degraded paths can absorb load surges while retaining cacheability and statelessness, which aids horizontal scaling. It is essential, however, to validate degraded modes with the same rigor as normal ones to avoid introducing silent data loss or inconsistencies during fallback scenarios.</p> <pre><code>flowchart TD\n    A[Request Received] --&gt; B{High Load?}\n    B -- Yes --&gt; C[Return Degraded Response]\n    B -- No --&gt; D[Return Full Response]</code></pre> The Sketch <p>This example shows how to implement graceful overload rejection at the HTTP layer. Instead of letting requests pile up in queues or consume server threads during overload, the handler checks system health via <code>isOverloaded()</code>. If the server is under pressure, it returns a 503 Service Unavailable response with a Retry-After header. This explicitly asks clients to wait before retrying, which is especially useful for well-behaved HTTP clients, load balancers, or reverse proxies that honor such signals.</p> <p>By rejecting early and clearly, you reduce backend strain, avoid cascading timeouts, and preserve responsiveness for healthy traffic.</p> <pre><code>if highLoad() {\n    // degrade: return minimal response\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    w.Write([]byte(`{\"data\":\"partial\"}`))\n    return\n}\n</code></pre>"},{"location":"02-networking/resilient-connection-handling/#why-it-matters_7","title":"Why It Matters","text":"<p>Feature degradation allows services to selectively conserve resources by disabling or simplifying non-essential behavior under load. Instead of failing entirely, the system returns a leaner response\u2014such as omitting analytics, personalization, or dynamic content\u2014while preserving critical functionality. This approach helps maintain perceived uptime and minimizes business impact, especially in customer-facing applications where total failure is unacceptable. Degradation also reduces the computational and I/O footprint per request, freeing up headroom for other traffic classes. Strategically designed degraded paths can absorb load surges while retaining cacheability and statelessness, which aids horizontal scaling. It is essential, however, to validate degraded modes with the same rigor as normal ones to avoid introducing silent data loss or inconsistencies during fallback scenarios.</p> <p>Handling overload is not a one-off feature but an architectural mindset. Circuit breakers isolate faults, load shedding preserves core capacity, backpressure smooths traffic, and graceful degradation maintains user trust. Deeply understanding each pattern and its trade\u2011offs is essential when building services that withstand the unpredictable.</p> <p>Continue exploring edge cases\u2014such as starvation under mixed load classes or coordination across microservices\u2014and tune thresholds based on real metrics. With these foundations, your system stays responsive even under the heaviest of loads.</p>"},{"location":"02-networking/tcp-http2-grpc/","title":"Comparing TCP, HTTP/2, and gRPC Performance in Go","text":"<p>In distributed systems, the choice of communication protocol shapes how services interact under real-world load. It influences not just raw throughput and latency, but also how well the system scales, how much CPU and memory it consumes, and how predictable its behavior remains under pressure. In this article, we dissect three prominent options\u2014raw TCP with custom framing, HTTP/2 via Go's built-in <code>net/http</code> package, and gRPC\u2014and explore their performance characteristics through detailed benchmarks and practical scenarios.</p>"},{"location":"02-networking/tcp-http2-grpc/#raw-tcp-with-custom-framing","title":"Raw TCP with Custom Framing","text":"<p>Raw TCP provides maximum flexibility with virtually no protocol overhead, but that comes at a cost: all message boundaries, framing logic, and error handling must be implemented manually. Since TCP delivers a continuous byte stream with no inherent notion of messages, applications must explicitly define how to separate and interpret those bytes.</p>"},{"location":"02-networking/tcp-http2-grpc/#custom-framing-protocol","title":"Custom Framing Protocol","text":"<p>A common way to handle message boundaries over TCP is to use length-prefix framing: each message starts with a 4-byte header that tells the receiver how many bytes to read next. The length is encoded in big-endian format, following the standard network byte order, so it behaves consistently across different systems. This setup solves a core issue with TCP\u2014while it guarantees reliable delivery, it doesn\u2019t preserve message boundaries. Without knowing the size upfront, the receiver has no way to tell where one message ends and the next begins.</p> <p>TCP guarantees reliable, in-order delivery of bytes, but it does not preserve or indicate message boundaries. For example, if a client sends three logical messages:</p> <pre><code>[msg1][msg2][msg3]\n</code></pre> <p>the server may receive them as a continuous byte stream with arbitrary segmentations, such as:</p> <pre><code>[msg1_part][msg2][msg3_part]\n</code></pre> <p>TCP delivers a continuous stream of bytes with no built-in concept of where one message stops and another starts. This means the receiver can\u2019t rely on read boundaries to infer message boundaries\u2014what arrives might be a partial message, multiple messages concatenated, or an arbitrary slice of both. To make sense of structured data over such a stream, the application needs a framing strategy. Length-prefixing does this by including the size of the message up front, so the receiver knows exactly how many bytes to expect before starting to parse the payload.</p>"},{"location":"02-networking/tcp-http2-grpc/#protocol-structure","title":"Protocol Structure","text":"<p>While length-prefixing is the most common and efficient framing strategy, there are other options depending on the use case. Other framing strategies exist, each with its own trade-offs in terms of simplicity, robustness, and flexibility. Delimiter-based framing uses a specific byte or sequence\u2014like <code>\\n</code> or <code>\\0</code> to signal the end of a message. It\u2019s easy to implement but fragile if the delimiter can appear in the payload. Fixed-size framing avoids ambiguity by making every message the same length, which simplifies parsing and memory allocation but doesn\u2019t work well when message sizes vary. Self-describing formats like Protobuf or ASN.1 embed length and type information inside the payload itself, allowing for richer structure and evolution over time, but require more sophisticated parsing logic and schema awareness on both ends. Choosing the right approach depends on how much control you need, how predictable your data is, and how much complexity you\u2019re willing to absorb.</p> <p>Each frame of length-prefixing implementation consists of:</p> <pre><code>| Length (4 bytes) | Payload (Length bytes) |\n</code></pre> <ul> <li>Length: A 4-byte unsigned integer encoded in big-endian format (network byte order), representing the number of bytes in the payload.</li> <li>Payload: Raw binary data of arbitrary length.</li> </ul> <p>The use of <code>binary.BigEndian.PutUint32</code> ensures the frame length is encoded in a standardized format\u2014most significant byte first. This is consistent with Internet protocol standards (1) , allowing for predictable decoding and reliable interoperation between heterogeneous systems.</p> <ol> <li>Following the established convention of network byte order, which is defined as big-endian in RFC 791, Section 3.1 and used consistently in transport and application protocols such as TCP (RFC 793).</li> </ol> <pre><code>func writeFrame(conn net.Conn, payload []byte) error {\n    frameLen := uint32(len(payload))\n    buf := make([]byte, 4+len(payload))\n    binary.BigEndian.PutUint32(buf[:4], frameLen)\n    copy(buf[4:], payload)\n    _, err := conn.Write(buf)\n    return err\n}\n\nfunc readFrame(conn net.Conn) ([]byte, error) {\n    lenBuf := make([]byte, 4)\n    if _, err := io.ReadFull(conn, lenBuf); err != nil {\n        return nil, err\n    }\n    frameLen := binary.BigEndian.Uint32(lenBuf)\n    payload := make([]byte, frameLen)\n    if _, err := io.ReadFull(conn, payload); err != nil {\n        return nil, err\n    }\n    return payload, nil\n}\n</code></pre> <p>This approach is straightforward, performant, and predictable, yet it provides no built-in concurrency management, request multiplexing, or flow control\u2014these must be explicitly managed by the developer.</p>"},{"location":"02-networking/tcp-http2-grpc/#disadvantages","title":"Disadvantages","text":"<p>While the protocol is efficient and minimal, it lacks several features commonly found in more complex transport protocols. The lack of built-in framing features in raw TCP means that key responsibilities shift entirely to the application layer. There\u2019s no support for multiplexing, so only one logical message can be in flight per connection unless additional coordination is built manually\u2014pushing clients to open multiple connections to achieve parallelism. Flow control is also absent; unlike HTTP/2 or gRPC, there\u2019s no way to signal backpressure, making it easy for a fast sender to overwhelm a slow receiver, potentially exhausting memory or triggering a crash. There\u2019s no space for structured metadata like message types, compression flags, or trace context unless you embed them yourself into the payload format. And error handling is purely ad hoc\u2014there\u2019s no protocol-level mechanism for communicating faults, so malformed frames or incorrect lengths often lead to abrupt connection resets or inconsistent state. </p> <p>These limitations might be manageable in tightly scoped, high-performance systems where both ends of the connection are under full control and the protocol behavior is well understood. In such environments, the minimal overhead and direct access to the wire can justify the trade-offs. But in broader production contexts\u2014especially those involving multiple teams, evolving requirements, or untrusted clients\u2014they introduce significant risk. Without strict validation, clear framing, and robust error handling, even small inconsistencies can lead to silent corruption, resource leaks, or hard-to-diagnose failures. Building on raw TCP demands both precise engineering and long-term maintenance discipline.</p>"},{"location":"02-networking/tcp-http2-grpc/#performance-insights","title":"Performance Insights","text":"<ul> <li>Latency: Lowest achievable due to minimal overhead; ideal for latency-critical scenarios like financial trading systems.</li> <li>Throughput: Excellent, constrained only by network and application-layer handling.</li> <li>CPU/Memory Cost: Very low, with negligible overhead from protocol management.</li> </ul>"},{"location":"02-networking/tcp-http2-grpc/#http2-via-nethttp","title":"HTTP/2 via net/http","text":"<p>HTTP/2 brought several protocol-level improvements over HTTP/1.1, including multiplexed streams over a single connection, header compression via HPACK, and support for server push. In Go, these features are integrated directly into the <code>net/http</code> standard library, which handles connection reuse, stream multiplexing, and concurrency without requiring manual intervention. Unlike raw TCP, where applications must explicitly define message boundaries, HTTP/2 defines them at the protocol level: each request and response is framed using structured <code>HEADERS</code> and <code>DATA</code> frames and explicitly closed with an <code>END_STREAM</code> flag. These frames are handled entirely within Go\u2019s HTTP/2 implementation, so developers interact with complete, logically isolated messages using the standard <code>http.Request</code> and <code>http.ResponseWriter</code> interfaces. You don\u2019t have to deal with byte streams or worry about where a message starts or ends\u2014by the time a request hits your handler, it\u2019s already been framed and parsed. When you write a response, the runtime takes care of wrapping it up and signaling completion. That frees you up to focus on the logic, not the plumbing, while still getting the performance benefits of HTTP/2 like multiplexing and connection reuse.</p>"},{"location":"02-networking/tcp-http2-grpc/#server-implementation","title":"Server Implementation","text":"<p>Beyond framing and multiplexing, HTTP/2 brings a handful of practical advantages that make server code easier to write and faster to run. It handles connection reuse out of the box, applies flow control to avoid overwhelming either side, and compresses headers using <code>HPACK</code> to cut down on overhead. Go\u2019s <code>net/http</code> stack takes care of all of this behind the scenes, so you get the benefits without needing to wire it up yourself. As a result, developers can build concurrent, efficient servers without managing low-level connection or stream state manually.</p> <pre><code>func handler(w http.ResponseWriter, r *http.Request) {\n    payload, err := io.ReadAll(r.Body)\n    if err != nil {\n        http.Error(w, \"invalid request\", http.StatusBadRequest)\n        return\n    }\n    defer r.Body.Close()\n\n    // Process payload...\n\n    w.WriteHeader(http.StatusOK)\n    w.Write([]byte(\"processed\"))\n}\n\nfunc main() {\n    server := &amp;http.Server{\n        Addr:    \":8080\",\n        Handler: http.HandlerFunc(handler),\n    }\n    log.Fatal(server.ListenAndServeTLS(\"server.crt\", \"server.key\"))\n}\n</code></pre> <p>Info</p> <p>Even this is not mentioned explisitly, this code serves HTTP/2 because it uses <code>ListenAndServeTLS</code>, which enables TLS-based communication. Go's <code>net/http</code> package automatically upgrades connections to HTTP/2 when a client supports it via ALPN (Application-Layer Protocol Negotiation) during the TLS handshake. Since Go 1.6, this upgrade is implicit\u2014no extra configuration is required. The server transparently handles HTTP/2 requests while remaining compatible with HTTP/1.1 clients.</p> <p>HTTP/2\u2019s multiplexing capability allows multiple independent streams to share a single TCP connection without blocking each other, which significantly improves connection reuse. This reduces the overhead of establishing and managing parallel connections, especially under high concurrency. As a result, latency is lower and throughput more consistent, even when multiple requests are in flight. These traits make HTTP/2 well-suited for general-purpose web services and internal APIs\u2014places where predictable latency, efficient connection reuse, and solid concurrency handling carry more weight than raw protocol minimalism.</p>"},{"location":"02-networking/tcp-http2-grpc/#performance-insights_1","title":"Performance Insights","text":"<ul> <li>Latency: Slightly higher than raw TCP because of framing and compression overhead, but stable and consistent thanks to multiplexing and persistent connections.</li> <li>Throughput: High under concurrent load; stream multiplexing and header compression help sustain performance without opening more sockets.</li> <li>CPU/Memory Cost: Moderate overhead, mostly due to header processing, TLS encryption, and flow control mechanisms.</li> </ul>"},{"location":"02-networking/tcp-http2-grpc/#grpc","title":"gRPC","text":"<p>gRPC is a high-performance, contract-first RPC framework built on top of HTTP/2, designed for low-latency, cross-language communication between services. It combines streaming-capable transport with strongly typed APIs defined using Protocol Buffers (Protobuf), enabling compact, efficient message serialization and seamless interoperability across platforms. Unlike traditional HTTP APIs, where endpoints are loosely defined by URL patterns and free-form JSON, gRPC enforces strict interface contracts through <code>.proto</code> definitions, which serve as both schema and implementation spec. The gRPC toolchain generates client and server code for multiple languages, eliminating manual serialization, improving safety, and standardizing interactions across heterogeneous systems.</p> <p>gRPC takes advantage of HTTP/2\u2019s core features\u2014stream multiplexing, flow control, and binary framing\u2014to support both one-off RPC calls and full-duplex streaming, all with built-in backpressure. But it goes further than just transport. It bakes in support for deadlines, cancellation, structured metadata, and standardized error reporting, all of which help services communicate clearly and fail predictably. This makes gRPC a solid choice for internal APIs, service meshes, and performance-critical systems where you need efficiency, strong contracts, and reliable behavior under load.</p>"},{"location":"02-networking/tcp-http2-grpc/#grpc-service-definition","title":"gRPC Service Definition","text":"<p>A minimal <code>.proto</code> file example:</p> <pre><code>syntax = \"proto3\";\n\nservice EchoService {\n  rpc Echo(EchoRequest) returns (EchoResponse);\n}\n\nmessage EchoRequest {\n  string message = 1;\n}\n\nmessage EchoResponse {\n  string message = 1;\n}\n</code></pre> <p>Generated Go stubs allow developers to easily implement the service:</p> <pre><code>type server struct {\n    UnimplementedEchoServiceServer\n}\n\nfunc (s *server) Echo(ctx context.Context, req *EchoRequest) (*EchoResponse, error) {\n    return &amp;EchoResponse{Message: req.Message}, nil\n}\n\nfunc main() {\n    lis, err := net.Listen(\"tcp\", \":50051\")\n    if err != nil {\n        log.Fatalf(\"failed to listen: %v\", err)\n    }\n    grpcServer := grpc.NewServer()\n    RegisterEchoServiceServer(grpcServer, &amp;server{})\n    grpcServer.Serve(lis)\n}\n</code></pre>"},{"location":"02-networking/tcp-http2-grpc/#performance-insights_2","title":"Performance Insights","text":"<ul> <li>Latency: Slightly higher than raw HTTP/2 due to additional serialization/deserialization steps, yet still performant for most scenarios.</li> <li>Throughput: High throughput thanks to efficient payload serialization (protobuf) and inherent HTTP/2 multiplexing capabilities.</li> <li>CPU/Memory Cost: Higher than HTTP/2 due to protobuf encoding overhead; memory consumption slightly increased due to temporary object allocations.</li> </ul>"},{"location":"02-networking/tcp-http2-grpc/#choosing-the-right-protocol","title":"Choosing the Right Protocol","text":"<ul> <li>Internal APIs and microservices: gRPC usually hits the sweet spot\u2014it\u2019s fast, strongly typed, and easy to work with once the tooling is in place.</li> <li>Low-latency systems and trading platforms: Raw TCP with custom framing gives you the lowest overhead, but you\u2019re on your own for everything else.</li> <li>Public APIs or general web services: HTTP/2 via net/http is a solid choice. You get connection reuse, multiplexing, and good performance without needing to pull in a full RPC stack.</li> </ul> <p>Raw TCP gives you maximum control and the best performance on paper\u2014but it also means building everything yourself: framing, flow control, error handling. HTTP/2 and gRPC trade some of that raw speed for built-in structure, better connection handling, and less code to maintain. What\u2019s right depends entirely on where performance matters and how much complexity you want to own.</p>"},{"location":"02-networking/tls-for-speed/","title":"Optimizing TLS for Speed: Handshake, Reuse, and Cipher Choice","text":"<p>TLS does what it\u2019s supposed to: it keeps your connections private and trustworthy. But it also slows things down \u2014 a lot more than most people realize. In Go, if you care about how quickly your service responds, you can squeeze out better performance by tuning how TLS negotiates and what it negotiates.</p>"},{"location":"02-networking/tls-for-speed/#understanding-tls-overhead-where-performance-suffers","title":"Understanding TLS Overhead: Where Performance Suffers","text":"<p>Most of the slowdown in TLS happens right at the start. The handshake is a back-and-forth process: picking algorithms, swapping keys, proving identities, and setting up the session. That back-and-forth usually takes two full trips across the network. In something like a trading platform or a real-time app, that delay is noticeable.</p> <p>To make TLS faster, the most effective place to start is cutting down the handshake steps and making the crypto work less expensive.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Server\n\n    Client-&gt;&gt;Server: ClientHello (supported ciphers, random)\n    Server-&gt;&gt;Client: ServerHello (chosen cipher, random)\n    Server-&gt;&gt;Client: Certificate\n    Server-&gt;&gt;Client: ServerKeyExchange\n    Client-&gt;&gt;Server: ClientKeyExchange\n    Client-&gt;&gt;Server: ChangeCipherSpec\n    Server-&gt;&gt;Client: ChangeCipherSpec\n    Note over Client,Server: Handshake Complete \u2013 Encrypted communication begins</code></pre>"},{"location":"02-networking/tls-for-speed/#session-resumption-cutting-handshake-latency","title":"Session Resumption: Cutting Handshake Latency","text":"<p>Because every new TLS connection runs the entire handshake \u2014 negotiating ciphers, exchanging keys, verifying certificates \u2014 it introduces noticeable latency. Session resumption sidesteps most of that by reusing the cryptographic state from an earlier session, making reconnects much faster.</p> <p>Session resumption is a mechanism in TLS to avoid repeating the full handshake on reconnect. There are two main approaches: session IDs and session tickets. Both rely on the idea that the server remembers (or encodes) the session\u2019s cryptographic state from a prior connection. When a client reconnects, it presents either the session ID or the session ticket, allowing the server to restore the session state and skip expensive asymmetric key exchange.</p> <p>A session ticket is a data blob issued by the server to the client at the end of the handshake. This ticket contains the encrypted session state (such as negotiated cipher suite, keys, and session parameters) and is opaque to the client. On reconnect, the client sends the ticket back, and the server decrypts it to resume the session without performing a full handshake.</p> <p>In Go, you enable session resumption by setting up session ticket keys. The server uses these keys to encrypt and decrypt the session state that clients send back when resuming a connection. You can generate a secure 32\u2011byte key at startup with crypto/rand and reuse it if your service is running across multiple instances behind a load balancer. Just make sure to rotate the key now and then to keep it secure.</p> <pre><code>tlsConfig := &amp;tls.Config{\n    SessionTicketsDisabled: false, // Enable session tickets explicitly\n    SessionTicketKey: [32]byte{...}, // Persist securely and rotate periodically\n}\n</code></pre> <p>What makes session resumption effective is that it avoids re\u2011doing the slowest parts of TLS. Instead of negotiating everything from scratch, the server decrypts the ticket, verifies it, and restores the session. That eliminates at least one full round trip and all the heavy asymmetric operations, which saves both time and resources.</p>"},{"location":"02-networking/tls-for-speed/#choosing-cipher-suites-wisely","title":"Choosing Cipher Suites Wisely","text":"<p>Cipher suites define the combination of algorithms used for key exchange, authentication, encryption, and integrity checking. Selecting efficient cipher suites significantly impacts performance.</p> <p>Why prefer one over another? Some suites use RSA for key exchange, which is much slower than elliptic-curve Diffie\u2013Hellman (ECDHE) and lacks forward secrecy. Suites with AES-GCM are faster on modern CPUs thanks to hardware acceleration, whereas older CBC-mode suites are slower and more error-prone. ECC-based authentication (ECDSA) also provides shorter signatures and lower computational cost compared to RSA.</p> <p>To choose wisely:</p> <ul> <li>Prioritize suites with ECDHE for forward secrecy and better performance.</li> <li>Prefer AES-GCM over AES-CBC or ChaCha20 unless targeting hardware that lacks AES acceleration (where ChaCha20 may win).</li> <li>Avoid suites using only RSA for key exchange, because they lack forward secrecy and require more expensive computations on the server side.</li> </ul> <p>The Go standard library ships with a solid set of secure and reasonably fast defaults. For most applications, you can stick with those and be fine. But if you\u2019re tuning for a high\u2011performance environment, defining your own preferred cipher suites gives you tighter control over what gets negotiated.</p> <p>An optimized config favors AES\u2011GCM for its hardware acceleration and ECC for its efficiency.</p> <pre><code>tlsConfig := &amp;tls.Config{\n    CipherSuites: []uint16{\n        tls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\n        tls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,\n        tls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,\n        tls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,\n    },\n    PreferServerCipherSuites: true,\n}\n</code></pre> <p>These suites strike a good balance between security and speed, leveraging forward secrecy and hardware-accelerated encryption. The second option, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, is a reasonable choice when clients may not support ECDSA certificates \u2014 it still provides forward secrecy and efficient AES-GCM encryption while relying on the more widely deployed RSA for authentication.</p>"},{"location":"02-networking/tls-for-speed/#using-alpn-wisely","title":"Using ALPN Wisely","text":"<p>Application-Layer Protocol Negotiation (ALPN) lets clients and servers agree upon the application protocol (like HTTP/2, HTTP/1.1, or gRPC) during the TLS handshake, avoiding additional round trips or guessing after establishing the connection. Without ALPN, a client would have to fall back to slower or less efficient methods to detect the server\u2019s supported protocol.</p> <p>In Go, the <code>NextProtos</code> field in <code>tls.Config</code> defines which application protocols the server supports and in what order of preference. During the TLS handshake, the client sends its own list, and the server picks the highest\u2011priority protocol that both sides support, then confirms it.</p> <p>If you want to support <code>HTTP/2</code> with a fallback to <code>HTTP/1.1</code>, you should set <code>NextProtos</code> explicitly, like this:</p> <pre><code>tlsConfig := &amp;tls.Config{\n    NextProtos: []string{\"h2\", \"http/1.1\"},\n}\n</code></pre> <p>The sequence of entries in <code>NextProtos</code> matters because the server walks the list in order and chooses the first protocol also offered by the client. By putting HTTP/2 (<code>h2</code>) at the top, the server ensures it prefers HTTP/2 when supported, falling back to HTTP/1.1 only when necessary. If the order is incorrect, the server may settle on a slower protocol or fail to agree entirely.</p> <p>When <code>NextProtos</code> is left empty, ALPN is effectively disabled. The client assumes HTTP/1.1 because it never sees an indication of HTTP/2 support during the handshake. Explicitly setting <code>NextProtos</code> advertises the server\u2019s capabilities and avoids unnecessary protocol downgrades.</p>"},{"location":"02-networking/tls-for-speed/#minimizing-certificate-verification-overhead","title":"Minimizing Certificate Verification Overhead","text":"<p>Certificate verification tends to be CPU\u2011heavy because it involves several expensive asymmetric cryptographic operations, which directly increase connection latency. This cost can be reduced in two ways: by using smaller, faster\u2011to\u2011verify ECC\u2011based (ECDSA) certificates instead of RSA, and by caching already\u2011validated certificate chains to avoid repeating the same verification work on subsequent connections.</p> <pre><code>tlsConfig := &amp;tls.Config{\n    ClientAuth: tls.RequireAndVerifyClientCert,\n    ClientCAs: certPool, // pre-verified CA pool\n    VerifyPeerCertificate: cachedCertVerifier, // custom verifier with caching\n}\n</code></pre> <p>A basic caching mechanism stores verification outcomes temporarily:</p> <pre><code>// Cache to avoid re-verifying the same certificate repeatedly\nvar verificationCache sync.Map\n\n// cachedCertVerifier verifies a peer certificate chain and caches successful leaf fingerprints\nfunc cachedCertVerifier(rawCerts [][]byte, verifiedChains [][]*x509.Certificate) error {\n    // Compute SHA-256 fingerprint of the leaf certificate\n    fingerprint := sha256.Sum256(rawCerts[0])\n    if _, exists := verificationCache.Load(fingerprint); exists {\n        // Already verified earlier; skip full verification\n        return nil\n    }\n\n    // Parse the leaf certificate\n    leafCert, err := x509.ParseCertificate(rawCerts[0])\n    if err != nil {\n        return fmt.Errorf(\"failed to parse leaf certificate: %w\", err)\n    }\n\n    // Build pool of intermediate certificates provided by peer\n    intermediatePool := x509.NewCertPool()\n    for _, raw := range rawCerts[1:] {\n        intermediateCert, err := x509.ParseCertificate(raw)\n        if err != nil {\n            return fmt.Errorf(\"failed to parse intermediate certificate: %w\", err)\n        }\n        intermediatePool.AddCert(intermediateCert)\n    }\n\n    // Prepare verification options with trusted roots &amp; intermediates\n    opts := x509.VerifyOptions{\n        Roots:         certPool,                            // trusted root CAs\n        Intermediates: intermediatePool,                    // peer-provided intermediates\n        KeyUsages:     []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth},\n    }\n\n    // Verify the full chain starting from leaf\n    chains, err := leafCert.Verify(opts)\n    if err == nil {\n        // Store successful verification in cache\n        verificationCache.Store(fingerprint, struct{}{})\n    }\n    return err\n}\n</code></pre> <p>Warning</p> <p>While the snippet illustrates key concepts, it should not be used in production as-is. Cryptographic code demands careful analysis and adaptation to the specific environment and threat model. Always validate and test security-related code before deployment to avoid introducing weaknesses.</p>"},{"location":"02-networking/tls-for-speed/#tls-best-practices-in-go","title":"TLS Best Practices in Go","text":"<p>The following configuration brings together these techniques into a tls.Config that is optimized for both performance and security.</p> <p>The <code>MinVersion</code> setting tells the server to reject older, broken versions of TLS. Anything below TLS 1.2 \u2014 like SSLv3, TLS 1.0, or 1.1 \u2014 is riddled with weaknesses and no longer considered safe. TLS 1.2 fixed many of those problems by introducing AEAD ciphers like AES\u2011GCM and defenses against attacks such as BEAST and POODLE.</p> <p>The <code>CurvePreferences</code> field prioritizes fast and secure elliptic curves supported by most clients. P-256 and X25519 are efficient, widely-supported, and often hardware-accelerated. P-256 (also known as secp256r1) is an NIST-recommended curve with broad compatibility and native support in most CPUs for fast computation. X25519 is a modern curve designed for both speed and simplicity, offering constant-time operations to resist side-channel attacks, and performs especially well on low-power or embedded devices.</p> <p>Te rest parts were discussed in details above.</p> <pre><code>tlsConfig := &amp;tls.Config{\n    SessionTicketsDisabled: false,\n    SessionTicketKey: [32]byte{...},\n    CipherSuites: []uint16{\n        tls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\n        tls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,\n        tls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,\n        tls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,\n    },\n    PreferServerCipherSuites: true,\n    NextProtos: []string{\"h2\", \"http/1.1\"},\n    ClientAuth: tls.RequireAndVerifyClientCert,\n    ClientCAs: certPool,\n    VerifyPeerCertificate: cachedCertVerifier,\n    MinVersion: tls.VersionTLS12,\n    CurvePreferences: []tls.CurveID{tls.CurveP256, tls.X25519},\n}\n</code></pre> <p>TLS optimization is about balancing performance and security effectively. By reducing handshake overhead through session reuse, wisely selecting cipher suites with ECC and hardware acceleration, efficiently leveraging ALPN, and minimizing certificate verification overhead through caching, Go applications can achieve both security and speed. This approach makes TLS less a performance bottleneck and more a seamless security foundation for modern network services.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/04/03/lazy-initialization-in-go-using-atomics/","title":"Lazy initialization in Go using atomics","text":"<p>Aside from the main performance guide, I'm considering using the blog to share quick, informal insights and quirks related to Go performance and optimizations. Let's see if this casual experiment survives contact with reality.</p> <p>Someone recently pointed out that my <code>getResource()</code> function using atomics has a race condition. Guilty as charged\u2014rookie mistake, really. The issue? I na\u00efvely set the <code>initialized</code> flag to <code>true</code> before the actual resource is ready. Brilliant move, right? This means that with concurrent calls, one goroutine might proudly claim victory while handing out a half-baked resource:</p> <pre><code>var initialized atomic.Bool\nvar resource *MyResource\n\nfunc getResource() *MyResource {\n    if !initialized.Load() {\n        if initialized.CompareAndSwap(false, true) {\n            resource = expensiveInit()\n        }\n    }\n    return resource\n}\n</code></pre> <p>Can this mess be salvaged? Almost certainly, it just needs a touch more thought. To squash the race, we need atomic operations directly on the pointer rather than messing with a separate boolean. Enter Go's atomic package with <code>unsafe.Pointer</code> magic:</p> <pre><code>import (\n    \"sync/atomic\"\n    \"unsafe\"\n)\n\nvar resource unsafe.Pointer // holds *MyResource\n\nfunc getResource() *MyResource {\n    // Attempt to load the resource atomically.\n    ptr := atomic.LoadPointer(&amp;resource)\n    if ptr != nil {\n        return (*MyResource)(ptr) // Resource already initialized, return it\n    }\n\n    // Resource appears uninitialized, perform expensive initialization\n    newRes := expensiveInit()\n\n    // Attempt to atomically set the resource to the newly initialized value\n    if atomic.CompareAndSwapPointer(&amp;resource, nil, unsafe.Pointer(newRes)) {\n        return newRes // Successfully initialized and stored\n    }\n\n    // Another goroutine beat us to initialization, return their initialized resource\n    return (*MyResource)(atomic.LoadPointer(&amp;resource))\n}\n</code></pre> <p>This does the trick\u2014but introduces another subtle hiccup: several goroutines might simultaneously invoke <code>expensiveInit()</code> if they concurrently see a <code>nil</code> pointer. You definetly don't want multiple expensive initializations\u2014unless you're swimming in CPU cycles.</p> <p>So, yes, we do need state tracking. The obvious fix? An intermediate initialization state:</p> <pre><code>import (\n    \"runtime\"\n    \"sync/atomic\"\n    \"unsafe\"\n)\n\nvar resource unsafe.Pointer\nvar initStatus int32 // 0: untouched, 1: in-progress, 2: done\n\nfunc getResource() *MyResource {\n    // Check quickly if initialization is already done\n    if atomic.LoadInt32(&amp;initStatus) == 2 {\n        return (*MyResource)(atomic.LoadPointer(&amp;resource)) // Initialization complete\n    }\n\n    // Attempt to become the goroutine that performs initialization\n    if atomic.CompareAndSwapInt32(&amp;initStatus, 0, 1) {\n        newRes := expensiveInit() // Only this goroutine initializes\n        atomic.StorePointer(&amp;resource, unsafe.Pointer(newRes)) // Store the initialized resource\n        atomic.StoreInt32(&amp;initStatus, 2) // Mark initialization as complete\n        return newRes\n    }\n\n    // Other goroutines wait until initialization completes\n    for atomic.LoadInt32(&amp;initStatus) != 2 {\n        runtime.Gosched() // Chill out and let the initializer finish\n    }\n    return (*MyResource)(atomic.LoadPointer(&amp;resource)) // Initialization complete, return resource\n}\n</code></pre> <p>With this approach, only one goroutine earns the privilege of performing <code>expensiveInit()</code>. Others politely wait, spinning their wheels (well, yielding the CPU politely) until initialization completes.</p> <p>Warning</p> <p>If <code>expensiveInit()</code> panics, this implementation will spin forever! Either handle panic properly or ensure that <code>expensiveInit()</code> has never panicked.</p> <p>Info</p> <p>It's worth noting that this atomic-based approach can be advantageous in scenarios involving a high frequency of calls, where a spinlock's short waiting cycles may be more efficient than a mutex. This is because mutexes can cause frequent context switches, handing control over to the OS scheduler, which can introduce additional overhead.</p> <p>Of course, the more practical solution is usually simpler\u2014<code>sync.Once</code> to the rescue:</p> <pre><code>import \"sync\"\n\nvar (\n    resource *MyResource\n    once     sync.Once\n)\n\nfunc getResource() *MyResource {\n    once.Do(func() {\n        resource = expensiveInit()\n    })\n    return resource\n}\n</code></pre> <p><code>sync.Once</code> elegantly handles initialization, saves your CPUs from unnecessary spin cycles, and keeps your code clean. So, stick to the tried and true unless you have very specific reasons to juggle atomics. Trust me\u2014your future self will thank you.</p>"},{"location":"blog/2025/07/31/when-c-optimization-slows-down-your-go-code/","title":"When C++ Optimization Slows Down Your Go Code","text":"<p>When you have years of C++ experience, you definitely obtain some habits. These habits are good for C++, but could cause you some surprises in Go. In C++, you usually preallocate everything, avoiding unnecessary allocations, caching values aggressively, and always thinking of CPU cache misses. So when I rewrote a simple algorithm in Go\u2014finding the number of days until the next warmer temperature\u2014I reached for the same tricks. But this time, they backfired.</p> <p>Here\u2019s how applying familiar C++ optimizations ended up making my Go code slower and heavier.</p>"},{"location":"blog/2025/07/31/when-c-optimization-slows-down-your-go-code/#the-c-context","title":"The C++ Context","text":"<p>The original problem: for each day, figure out how many days pass until a warmer temperature appears. A classic use case for a monotonic stack. Here's the performance data from a C++ implementation:</p> dailyTemperatures, basic implementation <pre><code>std::vector&lt;int&gt; dailyTemperatures( const std::vector&lt;int&gt; &amp;temperatures )\n{\n    std::vector&lt;int&gt; result( temperatures.size(), 0 );\n    std::stack&lt;int&gt;  s;\n    for( int i = 0; i &lt; temperatures.size(); ++i ) {\n        while( !s.empty() &amp;&amp; temperatures[i] &gt; temperatures[s.top()] ) {\n            int prev = s.top();\n            s.pop();\n            result[prev] = i - prev;\n        }\n        s.push( i );\n    }\n    return result;\n}\n</code></pre> dailyTemperatures, optimized implementation <p><pre><code>std::vector&lt;int&gt; dailyTemperaturesOpt( const std::vector&lt;int&gt; &amp;temperatures )\n{\n    std::vector&lt;int&gt; res( temperatures.size(), 0 );\n    std::vector&lt;int&gt; track; // (1)\n    track.reserve( temperatures.size() ); // (2)\n\n    for( int i = 0; i &lt; temperatures.size(); ++i ) {\n        int currTemp = temperatures[i]; // (3)\n        while( !track.empty() &amp;&amp; currTemp &gt; temperatures[track.back()] ) {\n            int prev = track.back();\n            track.pop_back();\n            res[prev] = i - prev;\n        }\n        track.push_back( i );\n    }\n    return res;\n}\n</code></pre></p> <ol> <li> <p><code>std::vector</code> is used instead of <code>std::stack</code> for better control and performance. Unlike <code>std::stack</code>, <code>std::vector</code> allows preallocation, random access, and avoids the overhead of an adapter layer. It's more cache-friendly and directly exposes the underlying data layout.</p> </li> <li> <p>We reserve the full capacity of the <code>track</code> stack to avoid dynamic memory reallocations during growth. Since the number of pushed elements can never exceed <code>temperatures.size()</code>, this is a safe and efficient optimization.</p> </li> <li> <p><code>currTemp</code> holds the value of <code>temperatures[i]</code> so we don\u2019t hit the same memory location multiple times inside the loop. While the access pattern is sequential and usually cache-friendly, reading the same slice element repeatedly adds work for the compiler and the CPU. By assigning it to a local variable, we give the compiler a clear signal that this value won\u2019t change. That usually means it stays in a register, which can reduce instruction count and make the inner loop tighter\u2014especially when you\u2019re benchmarking or pushing for low-latency behavior.</p> </li> </ol> Benchmark Time per op (ns) BM_DailyTemperatures/100000 206,340 BM_DailyTemperaturesOpt/100000 115,490 <p>Standard C++ optimization tactics can cut the runtime almost in half. But even if something works in C++ pretty well, it does not mean that the same approach will not make your Go code slower.</p>"},{"location":"blog/2025/07/31/when-c-optimization-slows-down-your-go-code/#translating-to-go","title":"Translating to Go","text":"<p>Here\u2019s what a clean idiomatic Go version looks like:</p> <pre><code>func DailyTemperatures(temperatures []int) []int {\n    result := make([]int, len(temperatures))\n    var stack []int\n\n    for i, temp := range temperatures {\n        for len(stack) &gt; 0 &amp;&amp; temp &gt; temperatures[stack[len(stack)-1]] {\n            prevIndex := stack[len(stack)-1]\n            stack = stack[:len(stack)-1]\n            result[prevIndex] = i - prevIndex\n        }\n        stack = append(stack, i)\n    }\n\n    return result\n}\n</code></pre> <p>Pretty typical: no preallocation, straightforward stack growth via append.</p> <p>I rewrote this with \u201coptimizations\u201d: preallocate the stack slice, replace variables early, and reduce slice bounds checks. Classic C++-style low-level thinking.</p> <p><pre><code>func DailyTemperaturesOpt(temperatures []int) []int {\n    n := len(temperatures)\n    result := make([]int, n)\n    stack := make([]int, 0, n/4) // (1)\n\n    for i := 0; i &lt; n; i++ {\n        curr := temperatures[i] // (2)\n        for len(stack) &gt; 0 &amp;&amp; curr &gt; temperatures[stack[len(stack)-1]] {\n            prev := stack[len(stack)-1]\n            stack = stack[:len(stack)-1]\n            result[prev] = i - prev\n        }\n        stack = append(stack, i)\n    }\n    return result\n}\n</code></pre></p> <ol> <li>The <code>stack</code> is preallocated with a capacity of <code>n/4</code>, not the full <code>n</code>, to strike a balance between reducing allocations and avoiding unnecessary heap growth. In Go, over-allocation can backfire due to GC sensitivity and increased memory footprint. A smaller initial capacity avoids paying for memory that may never be used. NOTE: The result will be even worse if we preallocate <code>len(temperatures)</code> here.</li> <li><code>curr</code> stores <code>temperatures[i]</code> so we don\u2019t keep looking it up inside the loop. Go\u2019s compiler might optimize that on its own, but being explicit gives it less to guess about. It also helps the value stay in a register instead of bouncing back to memory, which can matter in tight loops. If you\u2019re running benchmarks or chasing small gains, this kind of local caching can reduce register pressure and cut down on subtle overhead.</li> </ol> <p>And the Result? It got worse.</p> Benchmark Time per op (ns) Bytes per op Allocs per op BenchmarkDailyTemperatures/Baseline-14 174,419 862,847 15 BenchmarkDailyTemperatures/Optimized-14 175,021 1,007,620 2 <p>Same logic, but now slower and heavier. One fewer allocation, but an extra 150KB of memory usage. Why? Because in Go, allocating a 100,000-capacity slice (even if you barely use it) is expensive. The runtime doesn\u2019t treat that lightly.</p>"},{"location":"blog/2025/07/31/when-c-optimization-slows-down-your-go-code/#why-go-behaves-differently","title":"Why Go Behaves Differently","text":"<p>The runtime is more opinionated. Memory is GC-managed. There\u2019s no real benefit to preallocating more than you need, especially if your code doesn\u2019t end up using it. <code>append()</code> is cheap, and in many cases more efficient than second-guessing the allocator.</p> <p>On top of that, Go\u2019s escape analysis doesn\u2019t work the way C++\u2019s stack allocation does. What you think is local might end up on the heap, just because of one indirect reference.</p> <p>Key Takeaways</p> <ul> <li>Preallocation helps in C++ because memory layout and growth are under your control. In Go, the runtime handles it differently.</li> <li>Trust the idioms of the language. If you\u2019re writing Go, let Go be Go.</li> <li>Measure everything. Some optimizations only look good on paper\u2014or in other languages.</li> </ul> <p>I still write C++. I still optimize memory, inlining, and stack frames. But when I\u2019m in Go, I\u2019ve learned to lean into the model that Go is designed for. Sometimes performance comes from understanding how much less you need to do\u2014not how much you can tweak.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/optimizations/","title":"optimizations","text":""},{"location":"blog/category/atomics/","title":"atomics","text":""}]}